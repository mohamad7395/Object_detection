{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01cb3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_image_path example:  /Users/mohamad/python-workspace/lab/extract_img/tr_pic/train/37/8.jpg\n",
      "Train size: 422\n",
      "Valid size: 106\n",
      "Test size: 81\n",
      "<__main__.imgDataset object at 0x7fb5317ff490>\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.common import flatten\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "f = open('/Users/mohamad/python-workspace/lab/extract_img/labels.plk', 'rb')\n",
    "labels_p = pickle.load(f)   \n",
    "transform = T.Compose([\n",
    "                       T.ToTensor(),\n",
    "                      T.Resize(45)])\n",
    "\n",
    "####################################################\n",
    "#       Create Train, Valid and Test sets\n",
    "####################################################\n",
    "train_data_path = '/Users/mohamad/python-workspace/lab/extract_img/tr_pic/train' \n",
    "test_data_path = '/Users/mohamad/python-workspace/lab/extract_img/tr_pic/test' \n",
    "\n",
    "train_image_paths = [] #to store image paths in list\n",
    "\n",
    "#1.\n",
    "# get all the paths from train_data_path and append image paths and class to to respective lists\n",
    "# eg. train path-> 'images/train/26.Pont_du_Gard/4321ee6695c23c7b.jpg'\n",
    "# eg. class -> 26.Pont_du_Gard\n",
    "for data_path in glob.glob(train_data_path + '/*'):\n",
    "    train_image_paths.append(glob.glob(data_path + '/*'))\n",
    "    \n",
    "train_image_paths = list(flatten(train_image_paths))\n",
    "random.shuffle(train_image_paths)\n",
    "\n",
    "print('train_image_path example: ', train_image_paths[0])\n",
    "\n",
    "\n",
    "#2.\n",
    "# split train valid from train paths (80,20)\n",
    "train_image_paths, valid_image_paths = train_image_paths[:int(0.8*len(train_image_paths))], train_image_paths[int(0.8*len(train_image_paths)):] \n",
    "\n",
    "#3.\n",
    "# create the test_image_paths\n",
    "test_image_paths = []\n",
    "for data_path in glob.glob(test_data_path + '/*'):\n",
    "    test_image_paths.append(glob.glob(data_path + '/*'))\n",
    "\n",
    "test_image_paths = list(flatten(test_image_paths))\n",
    "\n",
    "print(\"Train size: {}\\nValid size: {}\\nTest size: {}\".format(len(train_image_paths), len(valid_image_paths), len(test_image_paths)))\n",
    "\n",
    "class imgDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.image_paths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "#         image = cv2.resize(image, (90,160), interpolation = cv2.INTER_AREA)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         image = cv2\n",
    "        indx=int(image_filepath.split('/')[-1].split('.')[-2])\n",
    "        row=int(image_filepath.split('/')[-2])\n",
    "        label=(labels_p[row][indx])/8\n",
    "        # label = image_filepath.split('/')[-2]\n",
    "        # label = class_to_idx[label]\n",
    "\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image, label \n",
    "    \n",
    "#######################################################\n",
    "#                  Create Dataset\n",
    "#######################################################\n",
    "\n",
    "train_dataset = imgDataset(train_image_paths, transform)\n",
    "valid_dataset = imgDataset(valid_image_paths, transform) #test transforms are applied\n",
    "test_dataset = imgDataset(test_image_paths, transform)\n",
    "\n",
    "\n",
    "print(test_dataset)\n",
    "\n",
    "#######################################################\n",
    "#                  Define Dataloaders\n",
    "#######################################################\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "7d175276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Cuda Available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "def create_lenet():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 8, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(2, stride=2),\n",
    "        nn.Conv2d(8, 16, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(2, stride=2),\n",
    "#         nn.Conv2d(16, 16, 5),\n",
    "#         nn.ReLU(),\n",
    "#         nn.AvgPool2d(2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(2176, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(64, 1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def validate(model, data):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    for i, (images, labels) in enumerate(data):\n",
    "        images = images.cpu()\n",
    "        images = torch.reshape(images,(images.shape[0],3,80,45))\n",
    "        \n",
    "        x = model(images)\n",
    "\n",
    "        ll = criterion(x,labels.to(torch.float))\n",
    "    return ll\n",
    "\n",
    "\n",
    "def train(numb_epoch=3, lr=1e-5, device=\"cpu\"):\n",
    "    accuracies = []\n",
    "    tr_losses = []\n",
    "    cnn = create_lenet().to(device)\n",
    "    cec = nn.MSELoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=lr)\n",
    "    max_accuracy = 2500.0\n",
    "    for epoch in range(numb_epoch):\n",
    "        for i, (images, labels) in enumerate(train_dl):\n",
    "            images = images.to(device)\n",
    "            images = torch.reshape(images,(images.shape[0],3,80,45))\n",
    "            \n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            pred = cnn(images)\n",
    "            loss = cec(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        accuracy = float(validate(cnn, valid_dl))\n",
    "        tr_loss = float(validate(cnn, train_dl))\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        if accuracy < max_accuracy:\n",
    "            best_model = copy.deepcopy(cnn)\n",
    "            max_accuracy = accuracy\n",
    "            print(\"Saving Best Model with Accuracy: \", accuracy)\n",
    "        print('Epoch:', epoch+1, \"Loss :\", accuracy)\n",
    "        print(\"training loss-------------> \" ,tr_loss)\n",
    "        tr_losses.append(tr_loss) \n",
    "    plt.plot(accuracies,)\n",
    "    plt.plot(tr_losses)\n",
    "    return best_model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No Cuda Available\")\n",
    "device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "cee97825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model with Accuracy:  1153.9420166015625\n",
      "Epoch: 1 Loss : 1153.9420166015625\n",
      "training loss------------->  3159.561279296875\n",
      "Saving Best Model with Accuracy:  875.736083984375\n",
      "Epoch: 2 Loss : 875.736083984375\n",
      "training loss------------->  1631.7200927734375\n",
      "Epoch: 3 Loss : 2167.229248046875\n",
      "training loss------------->  4028.599853515625\n",
      "Epoch: 4 Loss : 4707.16259765625\n",
      "training loss------------->  2064.33544921875\n",
      "Epoch: 5 Loss : 2152.73095703125\n",
      "training loss------------->  2474.44677734375\n",
      "Epoch: 6 Loss : 2517.5771484375\n",
      "training loss------------->  1494.2254638671875\n",
      "Epoch: 7 Loss : 3476.489501953125\n",
      "training loss------------->  2431.72021484375\n",
      "Epoch: 8 Loss : 3601.676513671875\n",
      "training loss------------->  1683.124267578125\n",
      "Saving Best Model with Accuracy:  856.5062255859375\n",
      "Epoch: 9 Loss : 856.5062255859375\n",
      "training loss------------->  1943.2882080078125\n",
      "Saving Best Model with Accuracy:  772.0859375\n",
      "Epoch: 10 Loss : 772.0859375\n",
      "training loss------------->  723.2012939453125\n",
      "Epoch: 11 Loss : 789.9647827148438\n",
      "training loss------------->  528.2804565429688\n",
      "Epoch: 12 Loss : 1761.2149658203125\n",
      "training loss------------->  318.69622802734375\n",
      "Epoch: 13 Loss : 1260.077392578125\n",
      "training loss------------->  361.89288330078125\n",
      "Epoch: 14 Loss : 811.7596435546875\n",
      "training loss------------->  915.2913208007812\n",
      "Saving Best Model with Accuracy:  483.5489196777344\n",
      "Epoch: 15 Loss : 483.5489196777344\n",
      "training loss------------->  1016.7456665039062\n",
      "Epoch: 16 Loss : 488.80377197265625\n",
      "training loss------------->  406.15435791015625\n",
      "Saving Best Model with Accuracy:  475.87640380859375\n",
      "Epoch: 17 Loss : 475.87640380859375\n",
      "training loss------------->  453.2030029296875\n",
      "Saving Best Model with Accuracy:  125.64686584472656\n",
      "Epoch: 18 Loss : 125.64686584472656\n",
      "training loss------------->  404.0859375\n",
      "Epoch: 19 Loss : 245.3933868408203\n",
      "training loss------------->  529.065673828125\n",
      "Epoch: 20 Loss : 448.1546936035156\n",
      "training loss------------->  471.06982421875\n",
      "Epoch: 21 Loss : 586.6452026367188\n",
      "training loss------------->  352.378173828125\n",
      "Saving Best Model with Accuracy:  92.79756927490234\n",
      "Epoch: 22 Loss : 92.79756927490234\n",
      "training loss------------->  645.1497802734375\n",
      "Epoch: 23 Loss : 146.35830688476562\n",
      "training loss------------->  205.192626953125\n",
      "Epoch: 24 Loss : 282.68438720703125\n",
      "training loss------------->  491.1002197265625\n",
      "Epoch: 25 Loss : 207.81512451171875\n",
      "training loss------------->  534.65966796875\n",
      "Epoch: 26 Loss : 1379.19921875\n",
      "training loss------------->  867.3941650390625\n",
      "Epoch: 27 Loss : 480.764892578125\n",
      "training loss------------->  296.5458984375\n",
      "Epoch: 28 Loss : 468.9277038574219\n",
      "training loss------------->  505.3138122558594\n",
      "Saving Best Model with Accuracy:  17.5097599029541\n",
      "Epoch: 29 Loss : 17.5097599029541\n",
      "training loss------------->  502.8096923828125\n",
      "Epoch: 30 Loss : 1165.5089111328125\n",
      "training loss------------->  249.9535369873047\n",
      "Epoch: 31 Loss : 209.20982360839844\n",
      "training loss------------->  958.5313720703125\n",
      "Epoch: 32 Loss : 239.62596130371094\n",
      "training loss------------->  377.462158203125\n",
      "Epoch: 33 Loss : 161.8532257080078\n",
      "training loss------------->  521.1465454101562\n",
      "Epoch: 34 Loss : 893.1674194335938\n",
      "training loss------------->  526.8421020507812\n",
      "Epoch: 35 Loss : 550.962158203125\n",
      "training loss------------->  500.2198486328125\n",
      "Saving Best Model with Accuracy:  15.994157791137695\n",
      "Epoch: 36 Loss : 15.994157791137695\n",
      "training loss------------->  269.23095703125\n",
      "Epoch: 37 Loss : 253.32498168945312\n",
      "training loss------------->  341.74615478515625\n",
      "Epoch: 38 Loss : 352.4804382324219\n",
      "training loss------------->  902.3367309570312\n",
      "Epoch: 39 Loss : 365.2745666503906\n",
      "training loss------------->  337.3902893066406\n",
      "Epoch: 40 Loss : 676.8065795898438\n",
      "training loss------------->  388.52996826171875\n",
      "Epoch: 41 Loss : 917.4826049804688\n",
      "training loss------------->  380.23236083984375\n",
      "Epoch: 42 Loss : 101.77186584472656\n",
      "training loss------------->  517.118896484375\n",
      "Epoch: 43 Loss : 826.28466796875\n",
      "training loss------------->  626.648193359375\n",
      "Epoch: 44 Loss : 33.699642181396484\n",
      "training loss------------->  549.8679809570312\n",
      "Epoch: 45 Loss : 587.5575561523438\n",
      "training loss------------->  173.89215087890625\n",
      "Epoch: 46 Loss : 305.8465270996094\n",
      "training loss------------->  777.3756713867188\n",
      "Epoch: 47 Loss : 652.8316650390625\n",
      "training loss------------->  642.544189453125\n",
      "Epoch: 48 Loss : 331.93206787109375\n",
      "training loss------------->  391.20916748046875\n",
      "Epoch: 49 Loss : 324.1444396972656\n",
      "training loss------------->  363.5601501464844\n",
      "Epoch: 50 Loss : 26.684520721435547\n",
      "training loss------------->  337.32720947265625\n",
      "Epoch: 51 Loss : 563.720458984375\n",
      "training loss------------->  332.7142333984375\n",
      "Epoch: 52 Loss : 1187.146240234375\n",
      "training loss------------->  571.3662109375\n",
      "Epoch: 53 Loss : 569.9237060546875\n",
      "training loss------------->  304.6512756347656\n",
      "Epoch: 54 Loss : 258.52789306640625\n",
      "training loss------------->  644.3316650390625\n",
      "Epoch: 55 Loss : 105.67305755615234\n",
      "training loss------------->  464.46142578125\n",
      "Epoch: 56 Loss : 206.30364990234375\n",
      "training loss------------->  989.1026611328125\n",
      "Epoch: 57 Loss : 890.057861328125\n",
      "training loss------------->  292.4618835449219\n",
      "Epoch: 58 Loss : 201.78717041015625\n",
      "training loss------------->  407.02325439453125\n",
      "Epoch: 59 Loss : 157.9216766357422\n",
      "training loss------------->  266.58465576171875\n",
      "Epoch: 60 Loss : 214.39674377441406\n",
      "training loss------------->  309.3009948730469\n",
      "Epoch: 61 Loss : 687.2578735351562\n",
      "training loss------------->  493.8240661621094\n",
      "Epoch: 62 Loss : 485.73687744140625\n",
      "training loss------------->  532.4180908203125\n",
      "Epoch: 63 Loss : 754.8446044921875\n",
      "training loss------------->  526.7014770507812\n",
      "Epoch: 64 Loss : 638.7634887695312\n",
      "training loss------------->  366.2430114746094\n",
      "Epoch: 65 Loss : 876.87939453125\n",
      "training loss------------->  355.22259521484375\n",
      "Epoch: 66 Loss : 68.7656478881836\n",
      "training loss------------->  449.5357971191406\n",
      "Epoch: 67 Loss : 781.7005615234375\n",
      "training loss------------->  355.0294494628906\n",
      "Epoch: 68 Loss : 252.70166015625\n",
      "training loss------------->  243.185791015625\n",
      "Epoch: 69 Loss : 496.21087646484375\n",
      "training loss------------->  341.45123291015625\n",
      "Epoch: 70 Loss : 508.928955078125\n",
      "training loss------------->  367.958251953125\n",
      "Epoch: 71 Loss : 130.95851135253906\n",
      "training loss------------->  284.7698669433594\n",
      "Epoch: 72 Loss : 185.12661743164062\n",
      "training loss------------->  272.283935546875\n",
      "Epoch: 73 Loss : 127.02306365966797\n",
      "training loss------------->  607.3617553710938\n",
      "Epoch: 74 Loss : 420.03033447265625\n",
      "training loss------------->  482.6741943359375\n",
      "Epoch: 75 Loss : 941.2916870117188\n",
      "training loss------------->  212.56182861328125\n",
      "Epoch: 76 Loss : 26.185779571533203\n",
      "training loss------------->  232.7129669189453\n",
      "Epoch: 77 Loss : 159.01466369628906\n",
      "training loss------------->  559.6153564453125\n",
      "Epoch: 78 Loss : 172.4547882080078\n",
      "training loss------------->  388.7127685546875\n",
      "Epoch: 79 Loss : 249.1376190185547\n",
      "training loss------------->  612.9127807617188\n",
      "Epoch: 80 Loss : 526.2060546875\n",
      "training loss------------->  262.0455017089844\n",
      "Epoch: 81 Loss : 114.53864288330078\n",
      "training loss------------->  309.0300598144531\n",
      "Epoch: 82 Loss : 149.8184051513672\n",
      "training loss------------->  649.0399169921875\n",
      "Epoch: 83 Loss : 358.98883056640625\n",
      "training loss------------->  405.8648681640625\n",
      "Epoch: 84 Loss : 115.71147155761719\n",
      "training loss------------->  524.7383422851562\n",
      "Epoch: 85 Loss : 507.6234436035156\n",
      "training loss------------->  491.09814453125\n",
      "Epoch: 86 Loss : 500.61212158203125\n",
      "training loss------------->  541.3513793945312\n",
      "Epoch: 87 Loss : 607.7239990234375\n",
      "training loss------------->  384.5415954589844\n",
      "Epoch: 88 Loss : 401.3287658691406\n",
      "training loss------------->  457.7523193359375\n",
      "Epoch: 89 Loss : 546.6043090820312\n",
      "training loss------------->  198.69491577148438\n",
      "Epoch: 90 Loss : 758.0615844726562\n",
      "training loss------------->  739.093017578125\n",
      "Epoch: 91 Loss : 73.57393646240234\n",
      "training loss------------->  312.40380859375\n",
      "Epoch: 92 Loss : 446.7853088378906\n",
      "training loss------------->  282.2769775390625\n",
      "Epoch: 93 Loss : 995.4293212890625\n",
      "training loss------------->  198.3243865966797\n",
      "Epoch: 94 Loss : 443.328125\n",
      "training loss------------->  499.2274169921875\n",
      "Epoch: 95 Loss : 116.75001525878906\n",
      "training loss------------->  273.7862548828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 Loss : 369.6788330078125\n",
      "training loss------------->  399.8455810546875\n",
      "Epoch: 97 Loss : 251.09774780273438\n",
      "training loss------------->  402.25567626953125\n",
      "Epoch: 98 Loss : 204.936767578125\n",
      "training loss------------->  405.7858581542969\n",
      "Epoch: 99 Loss : 939.6862182617188\n",
      "training loss------------->  276.4024353027344\n",
      "Epoch: 100 Loss : 403.1968688964844\n",
      "training loss------------->  571.7529296875\n",
      "Epoch: 101 Loss : 560.5029296875\n",
      "training loss------------->  287.3156433105469\n",
      "Epoch: 102 Loss : 121.10433197021484\n",
      "training loss------------->  350.37322998046875\n",
      "Epoch: 103 Loss : 419.0551452636719\n",
      "training loss------------->  157.26089477539062\n",
      "Epoch: 104 Loss : 114.04852294921875\n",
      "training loss------------->  600.9839477539062\n",
      "Epoch: 105 Loss : 97.28326416015625\n",
      "training loss------------->  495.0577392578125\n",
      "Epoch: 106 Loss : 254.07118225097656\n",
      "training loss------------->  348.5069274902344\n",
      "Epoch: 107 Loss : 410.4991760253906\n",
      "training loss------------->  620.5922241210938\n",
      "Epoch: 108 Loss : 574.1300048828125\n",
      "training loss------------->  433.7801513671875\n",
      "Epoch: 109 Loss : 669.399658203125\n",
      "training loss------------->  507.0562744140625\n",
      "Epoch: 110 Loss : 132.31175231933594\n",
      "training loss------------->  393.0824279785156\n",
      "Epoch: 111 Loss : 1397.006103515625\n",
      "training loss------------->  425.736083984375\n",
      "Epoch: 112 Loss : 122.91452026367188\n",
      "training loss------------->  471.74658203125\n",
      "Epoch: 113 Loss : 201.55364990234375\n",
      "training loss------------->  504.25054931640625\n",
      "Epoch: 114 Loss : 107.92863464355469\n",
      "training loss------------->  575.021240234375\n",
      "Epoch: 115 Loss : 108.51732635498047\n",
      "training loss------------->  461.3499450683594\n",
      "Epoch: 116 Loss : 430.4067687988281\n",
      "training loss------------->  433.7112121582031\n",
      "Epoch: 117 Loss : 324.71136474609375\n",
      "training loss------------->  344.3082275390625\n",
      "Epoch: 118 Loss : 518.2542724609375\n",
      "training loss------------->  326.345703125\n",
      "Epoch: 119 Loss : 115.93865966796875\n",
      "training loss------------->  288.4600830078125\n",
      "Epoch: 120 Loss : 343.18280029296875\n",
      "training loss------------->  313.5965576171875\n",
      "Epoch: 121 Loss : 574.865234375\n",
      "training loss------------->  428.7491455078125\n",
      "Epoch: 122 Loss : 203.78329467773438\n",
      "training loss------------->  267.2056884765625\n",
      "Epoch: 123 Loss : 156.31272888183594\n",
      "training loss------------->  370.6107482910156\n",
      "Epoch: 124 Loss : 537.7618408203125\n",
      "training loss------------->  181.33824157714844\n",
      "Epoch: 125 Loss : 507.6471862792969\n",
      "training loss------------->  461.86224365234375\n",
      "Epoch: 126 Loss : 158.93528747558594\n",
      "training loss------------->  430.9983215332031\n",
      "Epoch: 127 Loss : 227.34249877929688\n",
      "training loss------------->  599.8392944335938\n",
      "Epoch: 128 Loss : 460.766357421875\n",
      "training loss------------->  338.3720703125\n",
      "Epoch: 129 Loss : 369.2816162109375\n",
      "training loss------------->  333.8147277832031\n",
      "Epoch: 130 Loss : 245.5316162109375\n",
      "training loss------------->  284.0481262207031\n",
      "Epoch: 131 Loss : 344.0639953613281\n",
      "training loss------------->  142.89610290527344\n",
      "Epoch: 132 Loss : 523.695068359375\n",
      "training loss------------->  310.4656982421875\n",
      "Epoch: 133 Loss : 312.5487976074219\n",
      "training loss------------->  575.5286865234375\n",
      "Epoch: 134 Loss : 203.61094665527344\n",
      "training loss------------->  476.6506652832031\n",
      "Epoch: 135 Loss : 246.62890625\n",
      "training loss------------->  354.3394775390625\n",
      "Epoch: 136 Loss : 213.01283264160156\n",
      "training loss------------->  366.19293212890625\n",
      "Epoch: 137 Loss : 625.3928833007812\n",
      "training loss------------->  445.9288024902344\n",
      "Epoch: 138 Loss : 252.6812744140625\n",
      "training loss------------->  233.63055419921875\n",
      "Epoch: 139 Loss : 82.58149719238281\n",
      "training loss------------->  233.51226806640625\n",
      "Epoch: 140 Loss : 270.460205078125\n",
      "training loss------------->  332.73248291015625\n",
      "Epoch: 141 Loss : 973.2352294921875\n",
      "training loss------------->  341.4503479003906\n",
      "Epoch: 142 Loss : 1824.78125\n",
      "training loss------------->  732.5618286132812\n",
      "Epoch: 143 Loss : 472.8913879394531\n",
      "training loss------------->  405.6119384765625\n",
      "Epoch: 144 Loss : 243.59298706054688\n",
      "training loss------------->  588.1348876953125\n",
      "Epoch: 145 Loss : 515.5558471679688\n",
      "training loss------------->  144.4318389892578\n",
      "Epoch: 146 Loss : 156.63775634765625\n",
      "training loss------------->  515.2388916015625\n",
      "Epoch: 147 Loss : 689.1094970703125\n",
      "training loss------------->  432.0714111328125\n",
      "Epoch: 148 Loss : 240.27166748046875\n",
      "training loss------------->  262.79522705078125\n",
      "Epoch: 149 Loss : 531.8667602539062\n",
      "training loss------------->  421.1048583984375\n",
      "Epoch: 150 Loss : 683.3386840820312\n",
      "training loss------------->  522.4905395507812\n",
      "Epoch: 151 Loss : 395.8848571777344\n",
      "training loss------------->  370.2495422363281\n",
      "Epoch: 152 Loss : 303.454345703125\n",
      "training loss------------->  431.2586975097656\n",
      "Epoch: 153 Loss : 1054.360107421875\n",
      "training loss------------->  250.15164184570312\n",
      "Epoch: 154 Loss : 944.8536376953125\n",
      "training loss------------->  630.1472778320312\n",
      "Epoch: 155 Loss : 657.5278930664062\n",
      "training loss------------->  342.3911437988281\n",
      "Epoch: 156 Loss : 125.30879974365234\n",
      "training loss------------->  419.0892639160156\n",
      "Epoch: 157 Loss : 94.75932312011719\n",
      "training loss------------->  724.8667602539062\n",
      "Epoch: 158 Loss : 397.01641845703125\n",
      "training loss------------->  368.2093811035156\n",
      "Epoch: 159 Loss : 300.9632568359375\n",
      "training loss------------->  508.54193115234375\n",
      "Epoch: 160 Loss : 184.33694458007812\n",
      "training loss------------->  255.63177490234375\n",
      "Epoch: 161 Loss : 45.004295349121094\n",
      "training loss------------->  534.9078979492188\n",
      "Epoch: 162 Loss : 371.5804138183594\n",
      "training loss------------->  432.4778137207031\n",
      "Epoch: 163 Loss : 503.0086669921875\n",
      "training loss------------->  473.9405822753906\n",
      "Epoch: 164 Loss : 117.41416931152344\n",
      "training loss------------->  124.3726806640625\n",
      "Epoch: 165 Loss : 374.7565612792969\n",
      "training loss------------->  602.2434692382812\n",
      "Epoch: 166 Loss : 394.40521240234375\n",
      "training loss------------->  150.9379119873047\n",
      "Epoch: 167 Loss : 188.57569885253906\n",
      "training loss------------->  237.31622314453125\n",
      "Epoch: 168 Loss : 88.3929214477539\n",
      "training loss------------->  253.26803588867188\n",
      "Epoch: 169 Loss : 98.58038330078125\n",
      "training loss------------->  443.4754333496094\n",
      "Epoch: 170 Loss : 226.2509765625\n",
      "training loss------------->  252.0608673095703\n",
      "Epoch: 171 Loss : 79.10742950439453\n",
      "training loss------------->  328.75830078125\n",
      "Epoch: 172 Loss : 77.72370147705078\n",
      "training loss------------->  392.9498291015625\n",
      "Epoch: 173 Loss : 84.64714813232422\n",
      "training loss------------->  120.01021575927734\n",
      "Epoch: 174 Loss : 444.3910217285156\n",
      "training loss------------->  660.913330078125\n",
      "Epoch: 175 Loss : 514.23046875\n",
      "training loss------------->  429.9837951660156\n",
      "Epoch: 176 Loss : 200.89723205566406\n",
      "training loss------------->  415.9138488769531\n",
      "Epoch: 177 Loss : 242.79751586914062\n",
      "training loss------------->  339.4172668457031\n",
      "Epoch: 178 Loss : 896.78076171875\n",
      "training loss------------->  208.45046997070312\n",
      "Epoch: 179 Loss : 645.60693359375\n",
      "training loss------------->  706.6392822265625\n",
      "Epoch: 180 Loss : 110.68727111816406\n",
      "training loss------------->  393.4928894042969\n",
      "Epoch: 181 Loss : 413.6438293457031\n",
      "training loss------------->  782.0850830078125\n",
      "Epoch: 182 Loss : 494.39453125\n",
      "training loss------------->  522.0424194335938\n",
      "Saving Best Model with Accuracy:  7.896292209625244\n",
      "Epoch: 183 Loss : 7.896292209625244\n",
      "training loss------------->  218.73411560058594\n",
      "Epoch: 184 Loss : 227.57273864746094\n",
      "training loss------------->  790.609375\n",
      "Epoch: 185 Loss : 67.06769561767578\n",
      "training loss------------->  287.7856140136719\n",
      "Epoch: 186 Loss : 157.62001037597656\n",
      "training loss------------->  304.23468017578125\n",
      "Epoch: 187 Loss : 1011.4912109375\n",
      "training loss------------->  253.2648162841797\n",
      "Epoch: 188 Loss : 330.4750671386719\n",
      "training loss------------->  485.7374267578125\n",
      "Epoch: 189 Loss : 245.46636962890625\n",
      "training loss------------->  767.373291015625\n",
      "Epoch: 190 Loss : 426.6768493652344\n",
      "training loss------------->  795.8099975585938\n",
      "Epoch: 191 Loss : 1371.53759765625\n",
      "training loss------------->  432.93426513671875\n",
      "Epoch: 192 Loss : 130.3682403564453\n",
      "training loss------------->  518.3180541992188\n",
      "Epoch: 193 Loss : 389.8924865722656\n",
      "training loss------------->  203.41458129882812\n",
      "Epoch: 194 Loss : 157.07437133789062\n",
      "training loss------------->  335.07037353515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 Loss : 36.7437744140625\n",
      "training loss------------->  348.8717041015625\n",
      "Epoch: 196 Loss : 994.80419921875\n",
      "training loss------------->  325.95623779296875\n",
      "Epoch: 197 Loss : 362.7366027832031\n",
      "training loss------------->  503.6579895019531\n",
      "Epoch: 198 Loss : 816.0682983398438\n",
      "training loss------------->  487.5533752441406\n",
      "Epoch: 199 Loss : 600.02783203125\n",
      "training loss------------->  411.5770263671875\n",
      "Epoch: 200 Loss : 445.6448669433594\n",
      "training loss------------->  370.6314392089844\n",
      "Epoch: 201 Loss : 95.16404724121094\n",
      "training loss------------->  356.3995666503906\n",
      "Epoch: 202 Loss : 886.305419921875\n",
      "training loss------------->  464.9861755371094\n",
      "Epoch: 203 Loss : 213.59814453125\n",
      "training loss------------->  286.5016784667969\n",
      "Epoch: 204 Loss : 471.33837890625\n",
      "training loss------------->  596.8604736328125\n",
      "Epoch: 205 Loss : 334.82574462890625\n",
      "training loss------------->  161.908935546875\n",
      "Epoch: 206 Loss : 778.6909790039062\n",
      "training loss------------->  494.7234191894531\n",
      "Epoch: 207 Loss : 315.2733154296875\n",
      "training loss------------->  269.7067565917969\n",
      "Epoch: 208 Loss : 373.0309753417969\n",
      "training loss------------->  708.6018676757812\n",
      "Epoch: 209 Loss : 159.54652404785156\n",
      "training loss------------->  415.5006103515625\n",
      "Epoch: 210 Loss : 401.6181640625\n",
      "training loss------------->  607.2174072265625\n",
      "Epoch: 211 Loss : 515.1300659179688\n",
      "training loss------------->  690.031494140625\n",
      "Epoch: 212 Loss : 288.8512268066406\n",
      "training loss------------->  366.1876220703125\n",
      "Epoch: 213 Loss : 569.7481689453125\n",
      "training loss------------->  273.8103942871094\n",
      "Epoch: 214 Loss : 231.33851623535156\n",
      "training loss------------->  550.1534423828125\n",
      "Epoch: 215 Loss : 529.6669311523438\n",
      "training loss------------->  301.8383483886719\n",
      "Epoch: 216 Loss : 1213.2628173828125\n",
      "training loss------------->  568.5567626953125\n",
      "Epoch: 217 Loss : 176.20582580566406\n",
      "training loss------------->  540.8323364257812\n",
      "Epoch: 218 Loss : 227.71258544921875\n",
      "training loss------------->  372.759765625\n",
      "Epoch: 219 Loss : 10.897605895996094\n",
      "training loss------------->  570.5927734375\n",
      "Epoch: 220 Loss : 283.5735778808594\n",
      "training loss------------->  418.62030029296875\n",
      "Epoch: 221 Loss : 970.3696899414062\n",
      "training loss------------->  557.6885375976562\n",
      "Epoch: 222 Loss : 359.43060302734375\n",
      "training loss------------->  271.70855712890625\n",
      "Epoch: 223 Loss : 470.67022705078125\n",
      "training loss------------->  343.4269104003906\n",
      "Epoch: 224 Loss : 561.5126342773438\n",
      "training loss------------->  432.4295959472656\n",
      "Epoch: 225 Loss : 356.533935546875\n",
      "training loss------------->  408.4862365722656\n",
      "Epoch: 226 Loss : 354.37579345703125\n",
      "training loss------------->  306.5733642578125\n",
      "Epoch: 227 Loss : 373.3520812988281\n",
      "training loss------------->  388.4791259765625\n",
      "Epoch: 228 Loss : 808.2073364257812\n",
      "training loss------------->  99.78323364257812\n",
      "Epoch: 229 Loss : 206.40145874023438\n",
      "training loss------------->  448.40924072265625\n",
      "Epoch: 230 Loss : 523.5679931640625\n",
      "training loss------------->  392.3981628417969\n",
      "Epoch: 231 Loss : 527.6240234375\n",
      "training loss------------->  232.67662048339844\n",
      "Epoch: 232 Loss : 430.5696716308594\n",
      "training loss------------->  267.8730163574219\n",
      "Epoch: 233 Loss : 753.450439453125\n",
      "training loss------------->  251.33387756347656\n",
      "Epoch: 234 Loss : 26.474624633789062\n",
      "training loss------------->  70.95447540283203\n",
      "Epoch: 235 Loss : 645.5205688476562\n",
      "training loss------------->  428.58782958984375\n",
      "Epoch: 236 Loss : 56.94146728515625\n",
      "training loss------------->  504.3761291503906\n",
      "Epoch: 237 Loss : 835.3878784179688\n",
      "training loss------------->  489.9317932128906\n",
      "Epoch: 238 Loss : 1292.375\n",
      "training loss------------->  530.978271484375\n",
      "Epoch: 239 Loss : 244.02926635742188\n",
      "training loss------------->  190.93675231933594\n",
      "Epoch: 240 Loss : 563.1396484375\n",
      "training loss------------->  595.8678588867188\n",
      "Epoch: 241 Loss : 172.9644012451172\n",
      "training loss------------->  425.5965881347656\n",
      "Epoch: 242 Loss : 603.0549926757812\n",
      "training loss------------->  279.69927978515625\n",
      "Epoch: 243 Loss : 590.413818359375\n",
      "training loss------------->  150.54364013671875\n",
      "Epoch: 244 Loss : 413.1339111328125\n",
      "training loss------------->  534.6436157226562\n",
      "Epoch: 245 Loss : 81.07429504394531\n",
      "training loss------------->  607.2313842773438\n",
      "Epoch: 246 Loss : 524.7965698242188\n",
      "training loss------------->  819.5870971679688\n",
      "Epoch: 247 Loss : 356.64678955078125\n",
      "training loss------------->  572.7490844726562\n",
      "Epoch: 248 Loss : 117.5875244140625\n",
      "training loss------------->  347.8565673828125\n",
      "Epoch: 249 Loss : 394.3499755859375\n",
      "training loss------------->  581.1867065429688\n",
      "Epoch: 250 Loss : 484.0521240234375\n",
      "training loss------------->  172.96826171875\n",
      "Epoch: 251 Loss : 258.3892517089844\n",
      "training loss------------->  270.7521057128906\n",
      "Epoch: 252 Loss : 400.6242370605469\n",
      "training loss------------->  603.8538818359375\n",
      "Epoch: 253 Loss : 78.45416259765625\n",
      "training loss------------->  369.91217041015625\n",
      "Epoch: 254 Loss : 279.331298828125\n",
      "training loss------------->  463.3826904296875\n",
      "Epoch: 255 Loss : 471.72650146484375\n",
      "training loss------------->  124.648681640625\n",
      "Epoch: 256 Loss : 452.3405456542969\n",
      "training loss------------->  564.5953369140625\n",
      "Epoch: 257 Loss : 520.91650390625\n",
      "training loss------------->  559.3812866210938\n",
      "Epoch: 258 Loss : 381.9473876953125\n",
      "training loss------------->  593.4916381835938\n",
      "Epoch: 259 Loss : 498.42315673828125\n",
      "training loss------------->  248.11822509765625\n",
      "Epoch: 260 Loss : 901.5340576171875\n",
      "training loss------------->  479.39990234375\n",
      "Epoch: 261 Loss : 78.35022735595703\n",
      "training loss------------->  308.6301574707031\n",
      "Epoch: 262 Loss : 363.6419677734375\n",
      "training loss------------->  334.27215576171875\n",
      "Epoch: 263 Loss : 126.46211242675781\n",
      "training loss------------->  494.9787292480469\n",
      "Epoch: 264 Loss : 432.6934814453125\n",
      "training loss------------->  315.9964294433594\n",
      "Epoch: 265 Loss : 290.77130126953125\n",
      "training loss------------->  511.5115966796875\n",
      "Epoch: 266 Loss : 288.0215759277344\n",
      "training loss------------->  501.0265197753906\n",
      "Epoch: 267 Loss : 586.298828125\n",
      "training loss------------->  520.1780395507812\n",
      "Epoch: 268 Loss : 319.310546875\n",
      "training loss------------->  505.3221435546875\n",
      "Epoch: 269 Loss : 366.6724853515625\n",
      "training loss------------->  576.2332153320312\n",
      "Epoch: 270 Loss : 51.24174499511719\n",
      "training loss------------->  391.8194580078125\n",
      "Epoch: 271 Loss : 157.36517333984375\n",
      "training loss------------->  370.2547302246094\n",
      "Epoch: 272 Loss : 389.91558837890625\n",
      "training loss------------->  200.5945281982422\n",
      "Epoch: 273 Loss : 273.93194580078125\n",
      "training loss------------->  485.43939208984375\n",
      "Epoch: 274 Loss : 303.2637939453125\n",
      "training loss------------->  554.734619140625\n",
      "Epoch: 275 Loss : 188.6536102294922\n",
      "training loss------------->  465.7215576171875\n",
      "Epoch: 276 Loss : 817.5661010742188\n",
      "training loss------------->  578.231689453125\n",
      "Epoch: 277 Loss : 185.44586181640625\n",
      "training loss------------->  355.2448425292969\n",
      "Epoch: 278 Loss : 186.65631103515625\n",
      "training loss------------->  550.9564208984375\n",
      "Epoch: 279 Loss : 525.8824462890625\n",
      "training loss------------->  649.8946533203125\n",
      "Epoch: 280 Loss : 355.6683654785156\n",
      "training loss------------->  533.019775390625\n",
      "Epoch: 281 Loss : 56.26544189453125\n",
      "training loss------------->  465.6360168457031\n",
      "Epoch: 282 Loss : 769.7944946289062\n",
      "training loss------------->  563.7520751953125\n",
      "Epoch: 283 Loss : 639.0001831054688\n",
      "training loss------------->  433.006591796875\n",
      "Epoch: 284 Loss : 349.8362731933594\n",
      "training loss------------->  539.7578735351562\n",
      "Epoch: 285 Loss : 480.0147705078125\n",
      "training loss------------->  246.06521606445312\n",
      "Epoch: 286 Loss : 287.3436279296875\n",
      "training loss------------->  224.5912322998047\n",
      "Epoch: 287 Loss : 631.9765014648438\n",
      "training loss------------->  369.6441650390625\n",
      "Epoch: 288 Loss : 92.5670166015625\n",
      "training loss------------->  305.29327392578125\n",
      "Epoch: 289 Loss : 726.7453002929688\n",
      "training loss------------->  601.075439453125\n",
      "Epoch: 290 Loss : 484.8219909667969\n",
      "training loss------------->  373.8013000488281\n",
      "Epoch: 291 Loss : 1287.050048828125\n",
      "training loss------------->  324.8485107421875\n",
      "Epoch: 292 Loss : 684.300537109375\n",
      "training loss------------->  700.6857299804688\n",
      "Epoch: 293 Loss : 333.73394775390625\n",
      "training loss------------->  251.74278259277344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 Loss : 790.3533935546875\n",
      "training loss------------->  106.72123718261719\n",
      "Epoch: 295 Loss : 284.84979248046875\n",
      "training loss------------->  426.6549072265625\n",
      "Epoch: 296 Loss : 571.3209228515625\n",
      "training loss------------->  401.271240234375\n",
      "Epoch: 297 Loss : 673.4307250976562\n",
      "training loss------------->  198.83279418945312\n",
      "Epoch: 298 Loss : 187.96365356445312\n",
      "training loss------------->  368.2048645019531\n",
      "Epoch: 299 Loss : 1447.477783203125\n",
      "training loss------------->  383.4969177246094\n",
      "Epoch: 300 Loss : 57.714691162109375\n",
      "training loss------------->  329.8776550292969\n",
      "Epoch: 301 Loss : 155.32858276367188\n",
      "training loss------------->  435.53228759765625\n",
      "Epoch: 302 Loss : 177.65724182128906\n",
      "training loss------------->  305.2665100097656\n",
      "Epoch: 303 Loss : 354.0355529785156\n",
      "training loss------------->  352.23785400390625\n",
      "Epoch: 304 Loss : 452.9131164550781\n",
      "training loss------------->  181.85784912109375\n",
      "Epoch: 305 Loss : 338.8413391113281\n",
      "training loss------------->  508.2395935058594\n",
      "Epoch: 306 Loss : 1267.7237548828125\n",
      "training loss------------->  256.5997009277344\n",
      "Epoch: 307 Loss : 1214.8941650390625\n",
      "training loss------------->  574.8683471679688\n",
      "Epoch: 308 Loss : 704.5126953125\n",
      "training loss------------->  385.2286071777344\n",
      "Epoch: 309 Loss : 499.36492919921875\n",
      "training loss------------->  351.93426513671875\n",
      "Epoch: 310 Loss : 109.31756591796875\n",
      "training loss------------->  303.016357421875\n",
      "Epoch: 311 Loss : 680.291259765625\n",
      "training loss------------->  484.9708557128906\n",
      "Epoch: 312 Loss : 554.3744506835938\n",
      "training loss------------->  786.34423828125\n",
      "Epoch: 313 Loss : 270.7045593261719\n",
      "training loss------------->  399.99237060546875\n",
      "Epoch: 314 Loss : 474.8822937011719\n",
      "training loss------------->  591.614013671875\n",
      "Epoch: 315 Loss : 122.64678955078125\n",
      "training loss------------->  460.7357177734375\n",
      "Epoch: 316 Loss : 856.5306396484375\n",
      "training loss------------->  290.5793151855469\n",
      "Epoch: 317 Loss : 250.14944458007812\n",
      "training loss------------->  215.8007049560547\n",
      "Epoch: 318 Loss : 482.0807189941406\n",
      "training loss------------->  315.9265441894531\n",
      "Epoch: 319 Loss : 169.05831909179688\n",
      "training loss------------->  267.2728271484375\n",
      "Epoch: 320 Loss : 155.92471313476562\n",
      "training loss------------->  248.9397735595703\n",
      "Epoch: 321 Loss : 176.58767700195312\n",
      "training loss------------->  595.927490234375\n",
      "Epoch: 322 Loss : 412.2383117675781\n",
      "training loss------------->  590.1860961914062\n",
      "Epoch: 323 Loss : 902.2800903320312\n",
      "training loss------------->  461.7596435546875\n",
      "Epoch: 324 Loss : 163.83692932128906\n",
      "training loss------------->  112.58255004882812\n",
      "Epoch: 325 Loss : 298.7274169921875\n",
      "training loss------------->  370.9220275878906\n",
      "Epoch: 326 Loss : 222.26573181152344\n",
      "training loss------------->  504.1822814941406\n",
      "Epoch: 327 Loss : 687.9620971679688\n",
      "training loss------------->  682.4119262695312\n",
      "Epoch: 328 Loss : 413.94305419921875\n",
      "training loss------------->  329.6833801269531\n",
      "Epoch: 329 Loss : 519.9761352539062\n",
      "training loss------------->  201.430419921875\n",
      "Epoch: 330 Loss : 358.03558349609375\n",
      "training loss------------->  597.5342407226562\n",
      "Epoch: 331 Loss : 277.9664611816406\n",
      "training loss------------->  474.7020263671875\n",
      "Epoch: 332 Loss : 221.01754760742188\n",
      "training loss------------->  448.7371520996094\n",
      "Epoch: 333 Loss : 558.2269287109375\n",
      "training loss------------->  440.1268005371094\n",
      "Epoch: 334 Loss : 342.4087219238281\n",
      "training loss------------->  465.20599365234375\n",
      "Epoch: 335 Loss : 370.73876953125\n",
      "training loss------------->  296.93988037109375\n",
      "Epoch: 336 Loss : 60.0918083190918\n",
      "training loss------------->  326.3489990234375\n",
      "Epoch: 337 Loss : 382.6264953613281\n",
      "training loss------------->  339.46563720703125\n",
      "Epoch: 338 Loss : 599.9931640625\n",
      "training loss------------->  592.2262573242188\n",
      "Epoch: 339 Loss : 295.98162841796875\n",
      "training loss------------->  369.28424072265625\n",
      "Epoch: 340 Loss : 96.12260437011719\n",
      "training loss------------->  364.2532958984375\n",
      "Epoch: 341 Loss : 925.1535034179688\n",
      "training loss------------->  359.445068359375\n",
      "Epoch: 342 Loss : 620.5335083007812\n",
      "training loss------------->  236.70068359375\n",
      "Epoch: 343 Loss : 604.293212890625\n",
      "training loss------------->  408.1334228515625\n",
      "Epoch: 344 Loss : 111.02223205566406\n",
      "training loss------------->  245.16786193847656\n",
      "Epoch: 345 Loss : 266.5267639160156\n",
      "training loss------------->  512.1712036132812\n",
      "Epoch: 346 Loss : 2442.36181640625\n",
      "training loss------------->  914.257080078125\n",
      "Epoch: 347 Loss : 40.68362808227539\n",
      "training loss------------->  275.1545715332031\n",
      "Epoch: 348 Loss : 381.1217346191406\n",
      "training loss------------->  394.4415588378906\n",
      "Epoch: 349 Loss : 759.756103515625\n",
      "training loss------------->  463.96734619140625\n",
      "Epoch: 350 Loss : 126.71183013916016\n",
      "training loss------------->  339.3021240234375\n",
      "Epoch: 351 Loss : 697.7115478515625\n",
      "training loss------------->  273.14935302734375\n",
      "Epoch: 352 Loss : 851.0277099609375\n",
      "training loss------------->  612.2133178710938\n",
      "Epoch: 353 Loss : 248.95730590820312\n",
      "training loss------------->  256.24542236328125\n",
      "Epoch: 354 Loss : 87.90628051757812\n",
      "training loss------------->  466.6296691894531\n",
      "Epoch: 355 Loss : 388.4466552734375\n",
      "training loss------------->  643.0648193359375\n",
      "Epoch: 356 Loss : 144.0408172607422\n",
      "training loss------------->  408.3229675292969\n",
      "Epoch: 357 Loss : 86.21204376220703\n",
      "training loss------------->  227.8926544189453\n",
      "Epoch: 358 Loss : 591.7039794921875\n",
      "training loss------------->  345.2312927246094\n",
      "Epoch: 359 Loss : 448.3785705566406\n",
      "training loss------------->  282.80029296875\n",
      "Epoch: 360 Loss : 640.4910278320312\n",
      "training loss------------->  740.374267578125\n",
      "Epoch: 361 Loss : 41.81856918334961\n",
      "training loss------------->  441.4144592285156\n",
      "Epoch: 362 Loss : 249.48277282714844\n",
      "training loss------------->  466.2746887207031\n",
      "Epoch: 363 Loss : 179.22308349609375\n",
      "training loss------------->  551.132080078125\n",
      "Epoch: 364 Loss : 779.745849609375\n",
      "training loss------------->  362.4349365234375\n",
      "Epoch: 365 Loss : 199.1964111328125\n",
      "training loss------------->  152.06610107421875\n",
      "Epoch: 366 Loss : 214.7602996826172\n",
      "training loss------------->  481.3146667480469\n",
      "Epoch: 367 Loss : 303.5013732910156\n",
      "training loss------------->  559.0380249023438\n",
      "Epoch: 368 Loss : 163.64584350585938\n",
      "training loss------------->  391.9102783203125\n",
      "Epoch: 369 Loss : 39.10978317260742\n",
      "training loss------------->  763.958984375\n",
      "Epoch: 370 Loss : 90.38813781738281\n",
      "training loss------------->  564.4900512695312\n",
      "Epoch: 371 Loss : 765.2813720703125\n",
      "training loss------------->  467.7291259765625\n",
      "Epoch: 372 Loss : 305.26611328125\n",
      "training loss------------->  204.86614990234375\n",
      "Epoch: 373 Loss : 409.5724182128906\n",
      "training loss------------->  458.7144775390625\n",
      "Epoch: 374 Loss : 347.85498046875\n",
      "training loss------------->  373.29217529296875\n",
      "Epoch: 375 Loss : 780.0692138671875\n",
      "training loss------------->  474.6282653808594\n",
      "Epoch: 376 Loss : 258.55731201171875\n",
      "training loss------------->  510.42431640625\n",
      "Epoch: 377 Loss : 218.87384033203125\n",
      "training loss------------->  118.24018096923828\n",
      "Epoch: 378 Loss : 134.0322265625\n",
      "training loss------------->  423.35064697265625\n",
      "Epoch: 379 Loss : 164.05221557617188\n",
      "training loss------------->  459.4783020019531\n",
      "Epoch: 380 Loss : 1328.224609375\n",
      "training loss------------->  331.1202392578125\n",
      "Epoch: 381 Loss : 87.5095443725586\n",
      "training loss------------->  194.57432556152344\n",
      "Epoch: 382 Loss : 605.1953125\n",
      "training loss------------->  451.3193664550781\n",
      "Epoch: 383 Loss : 714.0514526367188\n",
      "training loss------------->  287.659423828125\n",
      "Epoch: 384 Loss : 494.24652099609375\n",
      "training loss------------->  575.5160522460938\n",
      "Epoch: 385 Loss : 165.11053466796875\n",
      "training loss------------->  270.3052978515625\n",
      "Epoch: 386 Loss : 258.91943359375\n",
      "training loss------------->  495.0769958496094\n",
      "Epoch: 387 Loss : 715.453857421875\n",
      "training loss------------->  269.8411865234375\n",
      "Epoch: 388 Loss : 232.90646362304688\n",
      "training loss------------->  667.9519653320312\n",
      "Epoch: 389 Loss : 558.0045776367188\n",
      "training loss------------->  484.94146728515625\n",
      "Epoch: 390 Loss : 433.23187255859375\n",
      "training loss------------->  335.4940185546875\n",
      "Epoch: 391 Loss : 691.912353515625\n",
      "training loss------------->  465.90423583984375\n",
      "Epoch: 392 Loss : 355.0613098144531\n",
      "training loss------------->  250.488037109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 393 Loss : 1294.91552734375\n",
      "training loss------------->  780.644287109375\n",
      "Epoch: 394 Loss : 380.1551513671875\n",
      "training loss------------->  563.474365234375\n",
      "Epoch: 395 Loss : 217.04470825195312\n",
      "training loss------------->  476.1794738769531\n",
      "Epoch: 396 Loss : 243.54656982421875\n",
      "training loss------------->  228.01136779785156\n",
      "Epoch: 397 Loss : 393.7717590332031\n",
      "training loss------------->  285.46160888671875\n",
      "Epoch: 398 Loss : 93.0206069946289\n",
      "training loss------------->  515.6854858398438\n",
      "Epoch: 399 Loss : 386.27667236328125\n",
      "training loss------------->  384.6395568847656\n",
      "Epoch: 400 Loss : 274.69842529296875\n",
      "training loss------------->  481.2595520019531\n",
      "Saving Best Model with Accuracy:  5.114650726318359\n",
      "Epoch: 401 Loss : 5.114650726318359\n",
      "training loss------------->  278.3155822753906\n",
      "Epoch: 402 Loss : 618.2691650390625\n",
      "training loss------------->  162.97557067871094\n",
      "Epoch: 403 Loss : 708.0152587890625\n",
      "training loss------------->  176.07708740234375\n",
      "Epoch: 404 Loss : 636.3187866210938\n",
      "training loss------------->  416.0533752441406\n",
      "Epoch: 405 Loss : 365.83099365234375\n",
      "training loss------------->  470.7449951171875\n",
      "Epoch: 406 Loss : 883.6040649414062\n",
      "training loss------------->  331.1601867675781\n",
      "Epoch: 407 Loss : 528.3560180664062\n",
      "training loss------------->  377.25732421875\n",
      "Epoch: 408 Loss : 447.7873229980469\n",
      "training loss------------->  366.1294860839844\n",
      "Epoch: 409 Loss : 1395.2626953125\n",
      "training loss------------->  256.78607177734375\n",
      "Epoch: 410 Loss : 284.3413391113281\n",
      "training loss------------->  1046.264404296875\n",
      "Epoch: 411 Loss : 29.772705078125\n",
      "training loss------------->  95.7365951538086\n",
      "Epoch: 412 Loss : 554.0986938476562\n",
      "training loss------------->  452.26556396484375\n",
      "Epoch: 413 Loss : 637.6658935546875\n",
      "training loss------------->  187.49044799804688\n",
      "Epoch: 414 Loss : 598.7939453125\n",
      "training loss------------->  202.08938598632812\n",
      "Epoch: 415 Loss : 148.2680206298828\n",
      "training loss------------->  762.6570434570312\n",
      "Epoch: 416 Loss : 499.81158447265625\n",
      "training loss------------->  322.4448547363281\n",
      "Epoch: 417 Loss : 1339.35986328125\n",
      "training loss------------->  593.0148315429688\n",
      "Epoch: 418 Loss : 342.0885009765625\n",
      "training loss------------->  143.99673461914062\n",
      "Epoch: 419 Loss : 540.3787231445312\n",
      "training loss------------->  126.43141174316406\n",
      "Epoch: 420 Loss : 68.21048736572266\n",
      "training loss------------->  232.24505615234375\n",
      "Epoch: 421 Loss : 391.58319091796875\n",
      "training loss------------->  139.5983428955078\n",
      "Epoch: 422 Loss : 154.78163146972656\n",
      "training loss------------->  198.64280700683594\n",
      "Epoch: 423 Loss : 258.1731262207031\n",
      "training loss------------->  633.0335083007812\n",
      "Epoch: 424 Loss : 387.28607177734375\n",
      "training loss------------->  512.8272094726562\n",
      "Epoch: 425 Loss : 520.581787109375\n",
      "training loss------------->  671.846435546875\n",
      "Epoch: 426 Loss : 1012.2822875976562\n",
      "training loss------------->  315.3521728515625\n",
      "Epoch: 427 Loss : 231.45982360839844\n",
      "training loss------------->  531.598876953125\n",
      "Epoch: 428 Loss : 482.1299743652344\n",
      "training loss------------->  275.25299072265625\n",
      "Epoch: 429 Loss : 391.82958984375\n",
      "training loss------------->  462.5779113769531\n",
      "Epoch: 430 Loss : 355.37408447265625\n",
      "training loss------------->  407.85516357421875\n",
      "Epoch: 431 Loss : 706.77490234375\n",
      "training loss------------->  344.20782470703125\n",
      "Epoch: 432 Loss : 926.5780639648438\n",
      "training loss------------->  276.76788330078125\n",
      "Epoch: 433 Loss : 1434.231201171875\n",
      "training loss------------->  637.6734008789062\n",
      "Epoch: 434 Loss : 144.73828125\n",
      "training loss------------->  649.8176879882812\n",
      "Epoch: 435 Loss : 106.98381042480469\n",
      "training loss------------->  525.919189453125\n",
      "Epoch: 436 Loss : 446.7955627441406\n",
      "training loss------------->  370.7992858886719\n",
      "Epoch: 437 Loss : 426.9183044433594\n",
      "training loss------------->  443.0421142578125\n",
      "Epoch: 438 Loss : 1178.858642578125\n",
      "training loss------------->  278.0504150390625\n",
      "Epoch: 439 Loss : 678.1512451171875\n",
      "training loss------------->  317.3833312988281\n",
      "Epoch: 440 Loss : 603.84326171875\n",
      "training loss------------->  565.182861328125\n",
      "Epoch: 441 Loss : 418.0542907714844\n",
      "training loss------------->  241.85194396972656\n",
      "Epoch: 442 Loss : 549.495361328125\n",
      "training loss------------->  255.88023376464844\n",
      "Epoch: 443 Loss : 438.8018798828125\n",
      "training loss------------->  488.22845458984375\n",
      "Epoch: 444 Loss : 74.1373519897461\n",
      "training loss------------->  351.5223388671875\n",
      "Epoch: 445 Loss : 722.2992553710938\n",
      "training loss------------->  457.7706298828125\n",
      "Epoch: 446 Loss : 192.56268310546875\n",
      "training loss------------->  412.3819580078125\n",
      "Epoch: 447 Loss : 34.642921447753906\n",
      "training loss------------->  189.94210815429688\n",
      "Epoch: 448 Loss : 179.68109130859375\n",
      "training loss------------->  260.920654296875\n",
      "Epoch: 449 Loss : 785.0841064453125\n",
      "training loss------------->  342.06610107421875\n",
      "Epoch: 450 Loss : 473.4889831542969\n",
      "training loss------------->  240.71002197265625\n",
      "Epoch: 451 Loss : 134.60723876953125\n",
      "training loss------------->  641.1807250976562\n",
      "Epoch: 452 Loss : 477.21502685546875\n",
      "training loss------------->  402.9749450683594\n",
      "Epoch: 453 Loss : 301.9088439941406\n",
      "training loss------------->  374.9147644042969\n",
      "Epoch: 454 Loss : 334.71630859375\n",
      "training loss------------->  326.5361328125\n",
      "Epoch: 455 Loss : 149.19947814941406\n",
      "training loss------------->  202.68186950683594\n",
      "Epoch: 456 Loss : 194.14955139160156\n",
      "training loss------------->  278.8260803222656\n",
      "Epoch: 457 Loss : 177.73892211914062\n",
      "training loss------------->  323.37945556640625\n",
      "Epoch: 458 Loss : 579.3902587890625\n",
      "training loss------------->  438.16290283203125\n",
      "Epoch: 459 Loss : 146.52452087402344\n",
      "training loss------------->  375.89080810546875\n",
      "Epoch: 460 Loss : 167.30340576171875\n",
      "training loss------------->  300.1955871582031\n",
      "Epoch: 461 Loss : 372.03692626953125\n",
      "training loss------------->  500.9189453125\n",
      "Epoch: 462 Loss : 49.81232833862305\n",
      "training loss------------->  203.32579040527344\n",
      "Epoch: 463 Loss : 279.0075378417969\n",
      "training loss------------->  715.0506591796875\n",
      "Epoch: 464 Loss : 729.7694702148438\n",
      "training loss------------->  310.7269287109375\n",
      "Epoch: 465 Loss : 651.5694580078125\n",
      "training loss------------->  292.8857116699219\n",
      "Epoch: 466 Loss : 28.70637321472168\n",
      "training loss------------->  308.9462585449219\n",
      "Epoch: 467 Loss : 1354.2510986328125\n",
      "training loss------------->  186.7509002685547\n",
      "Epoch: 468 Loss : 473.6391296386719\n",
      "training loss------------->  593.4796142578125\n",
      "Epoch: 469 Loss : 548.7960205078125\n",
      "training loss------------->  463.4371337890625\n",
      "Epoch: 470 Loss : 277.0567626953125\n",
      "training loss------------->  323.8802795410156\n",
      "Epoch: 471 Loss : 310.28338623046875\n",
      "training loss------------->  270.0130310058594\n",
      "Epoch: 472 Loss : 1142.48974609375\n",
      "training loss------------->  429.57440185546875\n",
      "Epoch: 473 Loss : 75.88938903808594\n",
      "training loss------------->  586.908447265625\n",
      "Epoch: 474 Loss : 751.6635131835938\n",
      "training loss------------->  851.9203491210938\n",
      "Epoch: 475 Loss : 446.9695739746094\n",
      "training loss------------->  297.7704772949219\n",
      "Epoch: 476 Loss : 658.1636352539062\n",
      "training loss------------->  306.9439697265625\n",
      "Epoch: 477 Loss : 268.53851318359375\n",
      "training loss------------->  353.29510498046875\n",
      "Epoch: 478 Loss : 281.1083984375\n",
      "training loss------------->  176.55503845214844\n",
      "Epoch: 479 Loss : 180.8316650390625\n",
      "training loss------------->  628.1849365234375\n",
      "Epoch: 480 Loss : 416.70758056640625\n",
      "training loss------------->  450.5450134277344\n",
      "Epoch: 481 Loss : 344.6979675292969\n",
      "training loss------------->  361.9330749511719\n",
      "Epoch: 482 Loss : 134.9781951904297\n",
      "training loss------------->  354.6505126953125\n",
      "Epoch: 483 Loss : 393.91534423828125\n",
      "training loss------------->  369.2354736328125\n",
      "Epoch: 484 Loss : 189.06741333007812\n",
      "training loss------------->  282.5947265625\n",
      "Epoch: 485 Loss : 707.21484375\n",
      "training loss------------->  237.2238006591797\n",
      "Epoch: 486 Loss : 199.65573120117188\n",
      "training loss------------->  654.3865966796875\n",
      "Epoch: 487 Loss : 229.89901733398438\n",
      "training loss------------->  399.2830505371094\n",
      "Epoch: 488 Loss : 258.9617004394531\n",
      "training loss------------->  499.7428283691406\n",
      "Epoch: 489 Loss : 694.236572265625\n",
      "training loss------------->  193.09092712402344\n",
      "Epoch: 490 Loss : 387.7811279296875\n",
      "training loss------------->  335.34771728515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 491 Loss : 160.6290740966797\n",
      "training loss------------->  176.636962890625\n",
      "Epoch: 492 Loss : 344.26763916015625\n",
      "training loss------------->  443.3307800292969\n",
      "Epoch: 493 Loss : 516.4324340820312\n",
      "training loss------------->  204.56805419921875\n",
      "Epoch: 494 Loss : 360.9331359863281\n",
      "training loss------------->  497.9903564453125\n",
      "Epoch: 495 Loss : 496.49822998046875\n",
      "training loss------------->  711.9371948242188\n",
      "Epoch: 496 Loss : 468.5911865234375\n",
      "training loss------------->  314.0248107910156\n",
      "Epoch: 497 Loss : 573.840576171875\n",
      "training loss------------->  317.13330078125\n",
      "Epoch: 498 Loss : 53.793190002441406\n",
      "training loss------------->  570.1721801757812\n",
      "Epoch: 499 Loss : 322.03759765625\n",
      "training loss------------->  172.11671447753906\n",
      "Epoch: 500 Loss : 370.03802490234375\n",
      "training loss------------->  576.1172485351562\n",
      "Epoch: 501 Loss : 758.6652221679688\n",
      "training loss------------->  587.2363891601562\n",
      "Epoch: 502 Loss : 621.7037963867188\n",
      "training loss------------->  286.2569580078125\n",
      "Epoch: 503 Loss : 349.9114990234375\n",
      "training loss------------->  486.3678283691406\n",
      "Epoch: 504 Loss : 549.0651245117188\n",
      "training loss------------->  503.1251525878906\n",
      "Epoch: 505 Loss : 363.62335205078125\n",
      "training loss------------->  503.3720703125\n",
      "Epoch: 506 Loss : 136.63873291015625\n",
      "training loss------------->  301.8008117675781\n",
      "Epoch: 507 Loss : 1541.514892578125\n",
      "training loss------------->  370.8001403808594\n",
      "Epoch: 508 Loss : 118.7460708618164\n",
      "training loss------------->  307.1874694824219\n",
      "Epoch: 509 Loss : 126.40983581542969\n",
      "training loss------------->  279.7999267578125\n",
      "Epoch: 510 Loss : 173.62567138671875\n",
      "training loss------------->  412.2850646972656\n",
      "Epoch: 511 Loss : 901.2477416992188\n",
      "training loss------------->  226.49905395507812\n",
      "Epoch: 512 Loss : 132.91741943359375\n",
      "training loss------------->  503.7606201171875\n",
      "Epoch: 513 Loss : 30.351341247558594\n",
      "training loss------------->  205.00521850585938\n",
      "Epoch: 514 Loss : 58.50734329223633\n",
      "training loss------------->  156.10833740234375\n",
      "Epoch: 515 Loss : 470.04669189453125\n",
      "training loss------------->  286.38568115234375\n",
      "Epoch: 516 Loss : 79.7201156616211\n",
      "training loss------------->  748.1088256835938\n",
      "Epoch: 517 Loss : 79.6288070678711\n",
      "training loss------------->  398.6249084472656\n",
      "Epoch: 518 Loss : 384.70135498046875\n",
      "training loss------------->  265.1526794433594\n",
      "Epoch: 519 Loss : 406.86358642578125\n",
      "training loss------------->  186.74786376953125\n",
      "Epoch: 520 Loss : 671.336181640625\n",
      "training loss------------->  370.40960693359375\n",
      "Epoch: 521 Loss : 598.8447265625\n",
      "training loss------------->  314.7540588378906\n",
      "Epoch: 522 Loss : 85.33306884765625\n",
      "training loss------------->  263.47247314453125\n",
      "Epoch: 523 Loss : 889.3557739257812\n",
      "training loss------------->  539.9617919921875\n",
      "Epoch: 524 Loss : 478.14739990234375\n",
      "training loss------------->  382.5232849121094\n",
      "Epoch: 525 Loss : 456.79766845703125\n",
      "training loss------------->  254.35511779785156\n",
      "Epoch: 526 Loss : 1031.60107421875\n",
      "training loss------------->  199.90618896484375\n",
      "Epoch: 527 Loss : 580.8499755859375\n",
      "training loss------------->  272.4434814453125\n",
      "Epoch: 528 Loss : 806.6031494140625\n",
      "training loss------------->  765.8896484375\n",
      "Epoch: 529 Loss : 766.3368530273438\n",
      "training loss------------->  234.9620819091797\n",
      "Epoch: 530 Loss : 753.209228515625\n",
      "training loss------------->  389.7422790527344\n",
      "Epoch: 531 Loss : 407.24371337890625\n",
      "training loss------------->  608.857177734375\n",
      "Epoch: 532 Loss : 464.0667419433594\n",
      "training loss------------->  155.05511474609375\n",
      "Epoch: 533 Loss : 317.2359313964844\n",
      "training loss------------->  493.32379150390625\n",
      "Epoch: 534 Loss : 200.56500244140625\n",
      "training loss------------->  249.99822998046875\n",
      "Epoch: 535 Loss : 220.20762634277344\n",
      "training loss------------->  487.6896667480469\n",
      "Epoch: 536 Loss : 992.11181640625\n",
      "training loss------------->  323.6051025390625\n",
      "Epoch: 537 Loss : 201.81918334960938\n",
      "training loss------------->  529.8208618164062\n",
      "Epoch: 538 Loss : 265.0167236328125\n",
      "training loss------------->  319.64056396484375\n",
      "Epoch: 539 Loss : 764.542724609375\n",
      "training loss------------->  136.1137237548828\n",
      "Epoch: 540 Loss : 568.6018676757812\n",
      "training loss------------->  452.11907958984375\n",
      "Epoch: 541 Loss : 415.0271911621094\n",
      "training loss------------->  321.7731018066406\n",
      "Epoch: 542 Loss : 146.74119567871094\n",
      "training loss------------->  370.4785461425781\n",
      "Epoch: 543 Loss : 237.97923278808594\n",
      "training loss------------->  248.34361267089844\n",
      "Epoch: 544 Loss : 148.2475128173828\n",
      "training loss------------->  426.88818359375\n",
      "Epoch: 545 Loss : 103.70600128173828\n",
      "training loss------------->  721.951416015625\n",
      "Epoch: 546 Loss : 419.5771789550781\n",
      "training loss------------->  234.50875854492188\n",
      "Epoch: 547 Loss : 271.84716796875\n",
      "training loss------------->  396.4649658203125\n",
      "Epoch: 548 Loss : 499.6142883300781\n",
      "training loss------------->  180.4413299560547\n",
      "Epoch: 549 Loss : 674.4873046875\n",
      "training loss------------->  366.7748107910156\n",
      "Epoch: 550 Loss : 248.27041625976562\n",
      "training loss------------->  335.7658996582031\n",
      "Epoch: 551 Loss : 27.293676376342773\n",
      "training loss------------->  579.5941772460938\n",
      "Epoch: 552 Loss : 432.6727294921875\n",
      "training loss------------->  271.5166015625\n",
      "Epoch: 553 Loss : 952.5618896484375\n",
      "training loss------------->  285.6898498535156\n",
      "Epoch: 554 Loss : 147.32150268554688\n",
      "training loss------------->  387.5386962890625\n",
      "Epoch: 555 Loss : 286.0615539550781\n",
      "training loss------------->  614.7363891601562\n",
      "Epoch: 556 Loss : 317.4734191894531\n",
      "training loss------------->  154.42286682128906\n",
      "Epoch: 557 Loss : 821.8997802734375\n",
      "training loss------------->  780.8236694335938\n",
      "Epoch: 558 Loss : 240.180908203125\n",
      "training loss------------->  278.8583984375\n",
      "Epoch: 559 Loss : 852.3565673828125\n",
      "training loss------------->  202.0513153076172\n",
      "Epoch: 560 Loss : 94.37886047363281\n",
      "training loss------------->  421.62744140625\n",
      "Epoch: 561 Loss : 95.99634552001953\n",
      "training loss------------->  493.1795349121094\n",
      "Epoch: 562 Loss : 980.4109497070312\n",
      "training loss------------->  753.600830078125\n",
      "Epoch: 563 Loss : 457.2972106933594\n",
      "training loss------------->  474.6833801269531\n",
      "Epoch: 564 Loss : 377.98443603515625\n",
      "training loss------------->  409.64276123046875\n",
      "Epoch: 565 Loss : 349.15130615234375\n",
      "training loss------------->  335.8012390136719\n",
      "Epoch: 566 Loss : 506.84967041015625\n",
      "training loss------------->  263.6549072265625\n",
      "Epoch: 567 Loss : 237.9887237548828\n",
      "training loss------------->  186.09524536132812\n",
      "Epoch: 568 Loss : 815.4816284179688\n",
      "training loss------------->  834.5562133789062\n",
      "Epoch: 569 Loss : 923.2386474609375\n",
      "training loss------------->  460.34429931640625\n",
      "Epoch: 570 Loss : 131.96905517578125\n",
      "training loss------------->  307.23785400390625\n",
      "Epoch: 571 Loss : 844.4501953125\n",
      "training loss------------->  438.66156005859375\n",
      "Epoch: 572 Loss : 767.8435668945312\n",
      "training loss------------->  553.1206665039062\n",
      "Epoch: 573 Loss : 87.52538299560547\n",
      "training loss------------->  243.68370056152344\n",
      "Epoch: 574 Loss : 466.37158203125\n",
      "training loss------------->  408.4556579589844\n",
      "Epoch: 575 Loss : 15.82760238647461\n",
      "training loss------------->  370.47467041015625\n",
      "Epoch: 576 Loss : 343.02642822265625\n",
      "training loss------------->  263.4333190917969\n",
      "Epoch: 577 Loss : 580.2962646484375\n",
      "training loss------------->  638.6851806640625\n",
      "Epoch: 578 Loss : 685.4868774414062\n",
      "training loss------------->  526.82763671875\n",
      "Epoch: 579 Loss : 482.39959716796875\n",
      "training loss------------->  481.6496276855469\n",
      "Epoch: 580 Loss : 241.34121704101562\n",
      "training loss------------->  300.9383544921875\n",
      "Epoch: 581 Loss : 585.54736328125\n",
      "training loss------------->  240.4986114501953\n",
      "Epoch: 582 Loss : 1234.3946533203125\n",
      "training loss------------->  319.63983154296875\n",
      "Epoch: 583 Loss : 391.781005859375\n",
      "training loss------------->  360.62396240234375\n",
      "Epoch: 584 Loss : 340.9371643066406\n",
      "training loss------------->  287.1789245605469\n",
      "Epoch: 585 Loss : 470.1419372558594\n",
      "training loss------------->  264.4876708984375\n",
      "Epoch: 586 Loss : 412.46466064453125\n",
      "training loss------------->  286.1179504394531\n",
      "Epoch: 587 Loss : 542.1243286132812\n",
      "training loss------------->  520.079345703125\n",
      "Epoch: 588 Loss : 897.8778686523438\n",
      "training loss------------->  476.6820068359375\n",
      "Epoch: 589 Loss : 311.1902770996094\n",
      "training loss------------->  179.91921997070312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 590 Loss : 582.9860229492188\n",
      "training loss------------->  606.5451049804688\n",
      "Epoch: 591 Loss : 385.9129333496094\n",
      "training loss------------->  254.77398681640625\n",
      "Epoch: 592 Loss : 581.50927734375\n",
      "training loss------------->  249.5474395751953\n",
      "Epoch: 593 Loss : 154.3114471435547\n",
      "training loss------------->  511.1145935058594\n",
      "Epoch: 594 Loss : 174.91781616210938\n",
      "training loss------------->  157.16378784179688\n",
      "Epoch: 595 Loss : 173.14089965820312\n",
      "training loss------------->  441.09234619140625\n",
      "Epoch: 596 Loss : 237.1745147705078\n",
      "training loss------------->  375.04522705078125\n",
      "Epoch: 597 Loss : 768.1890869140625\n",
      "training loss------------->  335.0718688964844\n",
      "Epoch: 598 Loss : 1026.394775390625\n",
      "training loss------------->  346.6227111816406\n",
      "Epoch: 599 Loss : 602.4208984375\n",
      "training loss------------->  329.1770935058594\n",
      "Epoch: 600 Loss : 416.5975646972656\n",
      "training loss------------->  315.8126525878906\n",
      "Epoch: 601 Loss : 879.3049926757812\n",
      "training loss------------->  352.04388427734375\n",
      "Epoch: 602 Loss : 447.65338134765625\n",
      "training loss------------->  426.6929626464844\n",
      "Epoch: 603 Loss : 369.0168762207031\n",
      "training loss------------->  375.1286926269531\n",
      "Epoch: 604 Loss : 240.61766052246094\n",
      "training loss------------->  245.97244262695312\n",
      "Epoch: 605 Loss : 365.7174377441406\n",
      "training loss------------->  353.4112854003906\n",
      "Epoch: 606 Loss : 220.8828887939453\n",
      "training loss------------->  594.9991455078125\n",
      "Epoch: 607 Loss : 126.54029083251953\n",
      "training loss------------->  357.38958740234375\n",
      "Epoch: 608 Loss : 559.0965576171875\n",
      "training loss------------->  483.4006652832031\n",
      "Epoch: 609 Loss : 86.76241302490234\n",
      "training loss------------->  344.2951354980469\n",
      "Epoch: 610 Loss : 391.3860168457031\n",
      "training loss------------->  608.45068359375\n",
      "Epoch: 611 Loss : 308.64984130859375\n",
      "training loss------------->  273.0561218261719\n",
      "Epoch: 612 Loss : 1367.1846923828125\n",
      "training loss------------->  409.1043701171875\n",
      "Epoch: 613 Loss : 597.0438842773438\n",
      "training loss------------->  351.9524841308594\n",
      "Epoch: 614 Loss : 946.8145751953125\n",
      "training loss------------->  276.1968688964844\n",
      "Epoch: 615 Loss : 65.52628326416016\n",
      "training loss------------->  270.7286682128906\n",
      "Epoch: 616 Loss : 595.4208984375\n",
      "training loss------------->  505.1287841796875\n",
      "Epoch: 617 Loss : 53.47661590576172\n",
      "training loss------------->  312.0412902832031\n",
      "Epoch: 618 Loss : 677.5225219726562\n",
      "training loss------------->  246.85023498535156\n",
      "Epoch: 619 Loss : 192.78407287597656\n",
      "training loss------------->  220.38925170898438\n",
      "Epoch: 620 Loss : 367.96221923828125\n",
      "training loss------------->  353.4247131347656\n",
      "Epoch: 621 Loss : 505.00531005859375\n",
      "training loss------------->  294.9205017089844\n",
      "Epoch: 622 Loss : 191.82818603515625\n",
      "training loss------------->  189.8360137939453\n",
      "Epoch: 623 Loss : 135.74081420898438\n",
      "training loss------------->  431.22198486328125\n",
      "Epoch: 624 Loss : 550.35009765625\n",
      "training loss------------->  319.1256408691406\n",
      "Epoch: 625 Loss : 254.52108764648438\n",
      "training loss------------->  170.38330078125\n",
      "Epoch: 626 Loss : 744.1114501953125\n",
      "training loss------------->  594.0592651367188\n",
      "Epoch: 627 Loss : 143.55593872070312\n",
      "training loss------------->  439.2371826171875\n",
      "Epoch: 628 Loss : 633.5833129882812\n",
      "training loss------------->  556.0205078125\n",
      "Epoch: 629 Loss : 297.42974853515625\n",
      "training loss------------->  246.66075134277344\n",
      "Epoch: 630 Loss : 245.38858032226562\n",
      "training loss------------->  738.0971069335938\n",
      "Epoch: 631 Loss : 848.32080078125\n",
      "training loss------------->  562.1600341796875\n",
      "Epoch: 632 Loss : 29.11495590209961\n",
      "training loss------------->  361.8094482421875\n",
      "Epoch: 633 Loss : 89.29209899902344\n",
      "training loss------------->  515.7474975585938\n",
      "Epoch: 634 Loss : 228.920166015625\n",
      "training loss------------->  647.3743896484375\n",
      "Epoch: 635 Loss : 140.05712890625\n",
      "training loss------------->  370.4402770996094\n",
      "Epoch: 636 Loss : 1147.56591796875\n",
      "training loss------------->  233.6022186279297\n",
      "Epoch: 637 Loss : 36.51737594604492\n",
      "training loss------------->  216.15603637695312\n",
      "Epoch: 638 Loss : 320.7093811035156\n",
      "training loss------------->  549.6758422851562\n",
      "Epoch: 639 Loss : 470.75311279296875\n",
      "training loss------------->  405.4818420410156\n",
      "Epoch: 640 Loss : 260.55810546875\n",
      "training loss------------->  277.81610107421875\n",
      "Epoch: 641 Loss : 237.8214569091797\n",
      "training loss------------->  275.8639221191406\n",
      "Epoch: 642 Loss : 520.4985961914062\n",
      "training loss------------->  208.49014282226562\n",
      "Epoch: 643 Loss : 717.4451293945312\n",
      "training loss------------->  575.580322265625\n",
      "Epoch: 644 Loss : 124.89766693115234\n",
      "training loss------------->  698.9119873046875\n",
      "Epoch: 645 Loss : 645.4364013671875\n",
      "training loss------------->  224.4011688232422\n",
      "Epoch: 646 Loss : 496.151611328125\n",
      "training loss------------->  295.6818542480469\n",
      "Epoch: 647 Loss : 698.0341796875\n",
      "training loss------------->  400.93658447265625\n",
      "Epoch: 648 Loss : 1544.9337158203125\n",
      "training loss------------->  456.7640380859375\n",
      "Epoch: 649 Loss : 261.2198486328125\n",
      "training loss------------->  489.91595458984375\n",
      "Epoch: 650 Loss : 553.0743408203125\n",
      "training loss------------->  298.7913818359375\n",
      "Epoch: 651 Loss : 777.2171630859375\n",
      "training loss------------->  425.86419677734375\n",
      "Epoch: 652 Loss : 547.1869506835938\n",
      "training loss------------->  350.0330505371094\n",
      "Epoch: 653 Loss : 68.58757781982422\n",
      "training loss------------->  424.4914855957031\n",
      "Epoch: 654 Loss : 466.0912170410156\n",
      "training loss------------->  335.02984619140625\n",
      "Epoch: 655 Loss : 892.7168579101562\n",
      "training loss------------->  511.33251953125\n",
      "Epoch: 656 Loss : 464.5074768066406\n",
      "training loss------------->  156.9224090576172\n",
      "Epoch: 657 Loss : 207.4567108154297\n",
      "training loss------------->  346.7069091796875\n",
      "Epoch: 658 Loss : 618.9833984375\n",
      "training loss------------->  502.7155456542969\n",
      "Epoch: 659 Loss : 251.46336364746094\n",
      "training loss------------->  321.97613525390625\n",
      "Epoch: 660 Loss : 805.3773193359375\n",
      "training loss------------->  674.0347290039062\n",
      "Epoch: 661 Loss : 773.4169921875\n",
      "training loss------------->  642.6819458007812\n",
      "Epoch: 662 Loss : 36.50616455078125\n",
      "training loss------------->  350.3599548339844\n",
      "Epoch: 663 Loss : 207.80844116210938\n",
      "training loss------------->  293.53155517578125\n",
      "Epoch: 664 Loss : 1869.5458984375\n",
      "training loss------------->  396.5218200683594\n",
      "Epoch: 665 Loss : 163.04942321777344\n",
      "training loss------------->  367.3264465332031\n",
      "Epoch: 666 Loss : 590.06005859375\n",
      "training loss------------->  280.7204284667969\n",
      "Epoch: 667 Loss : 595.2048950195312\n",
      "training loss------------->  207.09249877929688\n",
      "Epoch: 668 Loss : 428.6378173828125\n",
      "training loss------------->  406.69573974609375\n",
      "Epoch: 669 Loss : 523.1998901367188\n",
      "training loss------------->  228.84429931640625\n",
      "Epoch: 670 Loss : 306.8924255371094\n",
      "training loss------------->  437.1633605957031\n",
      "Epoch: 671 Loss : 157.5465087890625\n",
      "training loss------------->  525.1307373046875\n",
      "Epoch: 672 Loss : 155.05010986328125\n",
      "training loss------------->  319.3713684082031\n",
      "Epoch: 673 Loss : 226.0481719970703\n",
      "training loss------------->  251.71864318847656\n",
      "Epoch: 674 Loss : 636.870361328125\n",
      "training loss------------->  341.74456787109375\n",
      "Epoch: 675 Loss : 592.9533081054688\n",
      "training loss------------->  224.9664764404297\n",
      "Epoch: 676 Loss : 841.39892578125\n",
      "training loss------------->  278.610595703125\n",
      "Epoch: 677 Loss : 199.71896362304688\n",
      "training loss------------->  238.60107421875\n",
      "Epoch: 678 Loss : 389.8219299316406\n",
      "training loss------------->  331.8581237792969\n",
      "Epoch: 679 Loss : 108.58950805664062\n",
      "training loss------------->  267.34307861328125\n",
      "Epoch: 680 Loss : 998.4134521484375\n",
      "training loss------------->  333.4564208984375\n",
      "Epoch: 681 Loss : 328.638916015625\n",
      "training loss------------->  568.777587890625\n",
      "Epoch: 682 Loss : 1277.669921875\n",
      "training loss------------->  536.3118286132812\n",
      "Epoch: 683 Loss : 144.4131317138672\n",
      "training loss------------->  474.03802490234375\n",
      "Epoch: 684 Loss : 946.326904296875\n",
      "training loss------------->  247.1923828125\n",
      "Epoch: 685 Loss : 329.85662841796875\n",
      "training loss------------->  644.7459106445312\n",
      "Epoch: 686 Loss : 284.5347900390625\n",
      "training loss------------->  287.4581298828125\n",
      "Epoch: 687 Loss : 385.81878662109375\n",
      "training loss------------->  586.05615234375\n",
      "Epoch: 688 Loss : 276.5894470214844\n",
      "training loss------------->  225.2104034423828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 689 Loss : 296.0303955078125\n",
      "training loss------------->  250.005615234375\n",
      "Epoch: 690 Loss : 588.01123046875\n",
      "training loss------------->  450.14385986328125\n",
      "Epoch: 691 Loss : 1187.235107421875\n",
      "training loss------------->  581.1057739257812\n",
      "Epoch: 692 Loss : 1137.6617431640625\n",
      "training loss------------->  464.5203857421875\n",
      "Epoch: 693 Loss : 1157.937744140625\n",
      "training loss------------->  517.16357421875\n",
      "Epoch: 694 Loss : 231.33346557617188\n",
      "training loss------------->  221.99716186523438\n",
      "Epoch: 695 Loss : 212.36990356445312\n",
      "training loss------------->  319.54730224609375\n",
      "Epoch: 696 Loss : 543.5894775390625\n",
      "training loss------------->  541.3098754882812\n",
      "Epoch: 697 Loss : 183.38552856445312\n",
      "training loss------------->  438.2492980957031\n",
      "Epoch: 698 Loss : 39.605690002441406\n",
      "training loss------------->  566.0685424804688\n",
      "Epoch: 699 Loss : 518.8230590820312\n",
      "training loss------------->  353.00836181640625\n",
      "Epoch: 700 Loss : 806.0011596679688\n",
      "training loss------------->  265.0744323730469\n",
      "Epoch: 701 Loss : 472.88671875\n",
      "training loss------------->  504.5543212890625\n",
      "Epoch: 702 Loss : 69.34009552001953\n",
      "training loss------------->  383.9307861328125\n",
      "Epoch: 703 Loss : 725.7025146484375\n",
      "training loss------------->  455.8790588378906\n",
      "Epoch: 704 Loss : 97.56721496582031\n",
      "training loss------------->  523.5680541992188\n",
      "Epoch: 705 Loss : 376.2149353027344\n",
      "training loss------------->  555.197265625\n",
      "Epoch: 706 Loss : 488.8319396972656\n",
      "training loss------------->  337.03338623046875\n",
      "Epoch: 707 Loss : 200.62185668945312\n",
      "training loss------------->  137.9693603515625\n",
      "Epoch: 708 Loss : 759.147705078125\n",
      "training loss------------->  881.429931640625\n",
      "Epoch: 709 Loss : 541.2145385742188\n",
      "training loss------------->  195.35862731933594\n",
      "Epoch: 710 Loss : 130.90321350097656\n",
      "training loss------------->  297.1631164550781\n",
      "Epoch: 711 Loss : 30.003835678100586\n",
      "training loss------------->  256.326904296875\n",
      "Saving Best Model with Accuracy:  1.4608571529388428\n",
      "Epoch: 712 Loss : 1.4608571529388428\n",
      "training loss------------->  665.3163452148438\n",
      "Epoch: 713 Loss : 659.4473266601562\n",
      "training loss------------->  285.9385070800781\n",
      "Epoch: 714 Loss : 100.8363037109375\n",
      "training loss------------->  343.1459655761719\n",
      "Epoch: 715 Loss : 134.72463989257812\n",
      "training loss------------->  232.87754821777344\n",
      "Epoch: 716 Loss : 564.6320190429688\n",
      "training loss------------->  475.2279052734375\n",
      "Epoch: 717 Loss : 239.631103515625\n",
      "training loss------------->  690.7396850585938\n",
      "Epoch: 718 Loss : 239.16067504882812\n",
      "training loss------------->  420.52734375\n",
      "Epoch: 719 Loss : 89.59129333496094\n",
      "training loss------------->  477.84429931640625\n",
      "Epoch: 720 Loss : 153.68975830078125\n",
      "training loss------------->  226.21563720703125\n",
      "Epoch: 721 Loss : 354.57867431640625\n",
      "training loss------------->  381.89898681640625\n",
      "Epoch: 722 Loss : 580.8959350585938\n",
      "training loss------------->  356.9263916015625\n",
      "Epoch: 723 Loss : 92.14083862304688\n",
      "training loss------------->  568.7349243164062\n",
      "Epoch: 724 Loss : 483.9396057128906\n",
      "training loss------------->  314.1595153808594\n",
      "Epoch: 725 Loss : 1507.8978271484375\n",
      "training loss------------->  372.16229248046875\n",
      "Epoch: 726 Loss : 624.2737426757812\n",
      "training loss------------->  356.7825012207031\n",
      "Epoch: 727 Loss : 356.4162292480469\n",
      "training loss------------->  343.5370178222656\n",
      "Epoch: 728 Loss : 638.2390747070312\n",
      "training loss------------->  167.79368591308594\n",
      "Epoch: 729 Loss : 148.7685089111328\n",
      "training loss------------->  228.71839904785156\n",
      "Epoch: 730 Loss : 749.660888671875\n",
      "training loss------------->  546.757568359375\n",
      "Epoch: 731 Loss : 851.5327758789062\n",
      "training loss------------->  389.251953125\n",
      "Epoch: 732 Loss : 185.77288818359375\n",
      "training loss------------->  557.1092529296875\n",
      "Epoch: 733 Loss : 275.8992919921875\n",
      "training loss------------->  453.6189270019531\n",
      "Epoch: 734 Loss : 20.642322540283203\n",
      "training loss------------->  305.94671630859375\n",
      "Epoch: 735 Loss : 362.5523681640625\n",
      "training loss------------->  209.05392456054688\n",
      "Epoch: 736 Loss : 182.88357543945312\n",
      "training loss------------->  795.0604248046875\n",
      "Epoch: 737 Loss : 382.06829833984375\n",
      "training loss------------->  360.0187683105469\n",
      "Epoch: 738 Loss : 274.2195129394531\n",
      "training loss------------->  118.79637145996094\n",
      "Epoch: 739 Loss : 401.40826416015625\n",
      "training loss------------->  196.70726013183594\n",
      "Epoch: 740 Loss : 24.8199462890625\n",
      "training loss------------->  725.6323852539062\n",
      "Epoch: 741 Loss : 64.29212951660156\n",
      "training loss------------->  586.58203125\n",
      "Epoch: 742 Loss : 1014.4776611328125\n",
      "training loss------------->  385.14886474609375\n",
      "Epoch: 743 Loss : 941.81201171875\n",
      "training loss------------->  258.61517333984375\n",
      "Epoch: 744 Loss : 195.3001708984375\n",
      "training loss------------->  219.66604614257812\n",
      "Epoch: 745 Loss : 156.87554931640625\n",
      "training loss------------->  352.9092712402344\n",
      "Epoch: 746 Loss : 646.0599365234375\n",
      "training loss------------->  294.19171142578125\n",
      "Epoch: 747 Loss : 126.62792205810547\n",
      "training loss------------->  524.9041748046875\n",
      "Epoch: 748 Loss : 328.22308349609375\n",
      "training loss------------->  241.74664306640625\n",
      "Epoch: 749 Loss : 46.92733383178711\n",
      "training loss------------->  479.5139465332031\n",
      "Epoch: 750 Loss : 184.1444549560547\n",
      "training loss------------->  321.1092834472656\n",
      "Epoch: 751 Loss : 62.105018615722656\n",
      "training loss------------->  586.6344604492188\n",
      "Epoch: 752 Loss : 706.6170043945312\n",
      "training loss------------->  362.7188720703125\n",
      "Epoch: 753 Loss : 186.70301818847656\n",
      "training loss------------->  464.0955810546875\n",
      "Epoch: 754 Loss : 232.44476318359375\n",
      "training loss------------->  193.95355224609375\n",
      "Epoch: 755 Loss : 95.86865234375\n",
      "training loss------------->  413.1656494140625\n",
      "Epoch: 756 Loss : 95.1231460571289\n",
      "training loss------------->  208.5709228515625\n",
      "Epoch: 757 Loss : 1105.472412109375\n",
      "training loss------------->  572.48291015625\n",
      "Epoch: 758 Loss : 516.150634765625\n",
      "training loss------------->  421.7841491699219\n",
      "Epoch: 759 Loss : 292.3907775878906\n",
      "training loss------------->  359.7536926269531\n",
      "Epoch: 760 Loss : 496.45703125\n",
      "training loss------------->  477.66510009765625\n",
      "Epoch: 761 Loss : 85.17097473144531\n",
      "training loss------------->  238.07177734375\n",
      "Epoch: 762 Loss : 809.1640625\n",
      "training loss------------->  341.4353332519531\n",
      "Epoch: 763 Loss : 689.4193725585938\n",
      "training loss------------->  247.9278106689453\n",
      "Epoch: 764 Loss : 836.662109375\n",
      "training loss------------->  479.16949462890625\n",
      "Epoch: 765 Loss : 239.84817504882812\n",
      "training loss------------->  398.9764099121094\n",
      "Epoch: 766 Loss : 99.985107421875\n",
      "training loss------------->  242.20111083984375\n",
      "Epoch: 767 Loss : 209.83338928222656\n",
      "training loss------------->  440.0207214355469\n",
      "Epoch: 768 Loss : 166.16558837890625\n",
      "training loss------------->  224.23069763183594\n",
      "Epoch: 769 Loss : 577.3111572265625\n",
      "training loss------------->  377.3830261230469\n",
      "Epoch: 770 Loss : 280.9216613769531\n",
      "training loss------------->  460.4952392578125\n",
      "Epoch: 771 Loss : 426.887451171875\n",
      "training loss------------->  456.3279113769531\n",
      "Epoch: 772 Loss : 109.58157348632812\n",
      "training loss------------->  489.47991943359375\n",
      "Epoch: 773 Loss : 97.80536651611328\n",
      "training loss------------->  394.5755310058594\n",
      "Epoch: 774 Loss : 485.00213623046875\n",
      "training loss------------->  348.7822570800781\n",
      "Epoch: 775 Loss : 314.67950439453125\n",
      "training loss------------->  237.6871795654297\n",
      "Epoch: 776 Loss : 636.1370849609375\n",
      "training loss------------->  482.06549072265625\n",
      "Epoch: 777 Loss : 239.87115478515625\n",
      "training loss------------->  516.0155639648438\n",
      "Epoch: 778 Loss : 152.98033142089844\n",
      "training loss------------->  336.8760681152344\n",
      "Epoch: 779 Loss : 431.2825012207031\n",
      "training loss------------->  279.53656005859375\n",
      "Epoch: 780 Loss : 642.4151611328125\n",
      "training loss------------->  551.87744140625\n",
      "Epoch: 781 Loss : 399.4189453125\n",
      "training loss------------->  237.399658203125\n",
      "Epoch: 782 Loss : 473.45782470703125\n",
      "training loss------------->  698.1962280273438\n",
      "Epoch: 783 Loss : 928.1871337890625\n",
      "training loss------------->  175.6765594482422\n",
      "Epoch: 784 Loss : 89.03472900390625\n",
      "training loss------------->  450.5621337890625\n",
      "Epoch: 785 Loss : 146.72714233398438\n",
      "training loss------------->  310.1187438964844\n",
      "Epoch: 786 Loss : 187.4247589111328\n",
      "training loss------------->  535.5180053710938\n",
      "Epoch: 787 Loss : 876.8296508789062\n",
      "training loss------------->  185.28402709960938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 788 Loss : 150.38800048828125\n",
      "training loss------------->  321.81768798828125\n",
      "Epoch: 789 Loss : 98.4726333618164\n",
      "training loss------------->  680.5121459960938\n",
      "Epoch: 790 Loss : 314.3580017089844\n",
      "training loss------------->  661.5043334960938\n",
      "Epoch: 791 Loss : 594.2550659179688\n",
      "training loss------------->  259.4302062988281\n",
      "Epoch: 792 Loss : 720.4572143554688\n",
      "training loss------------->  220.62139892578125\n",
      "Epoch: 793 Loss : 349.00445556640625\n",
      "training loss------------->  445.4766845703125\n",
      "Epoch: 794 Loss : 479.953857421875\n",
      "training loss------------->  275.1241455078125\n",
      "Epoch: 795 Loss : 186.802001953125\n",
      "training loss------------->  145.2518310546875\n",
      "Epoch: 796 Loss : 575.7169799804688\n",
      "training loss------------->  217.4405975341797\n",
      "Epoch: 797 Loss : 671.1126098632812\n",
      "training loss------------->  363.6717529296875\n",
      "Epoch: 798 Loss : 349.1219482421875\n",
      "training loss------------->  468.8990783691406\n",
      "Epoch: 799 Loss : 197.72877502441406\n",
      "training loss------------->  261.70404052734375\n",
      "Epoch: 800 Loss : 138.3774871826172\n",
      "training loss------------->  730.18701171875\n",
      "Epoch: 801 Loss : 893.0833740234375\n",
      "training loss------------->  502.5475158691406\n",
      "Epoch: 802 Loss : 282.4642639160156\n",
      "training loss------------->  483.35302734375\n",
      "Epoch: 803 Loss : 358.5042724609375\n",
      "training loss------------->  413.50848388671875\n",
      "Epoch: 804 Loss : 344.387451171875\n",
      "training loss------------->  886.0701904296875\n",
      "Epoch: 805 Loss : 913.0178833007812\n",
      "training loss------------->  431.1326904296875\n",
      "Epoch: 806 Loss : 400.4330139160156\n",
      "training loss------------->  340.3307189941406\n",
      "Epoch: 807 Loss : 723.29638671875\n",
      "training loss------------->  316.20831298828125\n",
      "Epoch: 808 Loss : 316.756591796875\n",
      "training loss------------->  666.66015625\n",
      "Epoch: 809 Loss : 85.55598449707031\n",
      "training loss------------->  269.2975769042969\n",
      "Epoch: 810 Loss : 642.0980834960938\n",
      "training loss------------->  188.5902099609375\n",
      "Epoch: 811 Loss : 457.51983642578125\n",
      "training loss------------->  351.4952392578125\n",
      "Epoch: 812 Loss : 220.61492919921875\n",
      "training loss------------->  283.9059143066406\n",
      "Epoch: 813 Loss : 9.248947143554688\n",
      "training loss------------->  67.42405700683594\n",
      "Epoch: 814 Loss : 775.5184326171875\n",
      "training loss------------->  504.4366760253906\n",
      "Epoch: 815 Loss : 274.30169677734375\n",
      "training loss------------->  215.43458557128906\n",
      "Epoch: 816 Loss : 1227.714111328125\n",
      "training loss------------->  115.70328521728516\n",
      "Epoch: 817 Loss : 648.0899047851562\n",
      "training loss------------->  384.28594970703125\n",
      "Epoch: 818 Loss : 197.220703125\n",
      "training loss------------->  506.25439453125\n",
      "Epoch: 819 Loss : 385.5213928222656\n",
      "training loss------------->  358.40887451171875\n",
      "Epoch: 820 Loss : 170.7573699951172\n",
      "training loss------------->  335.64447021484375\n",
      "Epoch: 821 Loss : 474.6204833984375\n",
      "training loss------------->  257.4302978515625\n",
      "Epoch: 822 Loss : 1187.7193603515625\n",
      "training loss------------->  322.218994140625\n",
      "Epoch: 823 Loss : 81.7840805053711\n",
      "training loss------------->  700.5565795898438\n",
      "Epoch: 824 Loss : 254.4866485595703\n",
      "training loss------------->  519.0848388671875\n",
      "Epoch: 825 Loss : 34.873714447021484\n",
      "training loss------------->  327.7693176269531\n",
      "Epoch: 826 Loss : 564.8154296875\n",
      "training loss------------->  507.06976318359375\n",
      "Epoch: 827 Loss : 476.1938781738281\n",
      "training loss------------->  132.47552490234375\n",
      "Epoch: 828 Loss : 414.97186279296875\n",
      "training loss------------->  201.21481323242188\n",
      "Epoch: 829 Loss : 386.59033203125\n",
      "training loss------------->  363.84210205078125\n",
      "Epoch: 830 Loss : 358.9326477050781\n",
      "training loss------------->  572.7747192382812\n",
      "Epoch: 831 Loss : 347.7802429199219\n",
      "training loss------------->  353.1041564941406\n",
      "Epoch: 832 Loss : 356.4439697265625\n",
      "training loss------------->  158.7954559326172\n",
      "Epoch: 833 Loss : 277.9709167480469\n",
      "training loss------------->  566.11328125\n",
      "Epoch: 834 Loss : 302.8384094238281\n",
      "training loss------------->  634.484619140625\n",
      "Epoch: 835 Loss : 508.16265869140625\n",
      "training loss------------->  325.09735107421875\n",
      "Epoch: 836 Loss : 224.86622619628906\n",
      "training loss------------->  648.0241088867188\n",
      "Epoch: 837 Loss : 377.685791015625\n",
      "training loss------------->  279.3062438964844\n",
      "Epoch: 838 Loss : 405.7644348144531\n",
      "training loss------------->  640.6151733398438\n",
      "Epoch: 839 Loss : 123.45821380615234\n",
      "training loss------------->  376.32818603515625\n",
      "Epoch: 840 Loss : 32.14914321899414\n",
      "training loss------------->  264.02410888671875\n",
      "Epoch: 841 Loss : 901.4025268554688\n",
      "training loss------------->  524.6754760742188\n",
      "Epoch: 842 Loss : 297.84375\n",
      "training loss------------->  622.0647583007812\n",
      "Epoch: 843 Loss : 772.989990234375\n",
      "training loss------------->  617.3457641601562\n",
      "Epoch: 844 Loss : 385.4932556152344\n",
      "training loss------------->  339.2564697265625\n",
      "Epoch: 845 Loss : 653.08349609375\n",
      "training loss------------->  403.1426086425781\n",
      "Epoch: 846 Loss : 1273.3525390625\n",
      "training loss------------->  685.0023193359375\n",
      "Epoch: 847 Loss : 676.5349731445312\n",
      "training loss------------->  248.58963012695312\n",
      "Epoch: 848 Loss : 110.96867370605469\n",
      "training loss------------->  246.76565551757812\n",
      "Epoch: 849 Loss : 760.0856323242188\n",
      "training loss------------->  298.8196105957031\n",
      "Epoch: 850 Loss : 752.232177734375\n",
      "training loss------------->  384.4130859375\n",
      "Epoch: 851 Loss : 539.7184448242188\n",
      "training loss------------->  362.3350830078125\n",
      "Epoch: 852 Loss : 611.330078125\n",
      "training loss------------->  284.9307861328125\n",
      "Epoch: 853 Loss : 399.16412353515625\n",
      "training loss------------->  505.7950439453125\n",
      "Epoch: 854 Loss : 26.503061294555664\n",
      "training loss------------->  294.8419494628906\n",
      "Epoch: 855 Loss : 1181.091552734375\n",
      "training loss------------->  341.7591857910156\n",
      "Epoch: 856 Loss : 661.9606323242188\n",
      "training loss------------->  457.97259521484375\n",
      "Epoch: 857 Loss : 165.20916748046875\n",
      "training loss------------->  618.7798461914062\n",
      "Epoch: 858 Loss : 738.2672119140625\n",
      "training loss------------->  103.9148941040039\n",
      "Epoch: 859 Loss : 929.7203369140625\n",
      "training loss------------->  470.0774841308594\n",
      "Epoch: 860 Loss : 655.7705078125\n",
      "training loss------------->  514.216064453125\n",
      "Epoch: 861 Loss : 522.9315795898438\n",
      "training loss------------->  1217.1873779296875\n",
      "Epoch: 862 Loss : 375.5870361328125\n",
      "training loss------------->  490.169921875\n",
      "Epoch: 863 Loss : 332.9034118652344\n",
      "training loss------------->  224.02113342285156\n",
      "Epoch: 864 Loss : 116.79283905029297\n",
      "training loss------------->  428.4938659667969\n",
      "Epoch: 865 Loss : 909.110595703125\n",
      "training loss------------->  473.63671875\n",
      "Epoch: 866 Loss : 70.13906860351562\n",
      "training loss------------->  145.64755249023438\n",
      "Epoch: 867 Loss : 10.409832954406738\n",
      "training loss------------->  404.9852600097656\n",
      "Epoch: 868 Loss : 825.4364624023438\n",
      "training loss------------->  295.57440185546875\n",
      "Epoch: 869 Loss : 448.2239990234375\n",
      "training loss------------->  235.005859375\n",
      "Epoch: 870 Loss : 318.94677734375\n",
      "training loss------------->  250.78526306152344\n",
      "Epoch: 871 Loss : 84.2218017578125\n",
      "training loss------------->  370.63287353515625\n",
      "Epoch: 872 Loss : 354.8938903808594\n",
      "training loss------------->  558.331298828125\n",
      "Epoch: 873 Loss : 443.9151611328125\n",
      "training loss------------->  630.5072021484375\n",
      "Epoch: 874 Loss : 182.62648010253906\n",
      "training loss------------->  161.50990295410156\n",
      "Epoch: 875 Loss : 360.18292236328125\n",
      "training loss------------->  390.67864990234375\n",
      "Epoch: 876 Loss : 229.18020629882812\n",
      "training loss------------->  590.8997802734375\n",
      "Epoch: 877 Loss : 381.2378845214844\n",
      "training loss------------->  628.544921875\n",
      "Epoch: 878 Loss : 195.53289794921875\n",
      "training loss------------->  414.1387023925781\n",
      "Epoch: 879 Loss : 745.3975219726562\n",
      "training loss------------->  207.95266723632812\n",
      "Epoch: 880 Loss : 164.65040588378906\n",
      "training loss------------->  509.3619689941406\n",
      "Epoch: 881 Loss : 812.28271484375\n",
      "training loss------------->  405.0086364746094\n",
      "Epoch: 882 Loss : 941.0667114257812\n",
      "training loss------------->  280.909912109375\n",
      "Epoch: 883 Loss : 413.1980895996094\n",
      "training loss------------->  646.5643310546875\n",
      "Epoch: 884 Loss : 456.5119323730469\n",
      "training loss------------->  235.86151123046875\n",
      "Epoch: 885 Loss : 152.9555206298828\n",
      "training loss------------->  350.56817626953125\n",
      "Epoch: 886 Loss : 105.11941528320312\n",
      "training loss------------->  310.1162109375\n",
      "Epoch: 887 Loss : 78.74083709716797\n",
      "training loss------------->  214.18841552734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 888 Loss : 331.4777526855469\n",
      "training loss------------->  389.1461181640625\n",
      "Epoch: 889 Loss : 204.10845947265625\n",
      "training loss------------->  286.0475158691406\n",
      "Epoch: 890 Loss : 194.6613311767578\n",
      "training loss------------->  116.54181671142578\n",
      "Epoch: 891 Loss : 416.41058349609375\n",
      "training loss------------->  526.47998046875\n",
      "Epoch: 892 Loss : 15.831653594970703\n",
      "training loss------------->  194.94039916992188\n",
      "Epoch: 893 Loss : 740.135986328125\n",
      "training loss------------->  438.1089782714844\n",
      "Epoch: 894 Loss : 210.6803741455078\n",
      "training loss------------->  412.7508544921875\n",
      "Epoch: 895 Loss : 275.0735168457031\n",
      "training loss------------->  152.05206298828125\n",
      "Epoch: 896 Loss : 233.26708984375\n",
      "training loss------------->  444.1711120605469\n",
      "Epoch: 897 Loss : 475.23883056640625\n",
      "training loss------------->  144.62692260742188\n",
      "Epoch: 898 Loss : 573.0980224609375\n",
      "training loss------------->  430.3236083984375\n",
      "Epoch: 899 Loss : 147.91415405273438\n",
      "training loss------------->  636.619384765625\n",
      "Epoch: 900 Loss : 248.55130004882812\n",
      "training loss------------->  377.9520263671875\n",
      "Epoch: 901 Loss : 222.96226501464844\n",
      "training loss------------->  319.7181396484375\n",
      "Epoch: 902 Loss : 509.545166015625\n",
      "training loss------------->  339.41796875\n",
      "Epoch: 903 Loss : 137.7315673828125\n",
      "training loss------------->  184.38677978515625\n",
      "Epoch: 904 Loss : 108.40118408203125\n",
      "training loss------------->  329.80267333984375\n",
      "Epoch: 905 Loss : 248.86154174804688\n",
      "training loss------------->  678.6809692382812\n",
      "Epoch: 906 Loss : 79.35184478759766\n",
      "training loss------------->  565.3858642578125\n",
      "Epoch: 907 Loss : 336.8196716308594\n",
      "training loss------------->  270.55560302734375\n",
      "Epoch: 908 Loss : 342.4288635253906\n",
      "training loss------------->  343.8087158203125\n",
      "Epoch: 909 Loss : 499.202392578125\n",
      "training loss------------->  262.185791015625\n",
      "Epoch: 910 Loss : 412.6990661621094\n",
      "training loss------------->  451.7689208984375\n",
      "Epoch: 911 Loss : 467.859130859375\n",
      "training loss------------->  155.06227111816406\n",
      "Epoch: 912 Loss : 290.6163635253906\n",
      "training loss------------->  279.63690185546875\n",
      "Epoch: 913 Loss : 443.3821716308594\n",
      "training loss------------->  606.05224609375\n",
      "Epoch: 914 Loss : 338.4810791015625\n",
      "training loss------------->  307.3943176269531\n",
      "Epoch: 915 Loss : 140.86053466796875\n",
      "training loss------------->  361.8982238769531\n",
      "Epoch: 916 Loss : 650.6631469726562\n",
      "training loss------------->  232.10202026367188\n",
      "Epoch: 917 Loss : 587.12548828125\n",
      "training loss------------->  391.49237060546875\n",
      "Epoch: 918 Loss : 298.699462890625\n",
      "training loss------------->  435.2777099609375\n",
      "Epoch: 919 Loss : 291.31890869140625\n",
      "training loss------------->  345.61968994140625\n",
      "Epoch: 920 Loss : 310.193603515625\n",
      "training loss------------->  524.1100463867188\n",
      "Epoch: 921 Loss : 47.687767028808594\n",
      "training loss------------->  202.02008056640625\n",
      "Epoch: 922 Loss : 1056.4649658203125\n",
      "training loss------------->  363.6944580078125\n",
      "Epoch: 923 Loss : 553.606201171875\n",
      "training loss------------->  601.4790649414062\n",
      "Epoch: 924 Loss : 386.4120178222656\n",
      "training loss------------->  507.33837890625\n",
      "Epoch: 925 Loss : 509.55621337890625\n",
      "training loss------------->  301.6732482910156\n",
      "Epoch: 926 Loss : 377.66033935546875\n",
      "training loss------------->  218.97186279296875\n",
      "Epoch: 927 Loss : 180.29354858398438\n",
      "training loss------------->  608.837646484375\n",
      "Epoch: 928 Loss : 402.6320495605469\n",
      "training loss------------->  348.4160461425781\n",
      "Epoch: 929 Loss : 396.5949401855469\n",
      "training loss------------->  238.48171997070312\n",
      "Epoch: 930 Loss : 208.48326110839844\n",
      "training loss------------->  172.9954376220703\n",
      "Epoch: 931 Loss : 908.2488403320312\n",
      "training loss------------->  345.6399230957031\n",
      "Epoch: 932 Loss : 276.7607116699219\n",
      "training loss------------->  254.99891662597656\n",
      "Epoch: 933 Loss : 220.66162109375\n",
      "training loss------------->  394.15216064453125\n",
      "Epoch: 934 Loss : 333.48773193359375\n",
      "training loss------------->  317.19525146484375\n",
      "Epoch: 935 Loss : 403.1519470214844\n",
      "training loss------------->  416.2212829589844\n",
      "Epoch: 936 Loss : 341.6666259765625\n",
      "training loss------------->  294.4103088378906\n",
      "Epoch: 937 Loss : 642.5775756835938\n",
      "training loss------------->  200.56175231933594\n",
      "Epoch: 938 Loss : 498.21002197265625\n",
      "training loss------------->  286.2979736328125\n",
      "Epoch: 939 Loss : 343.1307373046875\n",
      "training loss------------->  428.253173828125\n",
      "Epoch: 940 Loss : 543.685546875\n",
      "training loss------------->  538.753173828125\n",
      "Epoch: 941 Loss : 504.30047607421875\n",
      "training loss------------->  295.6995849609375\n",
      "Epoch: 942 Loss : 532.4688720703125\n",
      "training loss------------->  481.6573791503906\n",
      "Epoch: 943 Loss : 333.8525695800781\n",
      "training loss------------->  455.6300354003906\n",
      "Epoch: 944 Loss : 229.40969848632812\n",
      "training loss------------->  458.7218933105469\n",
      "Epoch: 945 Loss : 335.7246398925781\n",
      "training loss------------->  250.9278106689453\n",
      "Epoch: 946 Loss : 196.40835571289062\n",
      "training loss------------->  320.3112487792969\n",
      "Epoch: 947 Loss : 340.97808837890625\n",
      "training loss------------->  139.10064697265625\n",
      "Epoch: 948 Loss : 845.6498413085938\n",
      "training loss------------->  502.0133972167969\n",
      "Epoch: 949 Loss : 289.522705078125\n",
      "training loss------------->  253.2447052001953\n",
      "Epoch: 950 Loss : 115.23120880126953\n",
      "training loss------------->  576.3970947265625\n",
      "Epoch: 951 Loss : 307.09869384765625\n",
      "training loss------------->  457.6416015625\n",
      "Epoch: 952 Loss : 285.3433532714844\n",
      "training loss------------->  497.580078125\n",
      "Epoch: 953 Loss : 123.32730102539062\n",
      "training loss------------->  602.5682373046875\n",
      "Epoch: 954 Loss : 97.20295715332031\n",
      "training loss------------->  475.04833984375\n",
      "Epoch: 955 Loss : 707.5608520507812\n",
      "training loss------------->  644.525634765625\n",
      "Epoch: 956 Loss : 948.6190185546875\n",
      "training loss------------->  168.59996032714844\n",
      "Epoch: 957 Loss : 65.88541412353516\n",
      "training loss------------->  511.3083801269531\n",
      "Epoch: 958 Loss : 362.9928894042969\n",
      "training loss------------->  522.0606689453125\n",
      "Epoch: 959 Loss : 338.1835021972656\n",
      "training loss------------->  669.1087036132812\n",
      "Epoch: 960 Loss : 406.807861328125\n",
      "training loss------------->  757.317138671875\n",
      "Epoch: 961 Loss : 730.4609985351562\n",
      "training loss------------->  350.8260803222656\n",
      "Epoch: 962 Loss : 641.4138793945312\n",
      "training loss------------->  367.8541259765625\n",
      "Epoch: 963 Loss : 728.7255859375\n",
      "training loss------------->  363.9977111816406\n",
      "Epoch: 964 Loss : 1125.575927734375\n",
      "training loss------------->  134.9707794189453\n",
      "Epoch: 965 Loss : 684.7035522460938\n",
      "training loss------------->  315.4195556640625\n",
      "Epoch: 966 Loss : 298.2806701660156\n",
      "training loss------------->  285.7888488769531\n",
      "Epoch: 967 Loss : 132.6539306640625\n",
      "training loss------------->  254.07383728027344\n",
      "Epoch: 968 Loss : 955.994140625\n",
      "training loss------------->  288.3927001953125\n",
      "Epoch: 969 Loss : 944.644775390625\n",
      "training loss------------->  246.9056854248047\n",
      "Epoch: 970 Loss : 531.5112915039062\n",
      "training loss------------->  515.3287963867188\n",
      "Epoch: 971 Loss : 151.15089416503906\n",
      "training loss------------->  465.3153076171875\n",
      "Epoch: 972 Loss : 758.0\n",
      "training loss------------->  564.36572265625\n",
      "Epoch: 973 Loss : 120.83718872070312\n",
      "training loss------------->  298.75115966796875\n",
      "Epoch: 974 Loss : 169.1522674560547\n",
      "training loss------------->  359.1315002441406\n",
      "Epoch: 975 Loss : 195.2159423828125\n",
      "training loss------------->  222.85198974609375\n",
      "Epoch: 976 Loss : 113.21226501464844\n",
      "training loss------------->  442.69921875\n",
      "Epoch: 977 Loss : 117.31634521484375\n",
      "training loss------------->  485.5065002441406\n",
      "Epoch: 978 Loss : 458.5042419433594\n",
      "training loss------------->  278.9252624511719\n",
      "Epoch: 979 Loss : 579.7064208984375\n",
      "training loss------------->  139.551025390625\n",
      "Epoch: 980 Loss : 142.39256286621094\n",
      "training loss------------->  677.0930786132812\n",
      "Epoch: 981 Loss : 1634.496337890625\n",
      "training loss------------->  501.3767395019531\n",
      "Epoch: 982 Loss : 19.7302188873291\n",
      "training loss------------->  258.3585205078125\n",
      "Epoch: 983 Loss : 963.2040405273438\n",
      "training loss------------->  332.00592041015625\n",
      "Epoch: 984 Loss : 657.416748046875\n",
      "training loss------------->  236.4473419189453\n",
      "Epoch: 985 Loss : 913.9779052734375\n",
      "training loss------------->  262.2329406738281\n",
      "Epoch: 986 Loss : 423.22412109375\n",
      "training loss------------->  325.6346435546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 987 Loss : 280.1052551269531\n",
      "training loss------------->  106.9992446899414\n",
      "Epoch: 988 Loss : 123.16557312011719\n",
      "training loss------------->  568.7413330078125\n",
      "Epoch: 989 Loss : 451.7486877441406\n",
      "training loss------------->  561.2792358398438\n",
      "Epoch: 990 Loss : 576.3417358398438\n",
      "training loss------------->  250.09597778320312\n",
      "Epoch: 991 Loss : 347.489013671875\n",
      "training loss------------->  367.4632263183594\n",
      "Epoch: 992 Loss : 656.1943969726562\n",
      "training loss------------->  764.369384765625\n",
      "Epoch: 993 Loss : 657.7208862304688\n",
      "training loss------------->  454.57305908203125\n",
      "Epoch: 994 Loss : 57.89375305175781\n",
      "training loss------------->  438.2232971191406\n",
      "Epoch: 995 Loss : 380.622314453125\n",
      "training loss------------->  370.8563232421875\n",
      "Epoch: 996 Loss : 281.1053771972656\n",
      "training loss------------->  464.28082275390625\n",
      "Epoch: 997 Loss : 160.38766479492188\n",
      "training loss------------->  205.69029235839844\n",
      "Epoch: 998 Loss : 315.56304931640625\n",
      "training loss------------->  261.3938903808594\n",
      "Epoch: 999 Loss : 274.50421142578125\n",
      "training loss------------->  408.39788818359375\n",
      "Epoch: 1000 Loss : 372.2355651855469\n",
      "training loss------------->  265.8140869140625\n",
      "Epoch: 1001 Loss : 66.6790771484375\n",
      "training loss------------->  448.599365234375\n",
      "Epoch: 1002 Loss : 159.99600219726562\n",
      "training loss------------->  507.5842590332031\n",
      "Epoch: 1003 Loss : 352.900390625\n",
      "training loss------------->  333.4093017578125\n",
      "Epoch: 1004 Loss : 133.48899841308594\n",
      "training loss------------->  426.48919677734375\n",
      "Epoch: 1005 Loss : 393.4902648925781\n",
      "training loss------------->  494.7781066894531\n",
      "Epoch: 1006 Loss : 1423.56982421875\n",
      "training loss------------->  457.0891418457031\n",
      "Epoch: 1007 Loss : 272.27593994140625\n",
      "training loss------------->  285.4558410644531\n",
      "Epoch: 1008 Loss : 170.65896606445312\n",
      "training loss------------->  571.2783203125\n",
      "Epoch: 1009 Loss : 498.00445556640625\n",
      "training loss------------->  153.667236328125\n",
      "Epoch: 1010 Loss : 954.144287109375\n",
      "training loss------------->  204.30575561523438\n",
      "Epoch: 1011 Loss : 425.1597595214844\n",
      "training loss------------->  734.4754638671875\n",
      "Epoch: 1012 Loss : 241.6932373046875\n",
      "training loss------------->  570.7001342773438\n",
      "Epoch: 1013 Loss : 103.58679962158203\n",
      "training loss------------->  491.4761962890625\n",
      "Epoch: 1014 Loss : 335.3489685058594\n",
      "training loss------------->  239.5323028564453\n",
      "Epoch: 1015 Loss : 128.6613311767578\n",
      "training loss------------->  146.4000701904297\n",
      "Epoch: 1016 Loss : 1239.07421875\n",
      "training loss------------->  224.4300994873047\n",
      "Epoch: 1017 Loss : 432.9084167480469\n",
      "training loss------------->  424.4018859863281\n",
      "Epoch: 1018 Loss : 159.2061309814453\n",
      "training loss------------->  309.6274719238281\n",
      "Epoch: 1019 Loss : 308.9847106933594\n",
      "training loss------------->  274.0600891113281\n",
      "Epoch: 1020 Loss : 150.81822204589844\n",
      "training loss------------->  398.7622985839844\n",
      "Epoch: 1021 Loss : 109.22852325439453\n",
      "training loss------------->  327.72210693359375\n",
      "Epoch: 1022 Loss : 799.1826171875\n",
      "training loss------------->  384.8143310546875\n",
      "Epoch: 1023 Loss : 121.34134674072266\n",
      "training loss------------->  544.342529296875\n",
      "Epoch: 1024 Loss : 125.99474334716797\n",
      "training loss------------->  341.2204284667969\n",
      "Epoch: 1025 Loss : 498.6419677734375\n",
      "training loss------------->  641.85302734375\n",
      "Epoch: 1026 Loss : 123.02371215820312\n",
      "training loss------------->  316.6385498046875\n",
      "Epoch: 1027 Loss : 516.1505737304688\n",
      "training loss------------->  545.000244140625\n",
      "Epoch: 1028 Loss : 524.927734375\n",
      "training loss------------->  334.1253662109375\n",
      "Epoch: 1029 Loss : 413.0679626464844\n",
      "training loss------------->  415.7787170410156\n",
      "Epoch: 1030 Loss : 255.6844482421875\n",
      "training loss------------->  277.7982482910156\n",
      "Epoch: 1031 Loss : 59.25233840942383\n",
      "training loss------------->  269.91705322265625\n",
      "Epoch: 1032 Loss : 16.000715255737305\n",
      "training loss------------->  327.9475402832031\n",
      "Epoch: 1033 Loss : 219.14854431152344\n",
      "training loss------------->  558.11376953125\n",
      "Epoch: 1034 Loss : 510.11407470703125\n",
      "training loss------------->  518.501220703125\n",
      "Epoch: 1035 Loss : 425.9783935546875\n",
      "training loss------------->  356.3294677734375\n",
      "Epoch: 1036 Loss : 641.9751586914062\n",
      "training loss------------->  412.4430236816406\n",
      "Epoch: 1037 Loss : 475.6251525878906\n",
      "training loss------------->  580.7742919921875\n",
      "Epoch: 1038 Loss : 751.472412109375\n",
      "training loss------------->  540.81591796875\n",
      "Epoch: 1039 Loss : 853.3621215820312\n",
      "training loss------------->  398.5378112792969\n",
      "Epoch: 1040 Loss : 1027.2982177734375\n",
      "training loss------------->  387.7264709472656\n",
      "Epoch: 1041 Loss : 184.66336059570312\n",
      "training loss------------->  596.369384765625\n",
      "Epoch: 1042 Loss : 369.24908447265625\n",
      "training loss------------->  261.6085205078125\n",
      "Epoch: 1043 Loss : 96.98033142089844\n",
      "training loss------------->  726.67138671875\n",
      "Epoch: 1044 Loss : 551.8966064453125\n",
      "training loss------------->  337.2043151855469\n",
      "Epoch: 1045 Loss : 543.5859375\n",
      "training loss------------->  335.0216979980469\n",
      "Epoch: 1046 Loss : 385.5982360839844\n",
      "training loss------------->  246.35902404785156\n",
      "Epoch: 1047 Loss : 276.01666259765625\n",
      "training loss------------->  173.43606567382812\n",
      "Epoch: 1048 Loss : 252.40048217773438\n",
      "training loss------------->  419.5628662109375\n",
      "Epoch: 1049 Loss : 787.264892578125\n",
      "training loss------------->  153.2041473388672\n",
      "Epoch: 1050 Loss : 1181.5361328125\n",
      "training loss------------->  575.4756469726562\n",
      "Epoch: 1051 Loss : 269.51446533203125\n",
      "training loss------------->  587.4935913085938\n",
      "Epoch: 1052 Loss : 443.19970703125\n",
      "training loss------------->  594.1610107421875\n",
      "Epoch: 1053 Loss : 340.1613464355469\n",
      "training loss------------->  354.36322021484375\n",
      "Epoch: 1054 Loss : 40.46620178222656\n",
      "training loss------------->  205.59951782226562\n",
      "Epoch: 1055 Loss : 214.0051727294922\n",
      "training loss------------->  536.453369140625\n",
      "Epoch: 1056 Loss : 145.78062438964844\n",
      "training loss------------->  597.9690551757812\n",
      "Epoch: 1057 Loss : 820.8212890625\n",
      "training loss------------->  166.44818115234375\n",
      "Epoch: 1058 Loss : 183.4640350341797\n",
      "training loss------------->  324.11297607421875\n",
      "Epoch: 1059 Loss : 326.0886535644531\n",
      "training loss------------->  505.9080505371094\n",
      "Epoch: 1060 Loss : 21.13593101501465\n",
      "training loss------------->  445.5147705078125\n",
      "Epoch: 1061 Loss : 496.27960205078125\n",
      "training loss------------->  353.95465087890625\n",
      "Epoch: 1062 Loss : 796.6142578125\n",
      "training loss------------->  442.0938720703125\n",
      "Epoch: 1063 Loss : 780.7083740234375\n",
      "training loss------------->  343.3042297363281\n",
      "Epoch: 1064 Loss : 497.97906494140625\n",
      "training loss------------->  479.7219543457031\n",
      "Epoch: 1065 Loss : 158.8387451171875\n",
      "training loss------------->  270.603759765625\n",
      "Epoch: 1066 Loss : 12.600796699523926\n",
      "training loss------------->  415.81201171875\n",
      "Epoch: 1067 Loss : 176.90411376953125\n",
      "training loss------------->  153.98854064941406\n",
      "Epoch: 1068 Loss : 111.0772705078125\n",
      "training loss------------->  542.4200439453125\n",
      "Epoch: 1069 Loss : 91.39630126953125\n",
      "training loss------------->  476.3016357421875\n",
      "Epoch: 1070 Loss : 580.920166015625\n",
      "training loss------------->  293.53424072265625\n",
      "Epoch: 1071 Loss : 387.13037109375\n",
      "training loss------------->  262.10894775390625\n",
      "Epoch: 1072 Loss : 407.8785705566406\n",
      "training loss------------->  356.7958068847656\n",
      "Epoch: 1073 Loss : 43.65567398071289\n",
      "training loss------------->  266.68939208984375\n",
      "Epoch: 1074 Loss : 72.24005126953125\n",
      "training loss------------->  427.9892578125\n",
      "Epoch: 1075 Loss : 260.4126281738281\n",
      "training loss------------->  285.30810546875\n",
      "Epoch: 1076 Loss : 182.75332641601562\n",
      "training loss------------->  313.2469482421875\n",
      "Epoch: 1077 Loss : 131.87728881835938\n",
      "training loss------------->  259.66046142578125\n",
      "Epoch: 1078 Loss : 1109.5618896484375\n",
      "training loss------------->  334.6840515136719\n",
      "Epoch: 1079 Loss : 281.6584777832031\n",
      "training loss------------->  500.3614807128906\n",
      "Epoch: 1080 Loss : 133.84054565429688\n",
      "training loss------------->  232.2776641845703\n",
      "Epoch: 1081 Loss : 338.6812744140625\n",
      "training loss------------->  554.4478149414062\n",
      "Epoch: 1082 Loss : 252.65277099609375\n",
      "training loss------------->  149.2308807373047\n",
      "Epoch: 1083 Loss : 765.886962890625\n",
      "training loss------------->  939.9013671875\n",
      "Epoch: 1084 Loss : 140.09762573242188\n",
      "training loss------------->  387.3611145019531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1085 Loss : 165.76820373535156\n",
      "training loss------------->  377.6846923828125\n",
      "Epoch: 1086 Loss : 453.441162109375\n",
      "training loss------------->  221.45973205566406\n",
      "Epoch: 1087 Loss : 718.9078979492188\n",
      "training loss------------->  270.0135498046875\n",
      "Epoch: 1088 Loss : 914.5381469726562\n",
      "training loss------------->  717.1061401367188\n",
      "Epoch: 1089 Loss : 284.2123107910156\n",
      "training loss------------->  141.95179748535156\n",
      "Epoch: 1090 Loss : 1254.5858154296875\n",
      "training loss------------->  370.0556335449219\n",
      "Epoch: 1091 Loss : 219.775146484375\n",
      "training loss------------->  438.0736083984375\n",
      "Epoch: 1092 Loss : 275.12506103515625\n",
      "training loss------------->  353.90234375\n",
      "Epoch: 1093 Loss : 395.82000732421875\n",
      "training loss------------->  358.64617919921875\n",
      "Epoch: 1094 Loss : 1659.2896728515625\n",
      "training loss------------->  479.8214111328125\n",
      "Epoch: 1095 Loss : 94.13876342773438\n",
      "training loss------------->  139.3335723876953\n",
      "Epoch: 1096 Loss : 82.74842834472656\n",
      "training loss------------->  442.8349914550781\n",
      "Epoch: 1097 Loss : 415.11376953125\n",
      "training loss------------->  283.6420593261719\n",
      "Epoch: 1098 Loss : 341.27264404296875\n",
      "training loss------------->  839.7640380859375\n",
      "Epoch: 1099 Loss : 359.112548828125\n",
      "training loss------------->  362.89300537109375\n",
      "Epoch: 1100 Loss : 124.712646484375\n",
      "training loss------------->  66.92294311523438\n",
      "Epoch: 1101 Loss : 988.167236328125\n",
      "training loss------------->  338.42535400390625\n",
      "Epoch: 1102 Loss : 273.33367919921875\n",
      "training loss------------->  200.21383666992188\n",
      "Epoch: 1103 Loss : 819.47900390625\n",
      "training loss------------->  573.640869140625\n",
      "Epoch: 1104 Loss : 160.4278106689453\n",
      "training loss------------->  415.1564025878906\n",
      "Epoch: 1105 Loss : 69.94769287109375\n",
      "training loss------------->  148.91610717773438\n",
      "Epoch: 1106 Loss : 347.4091796875\n",
      "training loss------------->  476.2330322265625\n",
      "Epoch: 1107 Loss : 309.4842529296875\n",
      "training loss------------->  409.03167724609375\n",
      "Epoch: 1108 Loss : 39.81321716308594\n",
      "training loss------------->  371.33270263671875\n",
      "Epoch: 1109 Loss : 273.48516845703125\n",
      "training loss------------->  450.92303466796875\n",
      "Epoch: 1110 Loss : 235.4337615966797\n",
      "training loss------------->  312.6116943359375\n",
      "Epoch: 1111 Loss : 516.1907958984375\n",
      "training loss------------->  468.2464294433594\n",
      "Epoch: 1112 Loss : 521.89599609375\n",
      "training loss------------->  134.52882385253906\n",
      "Epoch: 1113 Loss : 666.7972412109375\n",
      "training loss------------->  376.7083740234375\n",
      "Epoch: 1114 Loss : 389.0609436035156\n",
      "training loss------------->  256.55426025390625\n",
      "Epoch: 1115 Loss : 130.47015380859375\n",
      "training loss------------->  491.7233581542969\n",
      "Epoch: 1116 Loss : 1031.15673828125\n",
      "training loss------------->  676.0034790039062\n",
      "Epoch: 1117 Loss : 188.94671630859375\n",
      "training loss------------->  350.7281188964844\n",
      "Epoch: 1118 Loss : 208.15650939941406\n",
      "training loss------------->  290.6791076660156\n",
      "Epoch: 1119 Loss : 1078.3563232421875\n",
      "training loss------------->  523.9603881835938\n",
      "Epoch: 1120 Loss : 537.1300048828125\n",
      "training loss------------->  386.9416198730469\n",
      "Epoch: 1121 Loss : 344.86676025390625\n",
      "training loss------------->  596.5425415039062\n",
      "Epoch: 1122 Loss : 34.44261169433594\n",
      "training loss------------->  115.87773895263672\n",
      "Epoch: 1123 Loss : 164.6879425048828\n",
      "training loss------------->  348.980224609375\n",
      "Epoch: 1124 Loss : 260.6744384765625\n",
      "training loss------------->  392.0450134277344\n",
      "Epoch: 1125 Loss : 203.72157287597656\n",
      "training loss------------->  594.251708984375\n",
      "Epoch: 1126 Loss : 321.7545166015625\n",
      "training loss------------->  286.64019775390625\n",
      "Epoch: 1127 Loss : 500.2251281738281\n",
      "training loss------------->  179.18418884277344\n",
      "Epoch: 1128 Loss : 197.88182067871094\n",
      "training loss------------->  655.490966796875\n",
      "Epoch: 1129 Loss : 84.61512756347656\n",
      "training loss------------->  311.0750732421875\n",
      "Epoch: 1130 Loss : 104.18429565429688\n",
      "training loss------------->  272.54827880859375\n",
      "Epoch: 1131 Loss : 263.8555603027344\n",
      "training loss------------->  297.7264404296875\n",
      "Epoch: 1132 Loss : 882.8897094726562\n",
      "training loss------------->  544.15625\n",
      "Epoch: 1133 Loss : 477.0930480957031\n",
      "training loss------------->  319.7858581542969\n",
      "Epoch: 1134 Loss : 296.74493408203125\n",
      "training loss------------->  507.4309997558594\n",
      "Epoch: 1135 Loss : 267.9891662597656\n",
      "training loss------------->  461.0728759765625\n",
      "Epoch: 1136 Loss : 525.097412109375\n",
      "training loss------------->  553.6146240234375\n",
      "Epoch: 1137 Loss : 356.13671875\n",
      "training loss------------->  105.5015869140625\n",
      "Epoch: 1138 Loss : 161.3930206298828\n",
      "training loss------------->  455.1920166015625\n",
      "Epoch: 1139 Loss : 140.28854370117188\n",
      "training loss------------->  177.42787170410156\n",
      "Epoch: 1140 Loss : 515.2113647460938\n",
      "training loss------------->  341.0798645019531\n",
      "Epoch: 1141 Loss : 205.73062133789062\n",
      "training loss------------->  580.6588745117188\n",
      "Epoch: 1142 Loss : 897.57275390625\n",
      "training loss------------->  203.37701416015625\n",
      "Epoch: 1143 Loss : 131.43316650390625\n",
      "training loss------------->  553.9788818359375\n",
      "Epoch: 1144 Loss : 229.78721618652344\n",
      "training loss------------->  632.2914428710938\n",
      "Epoch: 1145 Loss : 67.33244323730469\n",
      "training loss------------->  362.8558044433594\n",
      "Epoch: 1146 Loss : 401.2927551269531\n",
      "training loss------------->  234.68670654296875\n",
      "Epoch: 1147 Loss : 312.97662353515625\n",
      "training loss------------->  317.67181396484375\n",
      "Epoch: 1148 Loss : 41.73042297363281\n",
      "training loss------------->  592.4534301757812\n",
      "Epoch: 1149 Loss : 529.5300903320312\n",
      "training loss------------->  414.0479431152344\n",
      "Epoch: 1150 Loss : 664.3233032226562\n",
      "training loss------------->  679.3029174804688\n",
      "Epoch: 1151 Loss : 372.6434326171875\n",
      "training loss------------->  400.12933349609375\n",
      "Epoch: 1152 Loss : 185.66009521484375\n",
      "training loss------------->  631.632568359375\n",
      "Epoch: 1153 Loss : 661.7711181640625\n",
      "training loss------------->  240.82354736328125\n",
      "Epoch: 1154 Loss : 240.528076171875\n",
      "training loss------------->  305.7151794433594\n",
      "Epoch: 1155 Loss : 10.854496002197266\n",
      "training loss------------->  235.2810821533203\n",
      "Epoch: 1156 Loss : 285.83935546875\n",
      "training loss------------->  252.42376708984375\n",
      "Epoch: 1157 Loss : 343.34808349609375\n",
      "training loss------------->  532.614501953125\n",
      "Epoch: 1158 Loss : 311.42181396484375\n",
      "training loss------------->  273.0977783203125\n",
      "Epoch: 1159 Loss : 423.19635009765625\n",
      "training loss------------->  152.52642822265625\n",
      "Epoch: 1160 Loss : 832.1571655273438\n",
      "training loss------------->  301.3233947753906\n",
      "Epoch: 1161 Loss : 470.952392578125\n",
      "training loss------------->  360.1385192871094\n",
      "Epoch: 1162 Loss : 161.641845703125\n",
      "training loss------------->  323.08258056640625\n",
      "Epoch: 1163 Loss : 329.4312438964844\n",
      "training loss------------->  308.6427917480469\n",
      "Epoch: 1164 Loss : 314.0591735839844\n",
      "training loss------------->  364.46929931640625\n",
      "Epoch: 1165 Loss : 468.5823669433594\n",
      "training loss------------->  209.52439880371094\n",
      "Epoch: 1166 Loss : 383.4642639160156\n",
      "training loss------------->  206.77435302734375\n",
      "Epoch: 1167 Loss : 433.9600524902344\n",
      "training loss------------->  613.851318359375\n",
      "Epoch: 1168 Loss : 1061.931396484375\n",
      "training loss------------->  487.9098205566406\n",
      "Epoch: 1169 Loss : 353.9110107421875\n",
      "training loss------------->  206.50022888183594\n",
      "Epoch: 1170 Loss : 70.33281707763672\n",
      "training loss------------->  239.7589569091797\n",
      "Epoch: 1171 Loss : 326.0179748535156\n",
      "training loss------------->  326.3704833984375\n",
      "Epoch: 1172 Loss : 1012.6707763671875\n",
      "training loss------------->  541.1925048828125\n",
      "Epoch: 1173 Loss : 69.87232208251953\n",
      "training loss------------->  264.9724426269531\n",
      "Epoch: 1174 Loss : 608.5996704101562\n",
      "training loss------------->  324.10345458984375\n",
      "Epoch: 1175 Loss : 6.992979049682617\n",
      "training loss------------->  322.6605224609375\n",
      "Epoch: 1176 Loss : 391.39544677734375\n",
      "training loss------------->  383.1800842285156\n",
      "Epoch: 1177 Loss : 268.0980529785156\n",
      "training loss------------->  292.68017578125\n",
      "Epoch: 1178 Loss : 516.00341796875\n",
      "training loss------------->  521.17529296875\n",
      "Epoch: 1179 Loss : 602.7337646484375\n",
      "training loss------------->  643.0052490234375\n",
      "Epoch: 1180 Loss : 1527.328125\n",
      "training loss------------->  148.1933135986328\n",
      "Epoch: 1181 Loss : 259.7461242675781\n",
      "training loss------------->  254.99148559570312\n",
      "Epoch: 1182 Loss : 59.685035705566406\n",
      "training loss------------->  556.7325439453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1183 Loss : 422.1278076171875\n",
      "training loss------------->  174.3972625732422\n",
      "Epoch: 1184 Loss : 400.96783447265625\n",
      "training loss------------->  246.31581115722656\n",
      "Epoch: 1185 Loss : 138.6061553955078\n",
      "training loss------------->  314.88446044921875\n",
      "Epoch: 1186 Loss : 397.0558166503906\n",
      "training loss------------->  427.3475341796875\n",
      "Epoch: 1187 Loss : 480.273681640625\n",
      "training loss------------->  271.84320068359375\n",
      "Epoch: 1188 Loss : 9.945436477661133\n",
      "training loss------------->  491.8147888183594\n",
      "Epoch: 1189 Loss : 467.7370300292969\n",
      "training loss------------->  736.2571411132812\n",
      "Epoch: 1190 Loss : 264.8762512207031\n",
      "training loss------------->  337.81036376953125\n",
      "Epoch: 1191 Loss : 1128.6998291015625\n",
      "training loss------------->  558.922607421875\n",
      "Epoch: 1192 Loss : 734.8463134765625\n",
      "training loss------------->  515.6748657226562\n",
      "Epoch: 1193 Loss : 276.20458984375\n",
      "training loss------------->  1073.8349609375\n",
      "Epoch: 1194 Loss : 226.2585906982422\n",
      "training loss------------->  158.6244659423828\n",
      "Epoch: 1195 Loss : 735.8858642578125\n",
      "training loss------------->  338.2170104980469\n",
      "Epoch: 1196 Loss : 445.6245422363281\n",
      "training loss------------->  210.4701690673828\n",
      "Epoch: 1197 Loss : 127.75080871582031\n",
      "training loss------------->  193.09054565429688\n",
      "Epoch: 1198 Loss : 276.24560546875\n",
      "training loss------------->  347.0209655761719\n",
      "Epoch: 1199 Loss : 299.48944091796875\n",
      "training loss------------->  264.299072265625\n",
      "Epoch: 1200 Loss : 367.8356628417969\n",
      "training loss------------->  217.19374084472656\n",
      "Epoch: 1201 Loss : 122.25041198730469\n",
      "training loss------------->  493.2625732421875\n",
      "Epoch: 1202 Loss : 557.4291381835938\n",
      "training loss------------->  231.33837890625\n",
      "Epoch: 1203 Loss : 1058.9393310546875\n",
      "training loss------------->  539.0184936523438\n",
      "Epoch: 1204 Loss : 290.3731994628906\n",
      "training loss------------->  446.23004150390625\n",
      "Epoch: 1205 Loss : 105.8238525390625\n",
      "training loss------------->  224.490966796875\n",
      "Epoch: 1206 Loss : 738.5910034179688\n",
      "training loss------------->  213.25244140625\n",
      "Epoch: 1207 Loss : 346.5023498535156\n",
      "training loss------------->  432.02252197265625\n",
      "Epoch: 1208 Loss : 219.9396209716797\n",
      "training loss------------->  192.41281127929688\n",
      "Epoch: 1209 Loss : 395.21441650390625\n",
      "training loss------------->  288.740234375\n",
      "Epoch: 1210 Loss : 97.52444458007812\n",
      "training loss------------->  235.01307678222656\n",
      "Epoch: 1211 Loss : 184.37997436523438\n",
      "training loss------------->  396.3959655761719\n",
      "Epoch: 1212 Loss : 344.07684326171875\n",
      "training loss------------->  221.75079345703125\n",
      "Epoch: 1213 Loss : 155.85330200195312\n",
      "training loss------------->  406.7603759765625\n",
      "Epoch: 1214 Loss : 500.38409423828125\n",
      "training loss------------->  303.746337890625\n",
      "Epoch: 1215 Loss : 380.5540771484375\n",
      "training loss------------->  683.4036254882812\n",
      "Epoch: 1216 Loss : 350.0882568359375\n",
      "training loss------------->  437.273193359375\n",
      "Epoch: 1217 Loss : 144.79405212402344\n",
      "training loss------------->  191.30238342285156\n",
      "Epoch: 1218 Loss : 686.3807373046875\n",
      "training loss------------->  244.40435791015625\n",
      "Epoch: 1219 Loss : 164.6806640625\n",
      "training loss------------->  293.25726318359375\n",
      "Epoch: 1220 Loss : 672.6025390625\n",
      "training loss------------->  183.89828491210938\n",
      "Epoch: 1221 Loss : 284.8510437011719\n",
      "training loss------------->  423.54449462890625\n",
      "Epoch: 1222 Loss : 187.39608764648438\n",
      "training loss------------->  325.65447998046875\n",
      "Epoch: 1223 Loss : 495.812744140625\n",
      "training loss------------->  383.2247314453125\n",
      "Epoch: 1224 Loss : 384.45361328125\n",
      "training loss------------->  289.7091064453125\n",
      "Epoch: 1225 Loss : 904.3609008789062\n",
      "training loss------------->  516.099365234375\n",
      "Epoch: 1226 Loss : 676.7711181640625\n",
      "training loss------------->  343.38336181640625\n",
      "Epoch: 1227 Loss : 753.1097412109375\n",
      "training loss------------->  538.2271118164062\n",
      "Epoch: 1228 Loss : 767.7288818359375\n",
      "training loss------------->  248.6710662841797\n",
      "Epoch: 1229 Loss : 512.7554931640625\n",
      "training loss------------->  504.79486083984375\n",
      "Epoch: 1230 Loss : 76.90473175048828\n",
      "training loss------------->  396.7538757324219\n",
      "Epoch: 1231 Loss : 526.6066284179688\n",
      "training loss------------->  475.08148193359375\n",
      "Epoch: 1232 Loss : 802.912841796875\n",
      "training loss------------->  201.71728515625\n",
      "Epoch: 1233 Loss : 240.33828735351562\n",
      "training loss------------->  332.8442077636719\n",
      "Epoch: 1234 Loss : 176.30374145507812\n",
      "training loss------------->  416.27886962890625\n",
      "Epoch: 1235 Loss : 623.7689819335938\n",
      "training loss------------->  390.796142578125\n",
      "Epoch: 1236 Loss : 233.83456420898438\n",
      "training loss------------->  412.16552734375\n",
      "Epoch: 1237 Loss : 101.99568176269531\n",
      "training loss------------->  296.86431884765625\n",
      "Epoch: 1238 Loss : 310.7579650878906\n",
      "training loss------------->  238.50990295410156\n",
      "Epoch: 1239 Loss : 812.9193115234375\n",
      "training loss------------->  242.25914001464844\n",
      "Epoch: 1240 Loss : 241.89117431640625\n",
      "training loss------------->  319.30438232421875\n",
      "Epoch: 1241 Loss : 546.1821899414062\n",
      "training loss------------->  383.6669616699219\n",
      "Epoch: 1242 Loss : 842.9242553710938\n",
      "training loss------------->  250.3888397216797\n",
      "Epoch: 1243 Loss : 733.0403442382812\n",
      "training loss------------->  564.8257446289062\n",
      "Epoch: 1244 Loss : 245.35693359375\n",
      "training loss------------->  288.0220947265625\n",
      "Epoch: 1245 Loss : 422.6656494140625\n",
      "training loss------------->  725.1603393554688\n",
      "Epoch: 1246 Loss : 937.73486328125\n",
      "training loss------------->  260.9139709472656\n",
      "Epoch: 1247 Loss : 202.07286071777344\n",
      "training loss------------->  582.9954833984375\n",
      "Epoch: 1248 Loss : 796.1683349609375\n",
      "training loss------------->  504.26898193359375\n",
      "Epoch: 1249 Loss : 540.5291748046875\n",
      "training loss------------->  593.6875\n",
      "Epoch: 1250 Loss : 704.8077392578125\n",
      "training loss------------->  546.7293090820312\n",
      "Epoch: 1251 Loss : 449.0628662109375\n",
      "training loss------------->  401.7197570800781\n",
      "Epoch: 1252 Loss : 236.45675659179688\n",
      "training loss------------->  510.1209411621094\n",
      "Epoch: 1253 Loss : 31.568687438964844\n",
      "training loss------------->  386.79510498046875\n",
      "Epoch: 1254 Loss : 251.76580810546875\n",
      "training loss------------->  270.0694885253906\n",
      "Epoch: 1255 Loss : 769.7408447265625\n",
      "training loss------------->  360.9700622558594\n",
      "Epoch: 1256 Loss : 222.11068725585938\n",
      "training loss------------->  249.78048706054688\n",
      "Epoch: 1257 Loss : 685.0676879882812\n",
      "training loss------------->  718.8225708007812\n",
      "Epoch: 1258 Loss : 233.7310333251953\n",
      "training loss------------->  602.737060546875\n",
      "Epoch: 1259 Loss : 566.7550659179688\n",
      "training loss------------->  480.0240783691406\n",
      "Epoch: 1260 Loss : 302.00567626953125\n",
      "training loss------------->  574.191162109375\n",
      "Epoch: 1261 Loss : 423.9864501953125\n",
      "training loss------------->  371.8922119140625\n",
      "Epoch: 1262 Loss : 56.124359130859375\n",
      "training loss------------->  551.8353271484375\n",
      "Epoch: 1263 Loss : 297.6112976074219\n",
      "training loss------------->  650.9134521484375\n",
      "Epoch: 1264 Loss : 257.5384216308594\n",
      "training loss------------->  339.52105712890625\n",
      "Epoch: 1265 Loss : 214.20797729492188\n",
      "training loss------------->  421.9957275390625\n",
      "Epoch: 1266 Loss : 1223.9027099609375\n",
      "training loss------------->  372.7353515625\n",
      "Epoch: 1267 Loss : 362.97711181640625\n",
      "training loss------------->  366.68353271484375\n",
      "Epoch: 1268 Loss : 1043.3814697265625\n",
      "training loss------------->  267.8109130859375\n",
      "Epoch: 1269 Loss : 759.664794921875\n",
      "training loss------------->  224.37649536132812\n",
      "Epoch: 1270 Loss : 313.91400146484375\n",
      "training loss------------->  570.0127563476562\n",
      "Epoch: 1271 Loss : 249.99884033203125\n",
      "training loss------------->  294.8390197753906\n",
      "Epoch: 1272 Loss : 734.851806640625\n",
      "training loss------------->  260.72198486328125\n",
      "Epoch: 1273 Loss : 248.61878967285156\n",
      "training loss------------->  471.6944580078125\n",
      "Epoch: 1274 Loss : 186.36337280273438\n",
      "training loss------------->  225.2543182373047\n",
      "Epoch: 1275 Loss : 318.77825927734375\n",
      "training loss------------->  490.4489440917969\n",
      "Epoch: 1276 Loss : 481.845703125\n",
      "training loss------------->  415.53717041015625\n",
      "Epoch: 1277 Loss : 1593.94091796875\n",
      "training loss------------->  465.9446105957031\n",
      "Epoch: 1278 Loss : 11.037582397460938\n",
      "training loss------------->  131.2880859375\n",
      "Epoch: 1279 Loss : 628.5089111328125\n",
      "training loss------------->  514.593994140625\n",
      "Epoch: 1280 Loss : 728.6029052734375\n",
      "training loss------------->  509.28271484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1281 Loss : 1294.678955078125\n",
      "training loss------------->  557.3192749023438\n",
      "Epoch: 1282 Loss : 370.1757507324219\n",
      "training loss------------->  475.4064025878906\n",
      "Epoch: 1283 Loss : 410.79315185546875\n",
      "training loss------------->  296.3744812011719\n",
      "Epoch: 1284 Loss : 235.26255798339844\n",
      "training loss------------->  279.8030700683594\n",
      "Epoch: 1285 Loss : 185.9703826904297\n",
      "training loss------------->  195.36061096191406\n",
      "Epoch: 1286 Loss : 384.14288330078125\n",
      "training loss------------->  417.5248107910156\n",
      "Epoch: 1287 Loss : 233.37353515625\n",
      "training loss------------->  404.1579895019531\n",
      "Epoch: 1288 Loss : 713.2728881835938\n",
      "training loss------------->  407.19915771484375\n",
      "Epoch: 1289 Loss : 168.0408172607422\n",
      "training loss------------->  274.8477478027344\n",
      "Epoch: 1290 Loss : 162.90582275390625\n",
      "training loss------------->  413.0578918457031\n",
      "Epoch: 1291 Loss : 238.310546875\n",
      "training loss------------->  218.43496704101562\n",
      "Epoch: 1292 Loss : 435.98065185546875\n",
      "training loss------------->  342.3341064453125\n",
      "Epoch: 1293 Loss : 239.6669464111328\n",
      "training loss------------->  486.06048583984375\n",
      "Epoch: 1294 Loss : 522.6226196289062\n",
      "training loss------------->  365.93902587890625\n",
      "Epoch: 1295 Loss : 474.623046875\n",
      "training loss------------->  196.57777404785156\n",
      "Epoch: 1296 Loss : 236.34913635253906\n",
      "training loss------------->  140.09312438964844\n",
      "Epoch: 1297 Loss : 636.7294921875\n",
      "training loss------------->  725.767822265625\n",
      "Epoch: 1298 Loss : 74.22711944580078\n",
      "training loss------------->  413.2547912597656\n",
      "Epoch: 1299 Loss : 37.218780517578125\n",
      "training loss------------->  185.83676147460938\n",
      "Epoch: 1300 Loss : 763.663818359375\n",
      "training loss------------->  255.04876708984375\n",
      "Epoch: 1301 Loss : 473.9066162109375\n",
      "training loss------------->  364.1295166015625\n",
      "Epoch: 1302 Loss : 442.6540832519531\n",
      "training loss------------->  415.25079345703125\n",
      "Epoch: 1303 Loss : 451.21728515625\n",
      "training loss------------->  263.3492126464844\n",
      "Epoch: 1304 Loss : 410.99383544921875\n",
      "training loss------------->  687.987548828125\n",
      "Epoch: 1305 Loss : 114.20591735839844\n",
      "training loss------------->  239.5707244873047\n",
      "Epoch: 1306 Loss : 191.9438934326172\n",
      "training loss------------->  374.301513671875\n",
      "Epoch: 1307 Loss : 361.94970703125\n",
      "training loss------------->  560.8234252929688\n",
      "Epoch: 1308 Loss : 394.4262390136719\n",
      "training loss------------->  94.21924591064453\n",
      "Epoch: 1309 Loss : 529.3765258789062\n",
      "training loss------------->  317.6680908203125\n",
      "Epoch: 1310 Loss : 102.06073760986328\n",
      "training loss------------->  151.78952026367188\n",
      "Epoch: 1311 Loss : 310.07763671875\n",
      "training loss------------->  661.1154174804688\n",
      "Epoch: 1312 Loss : 488.782470703125\n",
      "training loss------------->  278.65240478515625\n",
      "Epoch: 1313 Loss : 153.05027770996094\n",
      "training loss------------->  420.03033447265625\n",
      "Epoch: 1314 Loss : 172.1101531982422\n",
      "training loss------------->  383.9040832519531\n",
      "Epoch: 1315 Loss : 247.2069854736328\n",
      "training loss------------->  368.5089111328125\n",
      "Epoch: 1316 Loss : 246.06689453125\n",
      "training loss------------->  268.5345458984375\n",
      "Epoch: 1317 Loss : 387.3189697265625\n",
      "training loss------------->  104.74128723144531\n",
      "Epoch: 1318 Loss : 145.029541015625\n",
      "training loss------------->  437.82086181640625\n",
      "Epoch: 1319 Loss : 196.22744750976562\n",
      "training loss------------->  549.4624633789062\n",
      "Epoch: 1320 Loss : 29.730342864990234\n",
      "training loss------------->  486.1877746582031\n",
      "Epoch: 1321 Loss : 462.63018798828125\n",
      "training loss------------->  444.6171569824219\n",
      "Epoch: 1322 Loss : 1129.79833984375\n",
      "training loss------------->  493.16796875\n",
      "Epoch: 1323 Loss : 38.532752990722656\n",
      "training loss------------->  753.853515625\n",
      "Epoch: 1324 Loss : 371.3059997558594\n",
      "training loss------------->  285.1854248046875\n",
      "Epoch: 1325 Loss : 604.2523193359375\n",
      "training loss------------->  508.07611083984375\n",
      "Epoch: 1326 Loss : 209.77462768554688\n",
      "training loss------------->  454.8111572265625\n",
      "Epoch: 1327 Loss : 781.424072265625\n",
      "training loss------------->  680.1251220703125\n",
      "Epoch: 1328 Loss : 207.50267028808594\n",
      "training loss------------->  434.4615783691406\n",
      "Epoch: 1329 Loss : 307.58740234375\n",
      "training loss------------->  297.9880065917969\n",
      "Epoch: 1330 Loss : 132.80776977539062\n",
      "training loss------------->  326.0327453613281\n",
      "Epoch: 1331 Loss : 302.1285400390625\n",
      "training loss------------->  560.5659790039062\n",
      "Epoch: 1332 Loss : 173.00672912597656\n",
      "training loss------------->  194.06243896484375\n",
      "Epoch: 1333 Loss : 447.2784423828125\n",
      "training loss------------->  342.7845153808594\n",
      "Epoch: 1334 Loss : 588.6453247070312\n",
      "training loss------------->  461.33343505859375\n",
      "Epoch: 1335 Loss : 488.69122314453125\n",
      "training loss------------->  341.02880859375\n",
      "Epoch: 1336 Loss : 48.033607482910156\n",
      "training loss------------->  303.2716369628906\n",
      "Epoch: 1337 Loss : 771.0404052734375\n",
      "training loss------------->  266.332763671875\n",
      "Epoch: 1338 Loss : 334.73065185546875\n",
      "training loss------------->  322.5608825683594\n",
      "Epoch: 1339 Loss : 335.7181396484375\n",
      "training loss------------->  481.44232177734375\n",
      "Epoch: 1340 Loss : 196.49188232421875\n",
      "training loss------------->  397.8328857421875\n",
      "Epoch: 1341 Loss : 83.96497344970703\n",
      "training loss------------->  350.8087463378906\n",
      "Epoch: 1342 Loss : 762.9244995117188\n",
      "training loss------------->  149.97698974609375\n",
      "Epoch: 1343 Loss : 317.6734313964844\n",
      "training loss------------->  706.9609375\n",
      "Epoch: 1344 Loss : 548.4921875\n",
      "training loss------------->  261.5501403808594\n",
      "Epoch: 1345 Loss : 317.7345886230469\n",
      "training loss------------->  427.0908508300781\n",
      "Epoch: 1346 Loss : 851.7789916992188\n",
      "training loss------------->  216.43356323242188\n",
      "Epoch: 1347 Loss : 484.5997009277344\n",
      "training loss------------->  518.3888549804688\n",
      "Epoch: 1348 Loss : 698.9335327148438\n",
      "training loss------------->  278.8688049316406\n",
      "Epoch: 1349 Loss : 419.94976806640625\n",
      "training loss------------->  243.62152099609375\n",
      "Epoch: 1350 Loss : 618.2355346679688\n",
      "training loss------------->  409.51177978515625\n",
      "Epoch: 1351 Loss : 212.82533264160156\n",
      "training loss------------->  325.3489990234375\n",
      "Epoch: 1352 Loss : 804.1151123046875\n",
      "training loss------------->  375.37664794921875\n",
      "Epoch: 1353 Loss : 283.1505126953125\n",
      "training loss------------->  402.5718688964844\n",
      "Epoch: 1354 Loss : 163.3378143310547\n",
      "training loss------------->  190.9976806640625\n",
      "Epoch: 1355 Loss : 651.7514038085938\n",
      "training loss------------->  187.17770385742188\n",
      "Epoch: 1356 Loss : 21.930240631103516\n",
      "training loss------------->  293.99725341796875\n",
      "Epoch: 1357 Loss : 148.91209411621094\n",
      "training loss------------->  311.5206604003906\n",
      "Epoch: 1358 Loss : 1005.468505859375\n",
      "training loss------------->  222.96360778808594\n",
      "Epoch: 1359 Loss : 228.6516876220703\n",
      "training loss------------->  432.7593994140625\n",
      "Epoch: 1360 Loss : 1150.8095703125\n",
      "training loss------------->  540.9038696289062\n",
      "Epoch: 1361 Loss : 434.76220703125\n",
      "training loss------------->  377.8233642578125\n",
      "Epoch: 1362 Loss : 393.1009216308594\n",
      "training loss------------->  364.3233947753906\n",
      "Epoch: 1363 Loss : 478.171142578125\n",
      "training loss------------->  357.8125\n",
      "Epoch: 1364 Loss : 757.4131469726562\n",
      "training loss------------->  355.2554931640625\n",
      "Epoch: 1365 Loss : 44.7232551574707\n",
      "training loss------------->  271.9555358886719\n",
      "Epoch: 1366 Loss : 573.7213134765625\n",
      "training loss------------->  361.3084716796875\n",
      "Epoch: 1367 Loss : 153.37423706054688\n",
      "training loss------------->  498.4976806640625\n",
      "Epoch: 1368 Loss : 855.6622314453125\n",
      "training loss------------->  309.76312255859375\n",
      "Epoch: 1369 Loss : 468.825439453125\n",
      "training loss------------->  195.7583770751953\n",
      "Epoch: 1370 Loss : 858.1405029296875\n",
      "training loss------------->  187.5110321044922\n",
      "Epoch: 1371 Loss : 315.32415771484375\n",
      "training loss------------->  205.65240478515625\n",
      "Epoch: 1372 Loss : 150.10968017578125\n",
      "training loss------------->  226.7042999267578\n",
      "Epoch: 1373 Loss : 663.1129150390625\n",
      "training loss------------->  395.95416259765625\n",
      "Epoch: 1374 Loss : 410.26043701171875\n",
      "training loss------------->  614.8633422851562\n",
      "Epoch: 1375 Loss : 440.3755798339844\n",
      "training loss------------->  257.7286682128906\n",
      "Epoch: 1376 Loss : 662.716796875\n",
      "training loss------------->  189.15370178222656\n",
      "Epoch: 1377 Loss : 198.2369384765625\n",
      "training loss------------->  259.99139404296875\n",
      "Epoch: 1378 Loss : 421.126953125\n",
      "training loss------------->  335.66070556640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1379 Loss : 532.0469970703125\n",
      "training loss------------->  336.2354736328125\n",
      "Epoch: 1380 Loss : 276.94488525390625\n",
      "training loss------------->  178.75634765625\n",
      "Epoch: 1381 Loss : 149.80877685546875\n",
      "training loss------------->  151.20957946777344\n",
      "Epoch: 1382 Loss : 213.67196655273438\n",
      "training loss------------->  581.2310180664062\n",
      "Epoch: 1383 Loss : 770.9685668945312\n",
      "training loss------------->  495.9501953125\n",
      "Epoch: 1384 Loss : 847.6582641601562\n",
      "training loss------------->  455.2044372558594\n",
      "Epoch: 1385 Loss : 116.8877182006836\n",
      "training loss------------->  485.90185546875\n",
      "Epoch: 1386 Loss : 56.96065902709961\n",
      "training loss------------->  426.7613830566406\n",
      "Epoch: 1387 Loss : 230.6994171142578\n",
      "training loss------------->  605.6767578125\n",
      "Epoch: 1388 Loss : 40.250511169433594\n",
      "training loss------------->  308.1336669921875\n",
      "Epoch: 1389 Loss : 416.1261291503906\n",
      "training loss------------->  263.2646789550781\n",
      "Epoch: 1390 Loss : 413.0102844238281\n",
      "training loss------------->  371.3323059082031\n",
      "Epoch: 1391 Loss : 707.4705810546875\n",
      "training loss------------->  306.0448303222656\n",
      "Epoch: 1392 Loss : 732.3953247070312\n",
      "training loss------------->  368.52392578125\n",
      "Epoch: 1393 Loss : 790.18017578125\n",
      "training loss------------->  653.1873779296875\n",
      "Epoch: 1394 Loss : 319.559326171875\n",
      "training loss------------->  790.2771606445312\n",
      "Epoch: 1395 Loss : 91.51277160644531\n",
      "training loss------------->  273.5097351074219\n",
      "Epoch: 1396 Loss : 250.2043914794922\n",
      "training loss------------->  456.17327880859375\n",
      "Epoch: 1397 Loss : 602.22412109375\n",
      "training loss------------->  350.6607971191406\n",
      "Epoch: 1398 Loss : 296.5045166015625\n",
      "training loss------------->  308.6488037109375\n",
      "Epoch: 1399 Loss : 185.77175903320312\n",
      "training loss------------->  399.8877868652344\n",
      "Epoch: 1400 Loss : 1082.700927734375\n",
      "training loss------------->  316.2492980957031\n",
      "Epoch: 1401 Loss : 659.0736694335938\n",
      "training loss------------->  166.94903564453125\n",
      "Epoch: 1402 Loss : 183.73138427734375\n",
      "training loss------------->  217.33892822265625\n",
      "Epoch: 1403 Loss : 176.98858642578125\n",
      "training loss------------->  545.2890625\n",
      "Epoch: 1404 Loss : 864.8582153320312\n",
      "training loss------------->  230.42706298828125\n",
      "Epoch: 1405 Loss : 102.02705383300781\n",
      "training loss------------->  428.1227111816406\n",
      "Epoch: 1406 Loss : 306.8935241699219\n",
      "training loss------------->  441.6501770019531\n",
      "Epoch: 1407 Loss : 158.58860778808594\n",
      "training loss------------->  530.2545166015625\n",
      "Epoch: 1408 Loss : 564.5213012695312\n",
      "training loss------------->  634.757568359375\n",
      "Epoch: 1409 Loss : 311.94976806640625\n",
      "training loss------------->  375.3709716796875\n",
      "Epoch: 1410 Loss : 346.14300537109375\n",
      "training loss------------->  305.45355224609375\n",
      "Epoch: 1411 Loss : 307.3898620605469\n",
      "training loss------------->  164.1051483154297\n",
      "Epoch: 1412 Loss : 393.5656433105469\n",
      "training loss------------->  650.4741821289062\n",
      "Epoch: 1413 Loss : 500.9411315917969\n",
      "training loss------------->  247.1085662841797\n",
      "Epoch: 1414 Loss : 485.1317443847656\n",
      "training loss------------->  324.08013916015625\n",
      "Epoch: 1415 Loss : 560.41650390625\n",
      "training loss------------->  350.5198974609375\n",
      "Epoch: 1416 Loss : 231.70797729492188\n",
      "training loss------------->  351.2826232910156\n",
      "Epoch: 1417 Loss : 286.1270446777344\n",
      "training loss------------->  250.78179931640625\n",
      "Epoch: 1418 Loss : 320.7981872558594\n",
      "training loss------------->  524.2332763671875\n",
      "Epoch: 1419 Loss : 369.81024169921875\n",
      "training loss------------->  228.49668884277344\n",
      "Epoch: 1420 Loss : 440.09954833984375\n",
      "training loss------------->  675.530517578125\n",
      "Epoch: 1421 Loss : 581.4871826171875\n",
      "training loss------------->  306.17462158203125\n",
      "Epoch: 1422 Loss : 311.10369873046875\n",
      "training loss------------->  443.5031433105469\n",
      "Epoch: 1423 Loss : 179.37681579589844\n",
      "training loss------------->  95.55118560791016\n",
      "Epoch: 1424 Loss : 689.9066162109375\n",
      "training loss------------->  532.079833984375\n",
      "Epoch: 1425 Loss : 621.9235229492188\n",
      "training loss------------->  241.87542724609375\n",
      "Epoch: 1426 Loss : 220.10943603515625\n",
      "training loss------------->  256.45184326171875\n",
      "Epoch: 1427 Loss : 495.7289123535156\n",
      "training loss------------->  291.2686767578125\n",
      "Epoch: 1428 Loss : 262.3027648925781\n",
      "training loss------------->  455.9006652832031\n",
      "Epoch: 1429 Loss : 178.17921447753906\n",
      "training loss------------->  524.2039184570312\n",
      "Epoch: 1430 Loss : 226.13943481445312\n",
      "training loss------------->  534.2178344726562\n",
      "Epoch: 1431 Loss : 219.990478515625\n",
      "training loss------------->  278.13641357421875\n",
      "Epoch: 1432 Loss : 204.24891662597656\n",
      "training loss------------->  295.7850341796875\n",
      "Epoch: 1433 Loss : 631.07421875\n",
      "training loss------------->  541.7582397460938\n",
      "Epoch: 1434 Loss : 345.3875427246094\n",
      "training loss------------->  272.3595275878906\n",
      "Epoch: 1435 Loss : 97.66207885742188\n",
      "training loss------------->  310.63677978515625\n",
      "Epoch: 1436 Loss : 336.68658447265625\n",
      "training loss------------->  389.2151794433594\n",
      "Epoch: 1437 Loss : 312.3890380859375\n",
      "training loss------------->  205.37669372558594\n",
      "Epoch: 1438 Loss : 192.90037536621094\n",
      "training loss------------->  455.4137878417969\n",
      "Epoch: 1439 Loss : 400.51751708984375\n",
      "training loss------------->  192.2144317626953\n",
      "Epoch: 1440 Loss : 441.2539978027344\n",
      "training loss------------->  347.2285461425781\n",
      "Epoch: 1441 Loss : 1014.9644775390625\n",
      "training loss------------->  700.7711791992188\n",
      "Epoch: 1442 Loss : 407.5576477050781\n",
      "training loss------------->  438.2993469238281\n",
      "Epoch: 1443 Loss : 385.2256164550781\n",
      "training loss------------->  144.39581298828125\n",
      "Epoch: 1444 Loss : 555.4266357421875\n",
      "training loss------------->  660.319091796875\n",
      "Epoch: 1445 Loss : 363.6739501953125\n",
      "training loss------------->  262.3104248046875\n",
      "Epoch: 1446 Loss : 369.7330322265625\n",
      "training loss------------->  502.0252380371094\n",
      "Epoch: 1447 Loss : 169.32864379882812\n",
      "training loss------------->  579.9625854492188\n",
      "Epoch: 1448 Loss : 197.59422302246094\n",
      "training loss------------->  150.77679443359375\n",
      "Epoch: 1449 Loss : 216.8248748779297\n",
      "training loss------------->  390.90789794921875\n",
      "Epoch: 1450 Loss : 433.73162841796875\n",
      "training loss------------->  472.2107238769531\n",
      "Epoch: 1451 Loss : 865.201171875\n",
      "training loss------------->  245.72276306152344\n",
      "Epoch: 1452 Loss : 163.1182403564453\n",
      "training loss------------->  675.419677734375\n",
      "Epoch: 1453 Loss : 278.3007507324219\n",
      "training loss------------->  375.3138122558594\n",
      "Epoch: 1454 Loss : 642.6072387695312\n",
      "training loss------------->  551.947509765625\n",
      "Epoch: 1455 Loss : 728.0592041015625\n",
      "training loss------------->  348.648681640625\n",
      "Epoch: 1456 Loss : 625.9619140625\n",
      "training loss------------->  423.8931884765625\n",
      "Epoch: 1457 Loss : 229.0770263671875\n",
      "training loss------------->  461.0846252441406\n",
      "Epoch: 1458 Loss : 921.56982421875\n",
      "training loss------------->  696.8338623046875\n",
      "Epoch: 1459 Loss : 478.5141906738281\n",
      "training loss------------->  181.65325927734375\n",
      "Epoch: 1460 Loss : 1938.2181396484375\n",
      "training loss------------->  291.5386047363281\n",
      "Epoch: 1461 Loss : 202.42974853515625\n",
      "training loss------------->  418.34307861328125\n",
      "Epoch: 1462 Loss : 582.1102905273438\n",
      "training loss------------->  419.40234375\n",
      "Epoch: 1463 Loss : 40.672454833984375\n",
      "training loss------------->  470.7169189453125\n",
      "Epoch: 1464 Loss : 177.13368225097656\n",
      "training loss------------->  138.29058837890625\n",
      "Epoch: 1465 Loss : 204.8506317138672\n",
      "training loss------------->  579.224853515625\n",
      "Epoch: 1466 Loss : 375.848876953125\n",
      "training loss------------->  495.5714111328125\n",
      "Epoch: 1467 Loss : 75.94041442871094\n",
      "training loss------------->  208.7090301513672\n",
      "Epoch: 1468 Loss : 149.52130126953125\n",
      "training loss------------->  286.4128112792969\n",
      "Epoch: 1469 Loss : 348.6295471191406\n",
      "training loss------------->  488.32562255859375\n",
      "Epoch: 1470 Loss : 343.2513122558594\n",
      "training loss------------->  105.0901870727539\n",
      "Epoch: 1471 Loss : 434.2330627441406\n",
      "training loss------------->  252.68438720703125\n",
      "Epoch: 1472 Loss : 601.726806640625\n",
      "training loss------------->  260.43621826171875\n",
      "Epoch: 1473 Loss : 35.18816375732422\n",
      "training loss------------->  342.4975891113281\n",
      "Epoch: 1474 Loss : 120.078125\n",
      "training loss------------->  287.18365478515625\n",
      "Epoch: 1475 Loss : 104.20524597167969\n",
      "training loss------------->  442.9644775390625\n",
      "Epoch: 1476 Loss : 271.9005432128906\n",
      "training loss------------->  457.2487487792969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1477 Loss : 166.91363525390625\n",
      "training loss------------->  507.5993957519531\n",
      "Epoch: 1478 Loss : 373.6542053222656\n",
      "training loss------------->  276.42578125\n",
      "Epoch: 1479 Loss : 1105.45947265625\n",
      "training loss------------->  481.7054138183594\n",
      "Epoch: 1480 Loss : 502.1200256347656\n",
      "training loss------------->  313.5788879394531\n",
      "Epoch: 1481 Loss : 226.27816772460938\n",
      "training loss------------->  388.91363525390625\n",
      "Epoch: 1482 Loss : 812.6571655273438\n",
      "training loss------------->  500.1202697753906\n",
      "Epoch: 1483 Loss : 31.80202865600586\n",
      "training loss------------->  196.43630981445312\n",
      "Epoch: 1484 Loss : 643.90234375\n",
      "training loss------------->  444.6695251464844\n",
      "Epoch: 1485 Loss : 329.6289978027344\n",
      "training loss------------->  229.10427856445312\n",
      "Epoch: 1486 Loss : 476.71929931640625\n",
      "training loss------------->  446.0643005371094\n",
      "Epoch: 1487 Loss : 467.3967590332031\n",
      "training loss------------->  279.1427001953125\n",
      "Epoch: 1488 Loss : 280.4483947753906\n",
      "training loss------------->  397.3317565917969\n",
      "Epoch: 1489 Loss : 143.31744384765625\n",
      "training loss------------->  488.4473876953125\n",
      "Epoch: 1490 Loss : 297.4664001464844\n",
      "training loss------------->  417.6292724609375\n",
      "Epoch: 1491 Loss : 479.76190185546875\n",
      "training loss------------->  389.3608703613281\n",
      "Epoch: 1492 Loss : 975.5008544921875\n",
      "training loss------------->  417.5605773925781\n",
      "Epoch: 1493 Loss : 766.7905883789062\n",
      "training loss------------->  562.2549438476562\n",
      "Epoch: 1494 Loss : 136.42507934570312\n",
      "training loss------------->  530.157470703125\n",
      "Epoch: 1495 Loss : 41.8064079284668\n",
      "training loss------------->  459.36083984375\n",
      "Epoch: 1496 Loss : 169.8688201904297\n",
      "training loss------------->  546.27880859375\n",
      "Epoch: 1497 Loss : 77.56927490234375\n",
      "training loss------------->  163.81854248046875\n",
      "Epoch: 1498 Loss : 120.02033996582031\n",
      "training loss------------->  456.806640625\n",
      "Epoch: 1499 Loss : 434.06402587890625\n",
      "training loss------------->  119.1961669921875\n",
      "Epoch: 1500 Loss : 195.99609375\n",
      "training loss------------->  191.59751892089844\n",
      "Epoch: 1501 Loss : 864.65283203125\n",
      "training loss------------->  326.3601989746094\n",
      "Epoch: 1502 Loss : 537.7643432617188\n",
      "training loss------------->  494.53070068359375\n",
      "Epoch: 1503 Loss : 27.632713317871094\n",
      "training loss------------->  279.3441162109375\n",
      "Epoch: 1504 Loss : 459.84100341796875\n",
      "training loss------------->  615.1194458007812\n",
      "Epoch: 1505 Loss : 487.2438659667969\n",
      "training loss------------->  148.04144287109375\n",
      "Epoch: 1506 Loss : 700.1602172851562\n",
      "training loss------------->  249.48651123046875\n",
      "Epoch: 1507 Loss : 198.49111938476562\n",
      "training loss------------->  240.00677490234375\n",
      "Epoch: 1508 Loss : 36.7769775390625\n",
      "training loss------------->  364.41229248046875\n",
      "Epoch: 1509 Loss : 157.48965454101562\n",
      "training loss------------->  260.5840759277344\n",
      "Epoch: 1510 Loss : 600.5361328125\n",
      "training loss------------->  370.8553466796875\n",
      "Epoch: 1511 Loss : 444.8152770996094\n",
      "training loss------------->  330.332275390625\n",
      "Epoch: 1512 Loss : 173.96885681152344\n",
      "training loss------------->  330.6790771484375\n",
      "Epoch: 1513 Loss : 1040.229248046875\n",
      "training loss------------->  477.3760986328125\n",
      "Epoch: 1514 Loss : 46.26301574707031\n",
      "training loss------------->  342.4249572753906\n",
      "Epoch: 1515 Loss : 1292.1168212890625\n",
      "training loss------------->  634.7656860351562\n",
      "Epoch: 1516 Loss : 421.95013427734375\n",
      "training loss------------->  375.1233825683594\n",
      "Epoch: 1517 Loss : 205.91322326660156\n",
      "training loss------------->  408.2164306640625\n",
      "Epoch: 1518 Loss : 633.3384399414062\n",
      "training loss------------->  304.2637939453125\n",
      "Epoch: 1519 Loss : 136.05072021484375\n",
      "training loss------------->  342.29364013671875\n",
      "Epoch: 1520 Loss : 407.9317932128906\n",
      "training loss------------->  318.54730224609375\n",
      "Epoch: 1521 Loss : 704.0062255859375\n",
      "training loss------------->  445.49365234375\n",
      "Epoch: 1522 Loss : 314.84356689453125\n",
      "training loss------------->  500.2518005371094\n",
      "Epoch: 1523 Loss : 1248.70361328125\n",
      "training loss------------->  512.5629272460938\n",
      "Epoch: 1524 Loss : 773.7999267578125\n",
      "training loss------------->  435.98468017578125\n",
      "Epoch: 1525 Loss : 424.0145263671875\n",
      "training loss------------->  350.2926940917969\n",
      "Epoch: 1526 Loss : 380.84259033203125\n",
      "training loss------------->  382.9967346191406\n",
      "Epoch: 1527 Loss : 718.00341796875\n",
      "training loss------------->  468.24993896484375\n",
      "Epoch: 1528 Loss : 289.33819580078125\n",
      "training loss------------->  761.0750732421875\n",
      "Epoch: 1529 Loss : 512.60400390625\n",
      "training loss------------->  564.205322265625\n",
      "Epoch: 1530 Loss : 165.29100036621094\n",
      "training loss------------->  241.63194274902344\n",
      "Epoch: 1531 Loss : 798.9077758789062\n",
      "training loss------------->  397.2203369140625\n",
      "Epoch: 1532 Loss : 353.07281494140625\n",
      "training loss------------->  393.87109375\n",
      "Epoch: 1533 Loss : 384.7547912597656\n",
      "training loss------------->  468.4378356933594\n",
      "Epoch: 1534 Loss : 817.2930908203125\n",
      "training loss------------->  546.8272705078125\n",
      "Epoch: 1535 Loss : 645.3369750976562\n",
      "training loss------------->  408.1878967285156\n",
      "Epoch: 1536 Loss : 168.35696411132812\n",
      "training loss------------->  352.06561279296875\n",
      "Epoch: 1537 Loss : 217.26199340820312\n",
      "training loss------------->  569.7166748046875\n",
      "Epoch: 1538 Loss : 439.9977722167969\n",
      "training loss------------->  155.9047393798828\n",
      "Epoch: 1539 Loss : 154.15773010253906\n",
      "training loss------------->  337.1150207519531\n",
      "Epoch: 1540 Loss : 414.7804870605469\n",
      "training loss------------->  525.124267578125\n",
      "Epoch: 1541 Loss : 468.5042419433594\n",
      "training loss------------->  472.29010009765625\n",
      "Epoch: 1542 Loss : 454.81622314453125\n",
      "training loss------------->  493.04840087890625\n",
      "Epoch: 1543 Loss : 141.35842895507812\n",
      "training loss------------->  547.450439453125\n",
      "Epoch: 1544 Loss : 226.38967895507812\n",
      "training loss------------->  330.763671875\n",
      "Epoch: 1545 Loss : 82.46126556396484\n",
      "training loss------------->  354.7442321777344\n",
      "Epoch: 1546 Loss : 208.90264892578125\n",
      "training loss------------->  490.8250427246094\n",
      "Epoch: 1547 Loss : 695.4390869140625\n",
      "training loss------------->  257.19354248046875\n",
      "Epoch: 1548 Loss : 494.386962890625\n",
      "training loss------------->  404.6853332519531\n",
      "Epoch: 1549 Loss : 43.71709060668945\n",
      "training loss------------->  242.51446533203125\n",
      "Epoch: 1550 Loss : 85.2506332397461\n",
      "training loss------------->  316.78656005859375\n",
      "Epoch: 1551 Loss : 256.29541015625\n",
      "training loss------------->  278.9917297363281\n",
      "Epoch: 1552 Loss : 106.05309295654297\n",
      "training loss------------->  380.6368713378906\n",
      "Epoch: 1553 Loss : 209.6376190185547\n",
      "training loss------------->  456.0218505859375\n",
      "Epoch: 1554 Loss : 135.55938720703125\n",
      "training loss------------->  257.66546630859375\n",
      "Epoch: 1555 Loss : 324.94287109375\n",
      "training loss------------->  809.295654296875\n",
      "Epoch: 1556 Loss : 335.3563537597656\n",
      "training loss------------->  166.36074829101562\n",
      "Epoch: 1557 Loss : 1402.3399658203125\n",
      "training loss------------->  407.5051574707031\n",
      "Epoch: 1558 Loss : 21.713153839111328\n",
      "training loss------------->  256.1883850097656\n",
      "Epoch: 1559 Loss : 424.9049987792969\n",
      "training loss------------->  538.9581298828125\n",
      "Epoch: 1560 Loss : 188.9945526123047\n",
      "training loss------------->  406.400390625\n",
      "Epoch: 1561 Loss : 95.65380859375\n",
      "training loss------------->  293.40185546875\n",
      "Epoch: 1562 Loss : 103.25218200683594\n",
      "training loss------------->  289.3643493652344\n",
      "Epoch: 1563 Loss : 824.164794921875\n",
      "training loss------------->  431.2012634277344\n",
      "Epoch: 1564 Loss : 190.91253662109375\n",
      "training loss------------->  678.6319580078125\n",
      "Epoch: 1565 Loss : 364.19049072265625\n",
      "training loss------------->  310.4500732421875\n",
      "Epoch: 1566 Loss : 732.794677734375\n",
      "training loss------------->  596.237060546875\n",
      "Epoch: 1567 Loss : 442.16180419921875\n",
      "training loss------------->  326.7730712890625\n",
      "Epoch: 1568 Loss : 91.69660949707031\n",
      "training loss------------->  431.4901123046875\n",
      "Epoch: 1569 Loss : 778.770263671875\n",
      "training loss------------->  654.5441284179688\n",
      "Epoch: 1570 Loss : 76.92727661132812\n",
      "training loss------------->  528.3851318359375\n",
      "Epoch: 1571 Loss : 599.5617065429688\n",
      "training loss------------->  485.53961181640625\n",
      "Epoch: 1572 Loss : 294.5686950683594\n",
      "training loss------------->  370.6658935546875\n",
      "Epoch: 1573 Loss : 5.514856338500977\n",
      "training loss------------->  482.6241760253906\n",
      "Epoch: 1574 Loss : 718.1929931640625\n",
      "training loss------------->  209.23313903808594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1575 Loss : 162.7003631591797\n",
      "training loss------------->  468.5192565917969\n",
      "Epoch: 1576 Loss : 137.45425415039062\n",
      "training loss------------->  650.1443481445312\n",
      "Epoch: 1577 Loss : 336.73516845703125\n",
      "training loss------------->  318.42144775390625\n",
      "Epoch: 1578 Loss : 399.9829406738281\n",
      "training loss------------->  220.16348266601562\n",
      "Epoch: 1579 Loss : 294.60540771484375\n",
      "training loss------------->  417.0008544921875\n",
      "Epoch: 1580 Loss : 921.7083129882812\n",
      "training loss------------->  665.2936401367188\n",
      "Epoch: 1581 Loss : 467.9299011230469\n",
      "training loss------------->  871.878662109375\n",
      "Epoch: 1582 Loss : 813.4537963867188\n",
      "training loss------------->  340.9471130371094\n",
      "Epoch: 1583 Loss : 75.90409851074219\n",
      "training loss------------->  373.4344482421875\n",
      "Epoch: 1584 Loss : 400.5914306640625\n",
      "training loss------------->  453.7777404785156\n",
      "Epoch: 1585 Loss : 383.6833801269531\n",
      "training loss------------->  431.8970031738281\n",
      "Epoch: 1586 Loss : 460.07470703125\n",
      "training loss------------->  308.8821716308594\n",
      "Epoch: 1587 Loss : 229.56259155273438\n",
      "training loss------------->  498.92291259765625\n",
      "Epoch: 1588 Loss : 226.5634765625\n",
      "training loss------------->  335.29638671875\n",
      "Epoch: 1589 Loss : 254.6870880126953\n",
      "training loss------------->  389.86834716796875\n",
      "Epoch: 1590 Loss : 66.71233367919922\n",
      "training loss------------->  297.6073913574219\n",
      "Epoch: 1591 Loss : 373.2382507324219\n",
      "training loss------------->  336.0089111328125\n",
      "Epoch: 1592 Loss : 199.6463623046875\n",
      "training loss------------->  471.88922119140625\n",
      "Epoch: 1593 Loss : 350.1744689941406\n",
      "training loss------------->  187.66749572753906\n",
      "Epoch: 1594 Loss : 348.9981994628906\n",
      "training loss------------->  421.5202941894531\n",
      "Epoch: 1595 Loss : 303.5003967285156\n",
      "training loss------------->  497.0329895019531\n",
      "Epoch: 1596 Loss : 248.2131805419922\n",
      "training loss------------->  579.55859375\n",
      "Epoch: 1597 Loss : 112.3818130493164\n",
      "training loss------------->  376.8426818847656\n",
      "Epoch: 1598 Loss : 11.048691749572754\n",
      "training loss------------->  576.5442504882812\n",
      "Epoch: 1599 Loss : 229.48797607421875\n",
      "training loss------------->  283.012451171875\n",
      "Epoch: 1600 Loss : 353.74163818359375\n",
      "training loss------------->  398.3258972167969\n",
      "Epoch: 1601 Loss : 81.7919921875\n",
      "training loss------------->  528.8800659179688\n",
      "Epoch: 1602 Loss : 399.9150695800781\n",
      "training loss------------->  494.3385925292969\n",
      "Epoch: 1603 Loss : 849.4346923828125\n",
      "training loss------------->  307.45208740234375\n",
      "Epoch: 1604 Loss : 257.1942443847656\n",
      "training loss------------->  273.9270324707031\n",
      "Epoch: 1605 Loss : 416.1650390625\n",
      "training loss------------->  283.3250732421875\n",
      "Epoch: 1606 Loss : 400.8254699707031\n",
      "training loss------------->  331.4939880371094\n",
      "Epoch: 1607 Loss : 159.0969696044922\n",
      "training loss------------->  589.032958984375\n",
      "Epoch: 1608 Loss : 572.42138671875\n",
      "training loss------------->  294.2579040527344\n",
      "Epoch: 1609 Loss : 532.4198608398438\n",
      "training loss------------->  308.0440368652344\n",
      "Epoch: 1610 Loss : 401.7139892578125\n",
      "training loss------------->  387.0779113769531\n",
      "Epoch: 1611 Loss : 581.723388671875\n",
      "training loss------------->  246.58547973632812\n",
      "Epoch: 1612 Loss : 789.896240234375\n",
      "training loss------------->  321.3485107421875\n",
      "Epoch: 1613 Loss : 1090.4552001953125\n",
      "training loss------------->  340.4443054199219\n",
      "Epoch: 1614 Loss : 359.47576904296875\n",
      "training loss------------->  323.7896728515625\n",
      "Epoch: 1615 Loss : 413.21356201171875\n",
      "training loss------------->  416.53668212890625\n",
      "Epoch: 1616 Loss : 446.531005859375\n",
      "training loss------------->  538.2987670898438\n",
      "Epoch: 1617 Loss : 96.00465393066406\n",
      "training loss------------->  130.61526489257812\n",
      "Epoch: 1618 Loss : 805.1092529296875\n",
      "training loss------------->  583.2360229492188\n",
      "Epoch: 1619 Loss : 518.99951171875\n",
      "training loss------------->  502.90240478515625\n",
      "Epoch: 1620 Loss : 223.95216369628906\n",
      "training loss------------->  293.5982666015625\n",
      "Epoch: 1621 Loss : 590.2386474609375\n",
      "training loss------------->  487.0374755859375\n",
      "Epoch: 1622 Loss : 167.74444580078125\n",
      "training loss------------->  145.05592346191406\n",
      "Epoch: 1623 Loss : 137.9672088623047\n",
      "training loss------------->  638.980224609375\n",
      "Epoch: 1624 Loss : 209.435302734375\n",
      "training loss------------->  178.49610900878906\n",
      "Epoch: 1625 Loss : 544.6011962890625\n",
      "training loss------------->  377.7065124511719\n",
      "Epoch: 1626 Loss : 617.4151000976562\n",
      "training loss------------->  341.6522216796875\n",
      "Epoch: 1627 Loss : 234.2373046875\n",
      "training loss------------->  391.2269592285156\n",
      "Epoch: 1628 Loss : 681.9871826171875\n",
      "training loss------------->  458.88134765625\n",
      "Epoch: 1629 Loss : 365.1263427734375\n",
      "training loss------------->  388.49835205078125\n",
      "Epoch: 1630 Loss : 365.50555419921875\n",
      "training loss------------->  708.7066040039062\n",
      "Epoch: 1631 Loss : 453.94464111328125\n",
      "training loss------------->  107.95437622070312\n",
      "Epoch: 1632 Loss : 1021.7581787109375\n",
      "training loss------------->  141.8725128173828\n",
      "Epoch: 1633 Loss : 129.99671936035156\n",
      "training loss------------->  363.0118713378906\n",
      "Epoch: 1634 Loss : 169.85800170898438\n",
      "training loss------------->  438.96368408203125\n",
      "Epoch: 1635 Loss : 145.9708251953125\n",
      "training loss------------->  502.91546630859375\n",
      "Epoch: 1636 Loss : 1242.9183349609375\n",
      "training loss------------->  472.8248596191406\n",
      "Epoch: 1637 Loss : 1933.824462890625\n",
      "training loss------------->  581.0851440429688\n",
      "Epoch: 1638 Loss : 500.7576904296875\n",
      "training loss------------->  345.6681823730469\n",
      "Epoch: 1639 Loss : 954.8079833984375\n",
      "training loss------------->  213.23089599609375\n",
      "Epoch: 1640 Loss : 755.3536376953125\n",
      "training loss------------->  556.44970703125\n",
      "Epoch: 1641 Loss : 137.72906494140625\n",
      "training loss------------->  483.4195556640625\n",
      "Epoch: 1642 Loss : 284.42364501953125\n",
      "training loss------------->  415.6528015136719\n",
      "Epoch: 1643 Loss : 406.75238037109375\n",
      "training loss------------->  847.429443359375\n",
      "Epoch: 1644 Loss : 676.5231323242188\n",
      "training loss------------->  381.9781188964844\n",
      "Epoch: 1645 Loss : 99.30281066894531\n",
      "training loss------------->  275.3145751953125\n",
      "Epoch: 1646 Loss : 426.6630859375\n",
      "training loss------------->  344.8035888671875\n",
      "Epoch: 1647 Loss : 138.4346923828125\n",
      "training loss------------->  173.49459838867188\n",
      "Epoch: 1648 Loss : 118.70540618896484\n",
      "training loss------------->  292.5594177246094\n",
      "Epoch: 1649 Loss : 489.00164794921875\n",
      "training loss------------->  608.1244506835938\n",
      "Epoch: 1650 Loss : 591.4011840820312\n",
      "training loss------------->  371.3421936035156\n",
      "Epoch: 1651 Loss : 50.15312576293945\n",
      "training loss------------->  418.6530456542969\n",
      "Epoch: 1652 Loss : 577.6290283203125\n",
      "training loss------------->  633.7576904296875\n",
      "Epoch: 1653 Loss : 581.64013671875\n",
      "training loss------------->  322.1181640625\n",
      "Epoch: 1654 Loss : 55.358097076416016\n",
      "training loss------------->  361.6661682128906\n",
      "Epoch: 1655 Loss : 542.1948852539062\n",
      "training loss------------->  481.9915466308594\n",
      "Epoch: 1656 Loss : 929.1493530273438\n",
      "training loss------------->  237.05398559570312\n",
      "Epoch: 1657 Loss : 461.33349609375\n",
      "training loss------------->  324.2540283203125\n",
      "Epoch: 1658 Loss : 366.9316711425781\n",
      "training loss------------->  590.1889038085938\n",
      "Epoch: 1659 Loss : 441.133056640625\n",
      "training loss------------->  635.822998046875\n",
      "Epoch: 1660 Loss : 418.3288269042969\n",
      "training loss------------->  250.92979431152344\n",
      "Epoch: 1661 Loss : 126.52021789550781\n",
      "training loss------------->  281.9668273925781\n",
      "Epoch: 1662 Loss : 457.3857727050781\n",
      "training loss------------->  204.36538696289062\n",
      "Epoch: 1663 Loss : 1098.2076416015625\n",
      "training loss------------->  535.832763671875\n",
      "Epoch: 1664 Loss : 623.512451171875\n",
      "training loss------------->  322.0265808105469\n",
      "Epoch: 1665 Loss : 155.18292236328125\n",
      "training loss------------->  250.130615234375\n",
      "Epoch: 1666 Loss : 196.68687438964844\n",
      "training loss------------->  477.9425964355469\n",
      "Epoch: 1667 Loss : 583.9638671875\n",
      "training loss------------->  393.7776794433594\n",
      "Epoch: 1668 Loss : 363.67352294921875\n",
      "training loss------------->  422.6863708496094\n",
      "Epoch: 1669 Loss : 446.1275939941406\n",
      "training loss------------->  317.6119384765625\n",
      "Epoch: 1670 Loss : 222.82113647460938\n",
      "training loss------------->  494.5539245605469\n",
      "Epoch: 1671 Loss : 494.02166748046875\n",
      "training loss------------->  281.50604248046875\n",
      "Epoch: 1672 Loss : 13.636661529541016\n",
      "training loss------------->  259.20599365234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1673 Loss : 15.343598365783691\n",
      "training loss------------->  299.5289306640625\n",
      "Epoch: 1674 Loss : 692.9012451171875\n",
      "training loss------------->  590.686767578125\n",
      "Epoch: 1675 Loss : 943.42529296875\n",
      "training loss------------->  372.6621398925781\n",
      "Epoch: 1676 Loss : 309.4804992675781\n",
      "training loss------------->  517.975830078125\n",
      "Epoch: 1677 Loss : 656.3872680664062\n",
      "training loss------------->  63.18100357055664\n",
      "Epoch: 1678 Loss : 1013.1051025390625\n",
      "training loss------------->  402.0299072265625\n",
      "Epoch: 1679 Loss : 328.1600036621094\n",
      "training loss------------->  287.6165771484375\n",
      "Epoch: 1680 Loss : 478.90673828125\n",
      "training loss------------->  88.30221557617188\n",
      "Epoch: 1681 Loss : 389.20916748046875\n",
      "training loss------------->  719.6553955078125\n",
      "Epoch: 1682 Loss : 39.95215606689453\n",
      "training loss------------->  306.91937255859375\n",
      "Epoch: 1683 Loss : 75.08480072021484\n",
      "training loss------------->  167.11268615722656\n",
      "Epoch: 1684 Loss : 32.440345764160156\n",
      "training loss------------->  235.59197998046875\n",
      "Epoch: 1685 Loss : 811.0521850585938\n",
      "training loss------------->  255.66400146484375\n",
      "Epoch: 1686 Loss : 111.17457580566406\n",
      "training loss------------->  354.4658508300781\n",
      "Epoch: 1687 Loss : 430.83563232421875\n",
      "training loss------------->  182.974853515625\n",
      "Epoch: 1688 Loss : 88.10691833496094\n",
      "training loss------------->  289.5574951171875\n",
      "Epoch: 1689 Loss : 1038.990234375\n",
      "training loss------------->  411.1278076171875\n",
      "Epoch: 1690 Loss : 724.020751953125\n",
      "training loss------------->  512.9614868164062\n",
      "Epoch: 1691 Loss : 633.8946533203125\n",
      "training loss------------->  368.7799987792969\n",
      "Epoch: 1692 Loss : 232.22642517089844\n",
      "training loss------------->  524.0748291015625\n",
      "Epoch: 1693 Loss : 956.3472900390625\n",
      "training loss------------->  202.82249450683594\n",
      "Epoch: 1694 Loss : 604.131591796875\n",
      "training loss------------->  395.3775634765625\n",
      "Epoch: 1695 Loss : 1176.5509033203125\n",
      "training loss------------->  750.427001953125\n",
      "Epoch: 1696 Loss : 1660.76806640625\n",
      "training loss------------->  200.57351684570312\n",
      "Epoch: 1697 Loss : 312.2852783203125\n",
      "training loss------------->  402.94769287109375\n",
      "Epoch: 1698 Loss : 525.3292846679688\n",
      "training loss------------->  372.3896179199219\n",
      "Epoch: 1699 Loss : 460.804931640625\n",
      "training loss------------->  375.6380310058594\n",
      "Epoch: 1700 Loss : 150.78025817871094\n",
      "training loss------------->  390.6298828125\n",
      "Epoch: 1701 Loss : 34.57550811767578\n",
      "training loss------------->  193.4449920654297\n",
      "Epoch: 1702 Loss : 536.1143798828125\n",
      "training loss------------->  120.80374908447266\n",
      "Epoch: 1703 Loss : 488.2952575683594\n",
      "training loss------------->  219.92242431640625\n",
      "Epoch: 1704 Loss : 247.13743591308594\n",
      "training loss------------->  457.2555847167969\n",
      "Epoch: 1705 Loss : 455.56512451171875\n",
      "training loss------------->  465.91748046875\n",
      "Epoch: 1706 Loss : 1107.019287109375\n",
      "training loss------------->  832.7474365234375\n",
      "Epoch: 1707 Loss : 147.41188049316406\n",
      "training loss------------->  488.87445068359375\n",
      "Epoch: 1708 Loss : 555.2244873046875\n",
      "training loss------------->  557.13427734375\n",
      "Epoch: 1709 Loss : 595.9713134765625\n",
      "training loss------------->  432.04046630859375\n",
      "Epoch: 1710 Loss : 114.75811767578125\n",
      "training loss------------->  340.8988037109375\n",
      "Epoch: 1711 Loss : 331.3300476074219\n",
      "training loss------------->  669.4560546875\n",
      "Epoch: 1712 Loss : 664.5038452148438\n",
      "training loss------------->  449.89239501953125\n",
      "Epoch: 1713 Loss : 1676.0340576171875\n",
      "training loss------------->  410.85784912109375\n",
      "Epoch: 1714 Loss : 952.3587646484375\n",
      "training loss------------->  258.6716613769531\n",
      "Epoch: 1715 Loss : 303.37994384765625\n",
      "training loss------------->  436.3591613769531\n",
      "Epoch: 1716 Loss : 360.2601013183594\n",
      "training loss------------->  335.7979736328125\n",
      "Epoch: 1717 Loss : 440.0158996582031\n",
      "training loss------------->  877.6170043945312\n",
      "Epoch: 1718 Loss : 1257.9486083984375\n",
      "training loss------------->  200.90084838867188\n",
      "Epoch: 1719 Loss : 260.12176513671875\n",
      "training loss------------->  339.8079833984375\n",
      "Epoch: 1720 Loss : 636.7103271484375\n",
      "training loss------------->  364.14276123046875\n",
      "Epoch: 1721 Loss : 584.1390991210938\n",
      "training loss------------->  580.00537109375\n",
      "Epoch: 1722 Loss : 1922.9091796875\n",
      "training loss------------->  298.51116943359375\n",
      "Epoch: 1723 Loss : 841.0462646484375\n",
      "training loss------------->  480.7628173828125\n",
      "Epoch: 1724 Loss : 238.7358856201172\n",
      "training loss------------->  274.4796142578125\n",
      "Epoch: 1725 Loss : 757.2698364257812\n",
      "training loss------------->  214.4014129638672\n",
      "Epoch: 1726 Loss : 42.850555419921875\n",
      "training loss------------->  691.5718383789062\n",
      "Epoch: 1727 Loss : 288.35546875\n",
      "training loss------------->  475.0997314453125\n",
      "Epoch: 1728 Loss : 270.824951171875\n",
      "training loss------------->  270.81317138671875\n",
      "Epoch: 1729 Loss : 630.1921997070312\n",
      "training loss------------->  420.3504638671875\n",
      "Epoch: 1730 Loss : 469.55059814453125\n",
      "training loss------------->  204.12921142578125\n",
      "Epoch: 1731 Loss : 530.3862915039062\n",
      "training loss------------->  388.3208312988281\n",
      "Epoch: 1732 Loss : 208.75445556640625\n",
      "training loss------------->  641.8646240234375\n",
      "Epoch: 1733 Loss : 263.97247314453125\n",
      "training loss------------->  102.6173324584961\n",
      "Epoch: 1734 Loss : 721.8977661132812\n",
      "training loss------------->  436.43353271484375\n",
      "Epoch: 1735 Loss : 1017.486083984375\n",
      "training loss------------->  561.9141235351562\n",
      "Epoch: 1736 Loss : 166.7529296875\n",
      "training loss------------->  80.27408599853516\n",
      "Epoch: 1737 Loss : 52.24712371826172\n",
      "training loss------------->  602.4129638671875\n",
      "Epoch: 1738 Loss : 336.5133056640625\n",
      "training loss------------->  355.1148986816406\n",
      "Epoch: 1739 Loss : 770.3365478515625\n",
      "training loss------------->  578.6472778320312\n",
      "Epoch: 1740 Loss : 416.91180419921875\n",
      "training loss------------->  274.12664794921875\n",
      "Epoch: 1741 Loss : 594.2631225585938\n",
      "training loss------------->  218.23165893554688\n",
      "Epoch: 1742 Loss : 296.0277404785156\n",
      "training loss------------->  414.4742431640625\n",
      "Epoch: 1743 Loss : 121.4717025756836\n",
      "training loss------------->  604.998779296875\n",
      "Epoch: 1744 Loss : 913.3814086914062\n",
      "training loss------------->  389.9183349609375\n",
      "Epoch: 1745 Loss : 596.9794921875\n",
      "training loss------------->  349.6549072265625\n",
      "Epoch: 1746 Loss : 475.7935791015625\n",
      "training loss------------->  212.21026611328125\n",
      "Epoch: 1747 Loss : 466.5108642578125\n",
      "training loss------------->  680.8162841796875\n",
      "Epoch: 1748 Loss : 357.6630859375\n",
      "training loss------------->  349.6194152832031\n",
      "Epoch: 1749 Loss : 1253.0323486328125\n",
      "training loss------------->  501.8819580078125\n",
      "Epoch: 1750 Loss : 800.5885009765625\n",
      "training loss------------->  604.33056640625\n",
      "Epoch: 1751 Loss : 518.9410400390625\n",
      "training loss------------->  440.2647705078125\n",
      "Epoch: 1752 Loss : 98.36985778808594\n",
      "training loss------------->  621.1069946289062\n",
      "Epoch: 1753 Loss : 615.6397705078125\n",
      "training loss------------->  541.573974609375\n",
      "Epoch: 1754 Loss : 42.088111877441406\n",
      "training loss------------->  552.1110229492188\n",
      "Epoch: 1755 Loss : 126.0898666381836\n",
      "training loss------------->  561.111572265625\n",
      "Epoch: 1756 Loss : 852.1033325195312\n",
      "training loss------------->  516.7515869140625\n",
      "Epoch: 1757 Loss : 455.522705078125\n",
      "training loss------------->  161.43829345703125\n",
      "Epoch: 1758 Loss : 798.3497314453125\n",
      "training loss------------->  617.876220703125\n",
      "Epoch: 1759 Loss : 821.362548828125\n",
      "training loss------------->  562.9736328125\n",
      "Epoch: 1760 Loss : 517.3788452148438\n",
      "training loss------------->  494.6756591796875\n",
      "Epoch: 1761 Loss : 656.3114624023438\n",
      "training loss------------->  402.1085205078125\n",
      "Epoch: 1762 Loss : 58.03120803833008\n",
      "training loss------------->  318.2757873535156\n",
      "Epoch: 1763 Loss : 500.5708312988281\n",
      "training loss------------->  443.4899597167969\n",
      "Epoch: 1764 Loss : 324.7796936035156\n",
      "training loss------------->  212.85601806640625\n",
      "Epoch: 1765 Loss : 417.61737060546875\n",
      "training loss------------->  384.4384765625\n",
      "Epoch: 1766 Loss : 55.85218048095703\n",
      "training loss------------->  441.0073547363281\n",
      "Epoch: 1767 Loss : 752.8135986328125\n",
      "training loss------------->  341.0994567871094\n",
      "Epoch: 1768 Loss : 76.44548034667969\n",
      "training loss------------->  215.77377319335938\n",
      "Epoch: 1769 Loss : 406.98077392578125\n",
      "training loss------------->  463.5227966308594\n",
      "Epoch: 1770 Loss : 248.7731170654297\n",
      "training loss------------->  387.19915771484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1771 Loss : 518.2367553710938\n",
      "training loss------------->  557.1174926757812\n",
      "Epoch: 1772 Loss : 611.126220703125\n",
      "training loss------------->  213.7799530029297\n",
      "Epoch: 1773 Loss : 288.8436584472656\n",
      "training loss------------->  564.24658203125\n",
      "Epoch: 1774 Loss : 353.88568115234375\n",
      "training loss------------->  429.0544128417969\n",
      "Epoch: 1775 Loss : 318.7977600097656\n",
      "training loss------------->  108.42929077148438\n",
      "Epoch: 1776 Loss : 540.66845703125\n",
      "training loss------------->  318.2191467285156\n",
      "Epoch: 1777 Loss : 243.5813446044922\n",
      "training loss------------->  199.9134521484375\n",
      "Epoch: 1778 Loss : 238.9208984375\n",
      "training loss------------->  415.7751770019531\n",
      "Epoch: 1779 Loss : 417.0712585449219\n",
      "training loss------------->  358.3907775878906\n",
      "Epoch: 1780 Loss : 647.061767578125\n",
      "training loss------------->  421.51983642578125\n",
      "Epoch: 1781 Loss : 1003.9708862304688\n",
      "training loss------------->  574.6650390625\n",
      "Epoch: 1782 Loss : 568.9920654296875\n",
      "training loss------------->  328.2194519042969\n",
      "Epoch: 1783 Loss : 328.82989501953125\n",
      "training loss------------->  167.66571044921875\n",
      "Epoch: 1784 Loss : 309.0473937988281\n",
      "training loss------------->  443.214599609375\n",
      "Epoch: 1785 Loss : 238.45004272460938\n",
      "training loss------------->  342.0864562988281\n",
      "Epoch: 1786 Loss : 215.14080810546875\n",
      "training loss------------->  593.895751953125\n",
      "Epoch: 1787 Loss : 430.6807861328125\n",
      "training loss------------->  634.081298828125\n",
      "Epoch: 1788 Loss : 1770.447021484375\n",
      "training loss------------->  252.80628967285156\n",
      "Epoch: 1789 Loss : 223.12664794921875\n",
      "training loss------------->  410.23480224609375\n",
      "Epoch: 1790 Loss : 437.0240783691406\n",
      "training loss------------->  673.9727172851562\n",
      "Epoch: 1791 Loss : 60.90340042114258\n",
      "training loss------------->  296.10540771484375\n",
      "Epoch: 1792 Loss : 76.22908020019531\n",
      "training loss------------->  555.39501953125\n",
      "Epoch: 1793 Loss : 539.6046142578125\n",
      "training loss------------->  281.2425537109375\n",
      "Epoch: 1794 Loss : 408.03106689453125\n",
      "training loss------------->  298.660400390625\n",
      "Epoch: 1795 Loss : 89.53580474853516\n",
      "training loss------------->  533.394775390625\n",
      "Epoch: 1796 Loss : 598.0576171875\n",
      "training loss------------->  358.0697021484375\n",
      "Epoch: 1797 Loss : 356.9745788574219\n",
      "training loss------------->  304.3352966308594\n",
      "Epoch: 1798 Loss : 990.3887939453125\n",
      "training loss------------->  329.29510498046875\n",
      "Epoch: 1799 Loss : 283.452392578125\n",
      "training loss------------->  234.73541259765625\n",
      "Epoch: 1800 Loss : 308.6883239746094\n",
      "training loss------------->  405.9136962890625\n",
      "Epoch: 1801 Loss : 523.0192260742188\n",
      "training loss------------->  364.52911376953125\n",
      "Epoch: 1802 Loss : 313.945068359375\n",
      "training loss------------->  107.67498779296875\n",
      "Epoch: 1803 Loss : 233.9666290283203\n",
      "training loss------------->  420.56634521484375\n",
      "Epoch: 1804 Loss : 397.1055908203125\n",
      "training loss------------->  338.2516784667969\n",
      "Epoch: 1805 Loss : 881.940185546875\n",
      "training loss------------->  535.1986083984375\n",
      "Epoch: 1806 Loss : 112.04615020751953\n",
      "training loss------------->  405.2107238769531\n",
      "Epoch: 1807 Loss : 97.8468017578125\n",
      "training loss------------->  549.2593994140625\n",
      "Epoch: 1808 Loss : 232.748291015625\n",
      "training loss------------->  350.13763427734375\n",
      "Epoch: 1809 Loss : 402.56182861328125\n",
      "training loss------------->  283.5171203613281\n",
      "Epoch: 1810 Loss : 714.3773193359375\n",
      "training loss------------->  211.59957885742188\n",
      "Epoch: 1811 Loss : 210.2360076904297\n",
      "training loss------------->  572.2882080078125\n",
      "Epoch: 1812 Loss : 69.19192504882812\n",
      "training loss------------->  715.1588745117188\n",
      "Epoch: 1813 Loss : 486.3380126953125\n",
      "training loss------------->  250.06829833984375\n",
      "Epoch: 1814 Loss : 286.43310546875\n",
      "training loss------------->  265.76873779296875\n",
      "Epoch: 1815 Loss : 491.24310302734375\n",
      "training loss------------->  430.20074462890625\n",
      "Epoch: 1816 Loss : 434.0274353027344\n",
      "training loss------------->  329.5625\n",
      "Epoch: 1817 Loss : 841.248046875\n",
      "training loss------------->  286.05316162109375\n",
      "Epoch: 1818 Loss : 127.59524536132812\n",
      "training loss------------->  540.954833984375\n",
      "Epoch: 1819 Loss : 333.302734375\n",
      "training loss------------->  388.51214599609375\n",
      "Epoch: 1820 Loss : 499.4148864746094\n",
      "training loss------------->  472.39599609375\n",
      "Epoch: 1821 Loss : 97.27671813964844\n",
      "training loss------------->  374.6882019042969\n",
      "Epoch: 1822 Loss : 349.5003967285156\n",
      "training loss------------->  214.58985900878906\n",
      "Epoch: 1823 Loss : 355.7881774902344\n",
      "training loss------------->  288.3572082519531\n",
      "Epoch: 1824 Loss : 540.2944946289062\n",
      "training loss------------->  157.98387145996094\n",
      "Epoch: 1825 Loss : 137.90696716308594\n",
      "training loss------------->  318.7854309082031\n",
      "Epoch: 1826 Loss : 456.85919189453125\n",
      "training loss------------->  192.12879943847656\n",
      "Epoch: 1827 Loss : 559.9234619140625\n",
      "training loss------------->  307.8302307128906\n",
      "Epoch: 1828 Loss : 156.81393432617188\n",
      "training loss------------->  420.06243896484375\n",
      "Epoch: 1829 Loss : 291.4756774902344\n",
      "training loss------------->  416.5686340332031\n",
      "Epoch: 1830 Loss : 476.468017578125\n",
      "training loss------------->  423.0726013183594\n",
      "Epoch: 1831 Loss : 104.15103149414062\n",
      "training loss------------->  379.31695556640625\n",
      "Epoch: 1832 Loss : 698.7710571289062\n",
      "training loss------------->  476.3525390625\n",
      "Epoch: 1833 Loss : 215.53428649902344\n",
      "training loss------------->  337.79278564453125\n",
      "Epoch: 1834 Loss : 562.0406494140625\n",
      "training loss------------->  617.350830078125\n",
      "Epoch: 1835 Loss : 263.72711181640625\n",
      "training loss------------->  79.61607360839844\n",
      "Epoch: 1836 Loss : 584.33203125\n",
      "training loss------------->  363.8136901855469\n",
      "Epoch: 1837 Loss : 30.883697509765625\n",
      "training loss------------->  650.4033813476562\n",
      "Epoch: 1838 Loss : 823.3321533203125\n",
      "training loss------------->  903.7900390625\n",
      "Epoch: 1839 Loss : 165.59584045410156\n",
      "training loss------------->  289.7847595214844\n",
      "Epoch: 1840 Loss : 322.0984191894531\n",
      "training loss------------->  541.4026489257812\n",
      "Epoch: 1841 Loss : 740.8238525390625\n",
      "training loss------------->  185.29579162597656\n",
      "Epoch: 1842 Loss : 716.389404296875\n",
      "training loss------------->  331.0677490234375\n",
      "Epoch: 1843 Loss : 969.5042724609375\n",
      "training loss------------->  502.2288818359375\n",
      "Epoch: 1844 Loss : 437.4224548339844\n",
      "training loss------------->  707.575439453125\n",
      "Epoch: 1845 Loss : 146.87655639648438\n",
      "training loss------------->  365.3983154296875\n",
      "Epoch: 1846 Loss : 132.78590393066406\n",
      "training loss------------->  411.581298828125\n",
      "Epoch: 1847 Loss : 245.31346130371094\n",
      "training loss------------->  412.0550537109375\n",
      "Epoch: 1848 Loss : 769.9488525390625\n",
      "training loss------------->  221.49330139160156\n",
      "Epoch: 1849 Loss : 160.62506103515625\n",
      "training loss------------->  467.5110168457031\n",
      "Epoch: 1850 Loss : 258.9601135253906\n",
      "training loss------------->  397.7213134765625\n",
      "Epoch: 1851 Loss : 449.1076354980469\n",
      "training loss------------->  266.1351318359375\n",
      "Epoch: 1852 Loss : 529.351806640625\n",
      "training loss------------->  345.9508056640625\n",
      "Epoch: 1853 Loss : 335.7962341308594\n",
      "training loss------------->  274.89776611328125\n",
      "Epoch: 1854 Loss : 665.2396240234375\n",
      "training loss------------->  288.2267761230469\n",
      "Epoch: 1855 Loss : 1067.3629150390625\n",
      "training loss------------->  564.1703491210938\n",
      "Epoch: 1856 Loss : 115.81334686279297\n",
      "training loss------------->  255.39515686035156\n",
      "Epoch: 1857 Loss : 259.2671203613281\n",
      "training loss------------->  314.4483947753906\n",
      "Epoch: 1858 Loss : 771.84912109375\n",
      "training loss------------->  445.0318298339844\n",
      "Epoch: 1859 Loss : 357.5237121582031\n",
      "training loss------------->  351.2577819824219\n",
      "Epoch: 1860 Loss : 727.1382446289062\n",
      "training loss------------->  254.75054931640625\n",
      "Epoch: 1861 Loss : 51.0079231262207\n",
      "training loss------------->  341.7017517089844\n",
      "Epoch: 1862 Loss : 229.97857666015625\n",
      "training loss------------->  476.8009033203125\n",
      "Epoch: 1863 Loss : 631.9686889648438\n",
      "training loss------------->  212.79989624023438\n",
      "Epoch: 1864 Loss : 178.23870849609375\n",
      "training loss------------->  338.222900390625\n",
      "Epoch: 1865 Loss : 349.8738708496094\n",
      "training loss------------->  191.29019165039062\n",
      "Epoch: 1866 Loss : 270.2966613769531\n",
      "training loss------------->  670.7101440429688\n",
      "Epoch: 1867 Loss : 674.4007568359375\n",
      "training loss------------->  277.7414855957031\n",
      "Epoch: 1868 Loss : 641.4902954101562\n",
      "training loss------------->  308.2578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1869 Loss : 267.328857421875\n",
      "training loss------------->  396.4964294433594\n",
      "Epoch: 1870 Loss : 14.094141006469727\n",
      "training loss------------->  531.3442993164062\n",
      "Epoch: 1871 Loss : 455.14654541015625\n",
      "training loss------------->  352.1634216308594\n",
      "Epoch: 1872 Loss : 399.32574462890625\n",
      "training loss------------->  159.3826141357422\n",
      "Epoch: 1873 Loss : 818.5347900390625\n",
      "training loss------------->  283.1654052734375\n",
      "Epoch: 1874 Loss : 152.2107696533203\n",
      "training loss------------->  310.0457763671875\n",
      "Epoch: 1875 Loss : 59.257652282714844\n",
      "training loss------------->  304.3788757324219\n",
      "Epoch: 1876 Loss : 326.5618896484375\n",
      "training loss------------->  166.59808349609375\n",
      "Epoch: 1877 Loss : 161.75852966308594\n",
      "training loss------------->  213.59169006347656\n",
      "Epoch: 1878 Loss : 323.0368347167969\n",
      "training loss------------->  210.11318969726562\n",
      "Epoch: 1879 Loss : 492.2985534667969\n",
      "training loss------------->  470.1800231933594\n",
      "Epoch: 1880 Loss : 669.6380615234375\n",
      "training loss------------->  271.34246826171875\n",
      "Epoch: 1881 Loss : 477.24737548828125\n",
      "training loss------------->  456.64459228515625\n",
      "Epoch: 1882 Loss : 792.18310546875\n",
      "training loss------------->  519.4207763671875\n",
      "Epoch: 1883 Loss : 329.38763427734375\n",
      "training loss------------->  140.82672119140625\n",
      "Epoch: 1884 Loss : 213.84121704101562\n",
      "training loss------------->  540.122314453125\n",
      "Epoch: 1885 Loss : 146.33680725097656\n",
      "training loss------------->  670.7377319335938\n",
      "Epoch: 1886 Loss : 43.4621696472168\n",
      "training loss------------->  399.3144226074219\n",
      "Epoch: 1887 Loss : 235.70567321777344\n",
      "training loss------------->  200.66514587402344\n",
      "Epoch: 1888 Loss : 488.0261535644531\n",
      "training loss------------->  499.5818176269531\n",
      "Epoch: 1889 Loss : 324.4085388183594\n",
      "training loss------------->  422.33148193359375\n",
      "Epoch: 1890 Loss : 156.04055786132812\n",
      "training loss------------->  307.4215393066406\n",
      "Epoch: 1891 Loss : 611.8206176757812\n",
      "training loss------------->  196.25164794921875\n",
      "Epoch: 1892 Loss : 540.7107543945312\n",
      "training loss------------->  492.9237060546875\n",
      "Epoch: 1893 Loss : 69.23947143554688\n",
      "training loss------------->  404.7110900878906\n",
      "Epoch: 1894 Loss : 107.99468994140625\n",
      "training loss------------->  489.99517822265625\n",
      "Epoch: 1895 Loss : 363.063232421875\n",
      "training loss------------->  123.4517822265625\n",
      "Epoch: 1896 Loss : 431.9527587890625\n",
      "training loss------------->  594.8099975585938\n",
      "Epoch: 1897 Loss : 1132.458251953125\n",
      "training loss------------->  565.699951171875\n",
      "Epoch: 1898 Loss : 188.17591857910156\n",
      "training loss------------->  397.1617431640625\n",
      "Epoch: 1899 Loss : 1442.479736328125\n",
      "training loss------------->  627.4805908203125\n",
      "Epoch: 1900 Loss : 275.5875244140625\n",
      "training loss------------->  360.4025573730469\n",
      "Epoch: 1901 Loss : 155.3805389404297\n",
      "training loss------------->  432.48052978515625\n",
      "Epoch: 1902 Loss : 247.35169982910156\n",
      "training loss------------->  362.7716064453125\n",
      "Epoch: 1903 Loss : 593.7479248046875\n",
      "training loss------------->  367.08148193359375\n",
      "Epoch: 1904 Loss : 298.91424560546875\n",
      "training loss------------->  165.7345428466797\n",
      "Epoch: 1905 Loss : 244.12158203125\n",
      "training loss------------->  321.76190185546875\n",
      "Epoch: 1906 Loss : 345.26422119140625\n",
      "training loss------------->  547.9054565429688\n",
      "Epoch: 1907 Loss : 73.9666748046875\n",
      "training loss------------->  410.55267333984375\n",
      "Epoch: 1908 Loss : 156.08753967285156\n",
      "training loss------------->  342.85693359375\n",
      "Epoch: 1909 Loss : 92.93428039550781\n",
      "training loss------------->  312.2409362792969\n",
      "Epoch: 1910 Loss : 220.660400390625\n",
      "training loss------------->  373.4322814941406\n",
      "Epoch: 1911 Loss : 425.43115234375\n",
      "training loss------------->  463.9136962890625\n",
      "Epoch: 1912 Loss : 569.0706787109375\n",
      "training loss------------->  337.0106201171875\n",
      "Epoch: 1913 Loss : 436.35205078125\n",
      "training loss------------->  501.38189697265625\n",
      "Epoch: 1914 Loss : 664.1046142578125\n",
      "training loss------------->  562.8301391601562\n",
      "Epoch: 1915 Loss : 443.6571960449219\n",
      "training loss------------->  232.3450469970703\n",
      "Epoch: 1916 Loss : 648.7360229492188\n",
      "training loss------------->  419.9317626953125\n",
      "Epoch: 1917 Loss : 476.18170166015625\n",
      "training loss------------->  414.7865295410156\n",
      "Epoch: 1918 Loss : 318.6705322265625\n",
      "training loss------------->  477.3435974121094\n",
      "Epoch: 1919 Loss : 138.89227294921875\n",
      "training loss------------->  265.6365051269531\n",
      "Epoch: 1920 Loss : 549.3609619140625\n",
      "training loss------------->  291.91668701171875\n",
      "Epoch: 1921 Loss : 471.62835693359375\n",
      "training loss------------->  238.96163940429688\n",
      "Epoch: 1922 Loss : 633.54736328125\n",
      "training loss------------->  451.6838073730469\n",
      "Epoch: 1923 Loss : 826.4200439453125\n",
      "training loss------------->  198.80746459960938\n",
      "Epoch: 1924 Loss : 181.514892578125\n",
      "training loss------------->  250.11167907714844\n",
      "Epoch: 1925 Loss : 211.99942016601562\n",
      "training loss------------->  274.6192626953125\n",
      "Epoch: 1926 Loss : 113.87543487548828\n",
      "training loss------------->  420.3675537109375\n",
      "Epoch: 1927 Loss : 695.6804809570312\n",
      "training loss------------->  279.2580871582031\n",
      "Epoch: 1928 Loss : 784.7064819335938\n",
      "training loss------------->  531.9716796875\n",
      "Epoch: 1929 Loss : 448.507568359375\n",
      "training loss------------->  353.97723388671875\n",
      "Epoch: 1930 Loss : 310.6803894042969\n",
      "training loss------------->  389.3608703613281\n",
      "Epoch: 1931 Loss : 487.0419921875\n",
      "training loss------------->  404.6238708496094\n",
      "Epoch: 1932 Loss : 442.3795166015625\n",
      "training loss------------->  547.819091796875\n",
      "Epoch: 1933 Loss : 109.89178466796875\n",
      "training loss------------->  293.228271484375\n",
      "Epoch: 1934 Loss : 538.2415161132812\n",
      "training loss------------->  335.8800048828125\n",
      "Epoch: 1935 Loss : 639.4122314453125\n",
      "training loss------------->  316.4049987792969\n",
      "Epoch: 1936 Loss : 164.33352661132812\n",
      "training loss------------->  372.15081787109375\n",
      "Epoch: 1937 Loss : 237.4906005859375\n",
      "training loss------------->  504.6023864746094\n",
      "Epoch: 1938 Loss : 499.13555908203125\n",
      "training loss------------->  247.1570587158203\n",
      "Epoch: 1939 Loss : 111.03949737548828\n",
      "training loss------------->  261.32684326171875\n",
      "Epoch: 1940 Loss : 448.6634826660156\n",
      "training loss------------->  268.7483825683594\n",
      "Epoch: 1941 Loss : 76.01838684082031\n",
      "training loss------------->  447.3684997558594\n",
      "Epoch: 1942 Loss : 308.735107421875\n",
      "training loss------------->  566.0084838867188\n",
      "Epoch: 1943 Loss : 741.2362060546875\n",
      "training loss------------->  407.5132141113281\n",
      "Epoch: 1944 Loss : 493.05560302734375\n",
      "training loss------------->  250.82589721679688\n",
      "Epoch: 1945 Loss : 1018.8046264648438\n",
      "training loss------------->  402.025634765625\n",
      "Epoch: 1946 Loss : 130.41835021972656\n",
      "training loss------------->  254.10174560546875\n",
      "Epoch: 1947 Loss : 427.59344482421875\n",
      "training loss------------->  368.90972900390625\n",
      "Epoch: 1948 Loss : 567.41552734375\n",
      "training loss------------->  463.50732421875\n",
      "Epoch: 1949 Loss : 238.48582458496094\n",
      "training loss------------->  74.931396484375\n",
      "Epoch: 1950 Loss : 591.0975952148438\n",
      "training loss------------->  579.4263305664062\n",
      "Epoch: 1951 Loss : 901.473388671875\n",
      "training loss------------->  570.80224609375\n",
      "Epoch: 1952 Loss : 182.92095947265625\n",
      "training loss------------->  321.08160400390625\n",
      "Epoch: 1953 Loss : 101.82412719726562\n",
      "training loss------------->  320.4267578125\n",
      "Epoch: 1954 Loss : 629.6554565429688\n",
      "training loss------------->  607.017578125\n",
      "Epoch: 1955 Loss : 466.8873291015625\n",
      "training loss------------->  547.881103515625\n",
      "Epoch: 1956 Loss : 102.29792022705078\n",
      "training loss------------->  436.1629638671875\n",
      "Epoch: 1957 Loss : 449.6156005859375\n",
      "training loss------------->  498.5072021484375\n",
      "Epoch: 1958 Loss : 368.9860534667969\n",
      "training loss------------->  465.4836730957031\n",
      "Epoch: 1959 Loss : 635.1513061523438\n",
      "training loss------------->  648.9169311523438\n",
      "Epoch: 1960 Loss : 586.8916015625\n",
      "training loss------------->  564.1069946289062\n",
      "Epoch: 1961 Loss : 96.12379455566406\n",
      "training loss------------->  419.83746337890625\n",
      "Epoch: 1962 Loss : 98.95718383789062\n",
      "training loss------------->  413.4767761230469\n",
      "Epoch: 1963 Loss : 97.47412109375\n",
      "training loss------------->  335.5887145996094\n",
      "Epoch: 1964 Loss : 526.7437744140625\n",
      "training loss------------->  336.90155029296875\n",
      "Epoch: 1965 Loss : 119.82159423828125\n",
      "training loss------------->  204.29090881347656\n",
      "Epoch: 1966 Loss : 150.5644989013672\n",
      "training loss------------->  374.8242492675781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1967 Loss : 709.1270751953125\n",
      "training loss------------->  228.98635864257812\n",
      "Epoch: 1968 Loss : 378.8017578125\n",
      "training loss------------->  311.0481262207031\n",
      "Epoch: 1969 Loss : 9.920186042785645\n",
      "training loss------------->  197.47459411621094\n",
      "Epoch: 1970 Loss : 366.7737121582031\n",
      "training loss------------->  298.1424560546875\n",
      "Epoch: 1971 Loss : 101.71792602539062\n",
      "training loss------------->  422.5778503417969\n",
      "Epoch: 1972 Loss : 156.2931671142578\n",
      "training loss------------->  362.15240478515625\n",
      "Epoch: 1973 Loss : 120.54664611816406\n",
      "training loss------------->  372.2113037109375\n",
      "Epoch: 1974 Loss : 301.3681640625\n",
      "training loss------------->  616.1912231445312\n",
      "Epoch: 1975 Loss : 219.20980834960938\n",
      "training loss------------->  214.79571533203125\n",
      "Epoch: 1976 Loss : 451.47283935546875\n",
      "training loss------------->  166.24380493164062\n",
      "Epoch: 1977 Loss : 214.30413818359375\n",
      "training loss------------->  262.6910705566406\n",
      "Epoch: 1978 Loss : 730.937744140625\n",
      "training loss------------->  213.20599365234375\n",
      "Epoch: 1979 Loss : 108.32378387451172\n",
      "training loss------------->  407.0244140625\n",
      "Epoch: 1980 Loss : 687.875732421875\n",
      "training loss------------->  251.90478515625\n",
      "Epoch: 1981 Loss : 339.90179443359375\n",
      "training loss------------->  354.53790283203125\n",
      "Epoch: 1982 Loss : 468.04156494140625\n",
      "training loss------------->  174.75509643554688\n",
      "Epoch: 1983 Loss : 594.1683959960938\n",
      "training loss------------->  186.08534240722656\n",
      "Epoch: 1984 Loss : 734.6031494140625\n",
      "training loss------------->  371.7181701660156\n",
      "Epoch: 1985 Loss : 311.6361999511719\n",
      "training loss------------->  574.5260620117188\n",
      "Epoch: 1986 Loss : 246.78237915039062\n",
      "training loss------------->  374.9477844238281\n",
      "Epoch: 1987 Loss : 919.5701904296875\n",
      "training loss------------->  313.2691650390625\n",
      "Epoch: 1988 Loss : 780.4349975585938\n",
      "training loss------------->  434.18060302734375\n",
      "Epoch: 1989 Loss : 200.6353759765625\n",
      "training loss------------->  593.0751342773438\n",
      "Epoch: 1990 Loss : 11.835959434509277\n",
      "training loss------------->  352.50494384765625\n",
      "Epoch: 1991 Loss : 523.2852172851562\n",
      "training loss------------->  1066.689697265625\n",
      "Epoch: 1992 Loss : 235.17214965820312\n",
      "training loss------------->  214.4917449951172\n",
      "Epoch: 1993 Loss : 526.0718383789062\n",
      "training loss------------->  352.5340881347656\n",
      "Epoch: 1994 Loss : 565.6846923828125\n",
      "training loss------------->  367.63787841796875\n",
      "Epoch: 1995 Loss : 137.66561889648438\n",
      "training loss------------->  149.32794189453125\n",
      "Epoch: 1996 Loss : 104.04598236083984\n",
      "training loss------------->  381.1910705566406\n",
      "Epoch: 1997 Loss : 841.4456787109375\n",
      "training loss------------->  236.67724609375\n",
      "Epoch: 1998 Loss : 831.2616577148438\n",
      "training loss------------->  340.6894836425781\n",
      "Epoch: 1999 Loss : 239.03866577148438\n",
      "training loss------------->  572.899658203125\n",
      "Epoch: 2000 Loss : 281.88140869140625\n",
      "training loss------------->  527.7127685546875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lenet = train(2000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "4f263318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([39.2500, 19.7500, 65.6250, 64.8750, 52.6250, 60.7500, 57.3750, 63.2500],\n",
      "       dtype=torch.float64)\n",
      "tensor([[47.5604],\n",
      "        [40.6066],\n",
      "        [38.8053],\n",
      "        [28.4483],\n",
      "        [47.1046],\n",
      "        [42.9672],\n",
      "        [37.7778],\n",
      "        [46.1147]])\n",
      "tensor(396.2004, dtype=torch.float64)\n",
      "tensor([47.3750, 31.5000, 10.6250,  6.1250, 59.5000, 55.6250, 31.6250, 46.7500],\n",
      "       dtype=torch.float64)\n",
      "tensor([[34.3095],\n",
      "        [37.4516],\n",
      "        [57.4558],\n",
      "        [41.4743],\n",
      "        [43.6781],\n",
      "        [49.1540],\n",
      "        [30.5042],\n",
      "        [49.7340]])\n",
      "tensor(461.0689, dtype=torch.float64)\n",
      "tensor([54.2500, 59.1250, 43.8750, 25.6250,  0.6250, 64.6250, 60.3750, 26.8750],\n",
      "       dtype=torch.float64)\n",
      "tensor([[46.1755],\n",
      "        [41.7098],\n",
      "        [40.8055],\n",
      "        [52.2867],\n",
      "        [48.1866],\n",
      "        [58.0967],\n",
      "        [38.4282],\n",
      "        [48.4196]])\n",
      "tensor(498.4044, dtype=torch.float64)\n",
      "tensor([49.1250, 57.5000, 63.6250, 45.5000, 24.7500, 26.5000, 58.8750, 57.1250],\n",
      "       dtype=torch.float64)\n",
      "tensor([[38.5137],\n",
      "        [26.3576],\n",
      "        [44.4487],\n",
      "        [38.1627],\n",
      "        [47.9063],\n",
      "        [54.5815],\n",
      "        [54.4928],\n",
      "        [58.7617]])\n",
      "tensor(301.2646, dtype=torch.float64)\n",
      "tensor([42.0000, 51.6250, 52.7500, 57.5000, 43.6250, 29.0000, 59.2500, 55.1250],\n",
      "       dtype=torch.float64)\n",
      "tensor([[35.4184],\n",
      "        [30.9881],\n",
      "        [54.9383],\n",
      "        [69.9423],\n",
      "        [39.3103],\n",
      "        [43.7998],\n",
      "        [35.3350],\n",
      "        [40.9998]])\n",
      "tensor(257.0579, dtype=torch.float64)\n",
      "tensor([29.8750, 45.8750, 54.8750, 59.3750, 45.0000, 27.5000, 59.3750, 54.5000],\n",
      "       dtype=torch.float64)\n",
      "tensor([[54.8397],\n",
      "        [53.0083],\n",
      "        [32.9907],\n",
      "        [48.2415],\n",
      "        [27.8960],\n",
      "        [48.3133],\n",
      "        [35.8952],\n",
      "        [47.4445]])\n",
      "tensor(236.4322, dtype=torch.float64)\n",
      "tensor([23.1250, 43.6250, 53.8750, 59.1250, 42.6250, 23.1250,  1.1250, 63.7500],\n",
      "       dtype=torch.float64)\n",
      "tensor([[45.6336],\n",
      "        [33.9446],\n",
      "        [51.5815],\n",
      "        [73.0821],\n",
      "        [43.6222],\n",
      "        [37.4281],\n",
      "        [41.8543],\n",
      "        [46.3705]])\n",
      "tensor(589.0833, dtype=torch.float64)\n",
      "tensor([60.2500, 32.8750, 50.8750, 56.3750, 62.5000, 44.6250, 24.3750, 54.8750],\n",
      "       dtype=torch.float64)\n",
      "tensor([[56.9069],\n",
      "        [60.3588],\n",
      "        [64.2469],\n",
      "        [39.3054],\n",
      "        [42.7223],\n",
      "        [53.1522],\n",
      "        [47.5494],\n",
      "        [53.9795]])\n",
      "tensor(240.3426, dtype=torch.float64)\n",
      "tensor([48.3750,  7.6250, 34.3750, 52.1250, 56.0000, 42.8750, 26.5000, 29.7500],\n",
      "       dtype=torch.float64)\n",
      "tensor([[48.6316],\n",
      "        [52.7436],\n",
      "        [56.0284],\n",
      "        [38.4540],\n",
      "        [44.8590],\n",
      "        [46.4333],\n",
      "        [26.7120],\n",
      "        [47.8657]])\n",
      "tensor(360.1965, dtype=torch.float64)\n",
      "tensor([ 8.1250, 74.7500, 73.3750, 54.1250, 67.1250, 64.1250, 71.7500, 50.7500],\n",
      "       dtype=torch.float64)\n",
      "tensor([[22.8290],\n",
      "        [45.2135],\n",
      "        [40.1776],\n",
      "        [48.4834],\n",
      "        [47.5515],\n",
      "        [36.6427],\n",
      "        [40.7143],\n",
      "        [27.1115]])\n",
      "tensor(876.7053, dtype=torch.float64)\n",
      "tensor([28.8750], dtype=torch.float64)\n",
      "tensor([[33.5780]])\n",
      "tensor(22.1179, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "for i, (images, labels) in enumerate(test_dl):\n",
    "        images = images.to(device)\n",
    "        images = torch.reshape(images,(images.shape[0],3,80,45))\n",
    "        prediction=lenet(images)\n",
    "        prediction = prediction.to(torch.float)\n",
    "        los=nn.MSELoss()\n",
    "        print(labels)\n",
    "        print(prediction)\n",
    "        print(los(labels,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a03d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d826b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
